# Neural Networks

Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers that process information through weighted connections and activation functions.

## üìñ Mathematical Foundation

### Network Architecture

A basic neural network consists of:
- **Input Layer**: Receives input features
- **Hidden Layers**: Process information through weighted transformations
- **Output Layer**: Produces final predictions

### Forward Propagation

#### Input to Hidden Layer:
$$z^{[1]} = W^{[1]} x + b^{[1]}$$
$$a^{[1]} = g(z^{[1]})$$

#### Hidden to Output Layer:
$$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$$
$$a^{[2]} = g(z^{[2]})$$

Where:
- $W$ = weight matrices
- $b$ = bias vectors
- $g$ = activation function
- $z$ = linear combination before activation
- $a$ = activation output

### Activation Functions

#### Sigmoid Function
$$g(z) = \frac{1}{1 + e^{-z}}$$

**Properties**:
- Range: (0, 1)
- Smooth and differentiable
- Useful for binary classification output layers

#### Sigmoid Derivative
$$\frac{d}{dz} g(z) = g(z)(1 - g(z))$$

This derivative is crucial for backpropagation.

## üèóÔ∏è Basic Architecture: 2‚Üí2‚Üí1 Network

### Structure
```
Input Layer (2 neurons)
    ‚Üì
Hidden Layer (2 neurons, sigmoid)
    ‚Üì
Output Layer (1 neuron, sigmoid)
```

### Implementation Components

#### 1. Sigmoid Activation
```python
def sigmoid(x):
    """Converts any number to 0-1 range"""
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    """Derivative needed for backpropagation"""
    s = sigmoid(x)
    return s * (1 - s)
```

#### 2. Forward Pass
```python
def forward_pass(network, inputs):
    # Input to Hidden Layer
    hidden_input = np.dot(inputs, network.weights_input_to_hidden) + network.bias_hidden
    hidden_output = sigmoid(hidden_input)
    
    # Hidden to Output Layer
    output_input = np.dot(hidden_output, network.weights_hidden_to_output) + network.bias_output
    final_output = sigmoid(output_input)
    
    return {
        'hidden_output': hidden_output,
        'final_output': final_output
    }
```

### Cost Function

For binary classification, we use cross-entropy loss:

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(a^{(i)}) + (1-y^{(i)}) \log(1-a^{(i)})]$$

For regression, we use Mean Squared Error:

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (a^{(i)} - y^{(i)})^2$$

## üîß Training Process

### 1. Initialize Parameters
```python
# Random initialization
np.random.seed(42)
weights_input_to_hidden = np.random.uniform(-1, 1, (2, 2))
weights_hidden_to_output = np.random.uniform(-1, 1, (2, 1))
bias_hidden = np.zeros((1, 2))
bias_output = np.zeros((1, 1))
```

### 2. Training Loop
```python
for epoch in range(num_epochs):
    total_loss = 0
    
    for i in range(len(training_inputs)):
        # Forward pass
        forward_result = forward_pass(network, training_inputs[i])
        
        # Calculate loss
        loss = calculate_loss(forward_result['final_output'], training_outputs[i])
        
        # Backpropagation
        backpropagation(network, forward_result, training_outputs[i], learning_rate)
        
        total_loss += loss
    
    avg_loss = total_loss / len(training_inputs)
    losses.append(avg_loss)
```

## üìä Key Concepts

### Why Neural Networks Work

1. **Non-linearity**: Activation functions enable learning complex patterns
2. **Universal Approximation**: Can approximate any continuous function
3. **Feature Learning**: Automatically discovers relevant features
4. **Hierarchical Learning**: Layers learn increasingly abstract features

### Gradient Flow

The derivative of the activation function determines how well gradients flow:

- **Large derivative** (‚âà0.25): Strong gradient flow, fast learning
- **Moderate derivative** (‚âà0.2): Good gradient flow, stable learning
- **Small derivative** (<0.01): Weak gradient flow, slow learning (vanishing gradients)

## üéØ Learning Outcomes

After implementing a basic neural network, you understand:

1. **Architecture Design**: How to structure layers and neurons
2. **Forward Propagation**: How data flows through the network
3. **Activation Functions**: Role of non-linear transformations
4. **Loss Functions**: How to measure network performance
5. **Parameter Initialization**: Starting weights and biases
6. **Training Mechanics**: The learning process

## üìÅ Available Resources

### Notebooks
1. **Simple_Neural_Network_Beginner.ipynb** - Basic concepts and forward pass
2. **Neural_Network_Training_Beginner.ipynb** - Complete training implementation

### Documentation
- **Neural_Network_Implementation_Documentation.md** - Comprehensive guide
- **Beginner_Neural_Network_Concepts.md** - Conceptual explanations
- **Understanding_Activation_Derivatives.md** - Deep dive into derivatives

### Scripts
- **simple_network_demo.py** - Standalone implementation
- **backpropagation_visualization.py** - Visual learning tools

## üîÑ Next Steps

After mastering basic neural networks, progress to:
- **Advanced Neural Networks**: Deeper architectures with multiple activation functions
- **Backpropagation**: Understanding the learning algorithm in detail
- **Regularization**: Techniques to prevent overfitting
- **Optimization**: Momentum, learning rate decay, and advanced optimizers

---

*This implementation emphasizes educational clarity with NumPy-only core algorithms and comprehensive mathematical documentation.*
