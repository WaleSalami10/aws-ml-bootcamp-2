{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Complete Guide\n",
    "\n",
    "A comprehensive guide to logistic regression covering theory, implementation, regularization, and practical applications for classification problems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Mathematical Foundation](#mathematical-foundation)\n",
    "3. [Types of Logistic Regression](#types-of-logistic-regression)\n",
    "4. [Implementation from Scratch](#implementation-from-scratch)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Regularization Techniques](#regularization-techniques)\n",
    "7. [Model Evaluation](#model-evaluation)\n",
    "8. [Practical Considerations](#practical-considerations)\n",
    "9. [Advanced Topics](#advanced-topics)\n",
    "10. [Real-World Applications](#real-world-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Logistic regression is a fundamental classification algorithm that uses the logistic function to model the probability of binary or categorical outcomes. Despite its name, it's a classification algorithm, not a regression algorithm.\n",
    "\n",
    "### Key Characteristics\n",
    "- **Supervised Learning**: Requires labeled training data\n",
    "- **Classification Task**: Predicts discrete categories/classes\n",
    "- **Probabilistic**: Outputs probabilities between 0 and 1\n",
    "- **Linear Decision Boundary**: Creates linear separation between classes\n",
    "- **Interpretable**: Coefficients represent log-odds ratios\n",
    "\n",
    "### When to Use Logistic Regression\n",
    "- **Binary Classification**: Two-class problems (spam/not spam, disease/healthy)\n",
    "- **Multi-class Classification**: Multiple categories (with extensions)\n",
    "- **Probability Estimation**: When you need class probabilities\n",
    "- **Baseline Model**: Starting point for classification problems\n",
    "- **Interpretability**: When understanding feature importance is crucial\n",
    "\n",
    "### Advantages\n",
    "- Simple and fast\n",
    "- No tuning of hyperparameters required\n",
    "- Doesn't require feature scaling (but recommended)\n",
    "- Less prone to overfitting with low-dimensional data\n",
    "- Provides probability estimates\n",
    "- No assumptions about distributions of classes\n",
    "\n",
    "### Disadvantages\n",
    "- Assumes linear relationship between features and log-odds\n",
    "- Sensitive to outliers\n",
    "- Requires large sample sizes for stable results\n",
    "- Can struggle with complex relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### 1. The Logistic Function (Sigmoid)\n",
    "\n",
    "The core of logistic regression is the sigmoid function:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where $z = w^T x + b$\n",
    "\n",
    "**Properties of Sigmoid:**\n",
    "- Output range: $(0, 1)$\n",
    "- S-shaped curve\n",
    "- Smooth and differentiable\n",
    "- $\\sigma(0) = 0.5$\n",
    "- $\\sigma(-\\infty) = 0$, $\\sigma(+\\infty) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    # Clip z to prevent overflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Visualize the sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, y, 'b-', linewidth=2, label='Sigmoid Function')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Boundary (0.5)')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.7, label='z = 0')\n",
    "plt.xlabel('z = w^T x + b')\n",
    "plt.ylabel('Ïƒ(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ïƒ(-5) = {sigmoid(-5):.4f}\")\n",
    "print(f\"Ïƒ(0) = {sigmoid(0):.4f}\")\n",
    "print(f\"Ïƒ(5) = {sigmoid(5):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Probability Interpretation\n",
    "\n",
    "For binary classification:\n",
    "- $P(y=1|x) = \\sigma(w^T x + b)$\n",
    "- $P(y=0|x) = 1 - \\sigma(w^T x + b)$\n",
    "\n",
    "The decision boundary occurs when $P(y=1|x) = 0.5$, i.e., when $w^T x + b = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Odds and Log-Odds\n",
    "\n",
    "**Odds:**\n",
    "$$\\text{Odds} = \\frac{P(y=1|x)}{P(y=0|x)} = \\frac{\\sigma(z)}{1-\\sigma(z)} = e^z$$\n",
    "\n",
    "**Log-Odds (Logit):**\n",
    "$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = z = w^T x + b$$\n",
    "\n",
    "This shows that logistic regression models the log-odds as a linear function of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the relationship between probability, odds, and log-odds\n",
    "probabilities = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "odds = probabilities / (1 - probabilities)\n",
    "log_odds = np.log(odds)\n",
    "\n",
    "print(\"Probability\\tOdds\\t\\tLog-Odds\")\n",
    "print(\"-\" * 40)\n",
    "for p, o, lo in zip(probabilities, odds, log_odds):\n",
    "    print(f\"{p:.1f}\\t\\t{o:.2f}\\t\\t{lo:.2f}\")\n",
    "\n",
    "# Visualize the relationships\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Probability vs Log-Odds\n",
    "z_range = np.linspace(-5, 5, 100)\n",
    "prob_range = sigmoid(z_range)\n",
    "axes[0].plot(z_range, prob_range, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Log-Odds (z)')\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_title('Probability vs Log-Odds')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Odds vs Log-Odds\n",
    "odds_range = np.exp(z_range)\n",
    "axes[1].plot(z_range, odds_range, 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Log-Odds (z)')\n",
    "axes[1].set_ylabel('Odds')\n",
    "axes[1].set_title('Odds vs Log-Odds')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Probability vs Odds\n",
    "axes[2].plot(prob_range, odds_range, 'g-', linewidth=2)\n",
    "axes[2].set_xlabel('Probability')\n",
    "axes[2].set_ylabel('Odds')\n",
    "axes[2].set_title('Probability vs Odds')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost Function (Cross-Entropy Loss)\n",
    "\n",
    "Unlike linear regression, we can't use MSE because it's non-convex for logistic regression. Instead, we use the log-likelihood:\n",
    "\n",
    "**For a single example:**\n",
    "$$\\ell(y, \\hat{y}) = y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "**Cost function (negative log-likelihood):**\n",
    "$$J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "Where $\\hat{y}^{(i)} = \\sigma(w^T x^{(i)} + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_cost(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Compute logistic regression cost (cross-entropy)\n",
    "    \n",
    "    Args:\n",
    "        y_true: actual labels (0 or 1)\n",
    "        y_pred_proba: predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        cost: cross-entropy cost\n",
    "    \"\"\"\n",
    "    # Add small epsilon to prevent log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "    \n",
    "    cost = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                   (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    return cost\n",
    "\n",
    "# Example: Compare costs for different predictions\n",
    "y_true = np.array([1, 0, 1, 0, 1])\n",
    "y_pred_good = np.array([0.9, 0.1, 0.8, 0.2, 0.85])  # Good predictions\n",
    "y_pred_bad = np.array([0.3, 0.7, 0.4, 0.6, 0.45])   # Bad predictions\n",
    "\n",
    "cost_good = compute_logistic_cost(y_true, y_pred_good)\n",
    "cost_bad = compute_logistic_cost(y_true, y_pred_bad)\n",
    "\n",
    "print(f\"True labels: {y_true}\")\n",
    "print(f\"Good predictions: {y_pred_good}\")\n",
    "print(f\"Bad predictions: {y_pred_bad}\")\n",
    "print(f\"\\nCost (good predictions): {cost_good:.4f}\")\n",
    "print(f\"Cost (bad predictions): {cost_bad:.4f}\")\n",
    "print(f\"\\nLower cost indicates better predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent for Logistic Regression\n",
    "\n",
    "**Gradients:**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "**Parameter Updates:**\n",
    "$$w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}$$\n",
    "$$b := b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "Note: The gradient form is identical to linear regression, but $\\hat{y}$ is computed using the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute gradients for logistic regression\n",
    "    \n",
    "    Args:\n",
    "        X: feature matrix\n",
    "        y_true: actual labels\n",
    "        y_pred: predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        dw: gradient for weights\n",
    "        db: gradient for bias\n",
    "    \"\"\"\n",
    "    m = len(y_true)\n",
    "    dw = (1/m) * np.dot(X.T, (y_pred - y_true))\n",
    "    db = (1/m) * np.sum(y_pred - y_true)\n",
    "    return dw, db\n",
    "\n",
    "# Example gradient computation\n",
    "X = np.array([[1, 2], [2, 3], [3, 1], [1, 3]])\n",
    "y_true = np.array([1, 1, 0, 0])\n",
    "weights = np.array([0.5, -0.3])\n",
    "bias = 0.1\n",
    "\n",
    "# Forward pass\n",
    "z = np.dot(X, weights) + bias\n",
    "y_pred = sigmoid(z)\n",
    "\n",
    "# Compute gradients\n",
    "dw, db = compute_gradients(X, y_true, y_pred)\n",
    "\n",
    "print(f\"Features: \\n{X}\")\n",
    "print(f\"True labels: {y_true}\")\n",
    "print(f\"Predicted probabilities: {y_pred}\")\n",
    "print(f\"\\nWeight gradients: {dw}\")\n",
    "print(f\"Bias gradient: {db:.4f}\")\n",
    "\n",
    "# Update parameters\n",
    "learning_rate = 0.1\n",
    "weights_new = weights - learning_rate * dw\n",
    "bias_new = bias - learning_rate * db\n",
    "\n",
    "print(f\"\\nUpdated weights: {weights_new}\")\n",
    "print(f\"Updated bias: {bias_new:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Let's implement a complete Logistic Regression class from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function with numerical stability\"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred_proba):\n",
    "        \"\"\"Compute cross-entropy cost\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "        \n",
    "        cost = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                       (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        return cost\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred_proba):\n",
    "        \"\"\"Compute gradients for weights and bias\"\"\"\n",
    "        m = len(y_true)\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred_proba - y_true))\n",
    "        db = (1/m) * np.sum(y_pred_proba - y_true)\n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        # Initialize parameters\n",
    "        self.initialize_parameters(X.shape[1])\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            y_pred_proba = self.predict_proba(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(y, y_pred_proba)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self.compute_gradients(X, y, y_pred_proba)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Progress reporting\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if i > 0 and abs(self.cost_history[-2] - cost) < self.tolerance:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        \n",
    "        # Log loss\n",
    "        log_loss = self.compute_cost(y, y_pred_proba)\n",
    "        \n",
    "        return {'accuracy': accuracy, 'log_loss': log_loss}\n",
    "\n",
    "print(\"âœ… LogisticRegression class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Tumor Classification\n",
    "\n",
    "Let's test our implementation with a medical diagnosis example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample tumor classification data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features: tumor_size, patient_age (normalized)\n",
    "tumor_size = np.random.normal(0, 1, n_samples)\n",
    "patient_age = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.column_stack([tumor_size, patient_age])\n",
    "\n",
    "# Target: malignant (1) or benign (0) with some relationship to features\n",
    "# Larger tumors and older patients more likely to be malignant\n",
    "z_true = 0.8 * tumor_size + 0.3 * patient_age + 0.1\n",
    "probabilities = sigmoid(z_true)\n",
    "y = np.random.binomial(1, probabilities)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Malignant rate: {y.mean():.3f}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(learning_rate=0.01, max_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_metrics = model.evaluate(X_train, y_train)\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"Log Loss: {train_metrics['log_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Log Loss: {test_metrics['log_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"Weights: {model.weights}\")\n",
    "print(f\"Bias: {model.bias:.4f}\")\n",
    "\n",
    "# Interpret coefficients\n",
    "print(f\"\\nCoefficient Interpretation:\")\n",
    "print(f\"Tumor size coefficient: {model.weights[0]:.3f} (positive = larger tumors more likely malignant)\")\n",
    "print(f\"Patient age coefficient: {model.weights[1]:.3f} (positive = older patients more likely malignant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(model.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (Log Loss)')\n",
    "plt.title('Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "plt.subplot(1, 3, 2)\n",
    "# Create a mesh for decision boundary\n",
    "h = 0.1\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = model.predict_proba(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and data points\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n",
    "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', edgecolors='black')\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('Tumor Size (normalized)')\n",
    "plt.ylabel('Patient Age (normalized)')\n",
    "plt.title('Decision Boundary')\n",
    "\n",
    "# Plot 3: Probability predictions\n",
    "plt.subplot(1, 3, 3)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='Benign', color='blue')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='Malignant', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Probability Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Techniques\n",
    "\n",
    "Just like linear regression, logistic regression can benefit from regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ridge Logistic Regression (L2 Regularization)\n",
    "\n",
    "**Cost Function:**\n",
    "$$J_{ridge}(w, b) = J(w, b) + \\lambda \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "Where $J(w, b)$ is the standard logistic regression cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, max_iterations, tolerance)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred_proba):\n",
    "        \"\"\"Compute cost with L2 regularization\"\"\"\n",
    "        logistic_cost = super().compute_cost(y_true, y_pred_proba)\n",
    "        l2_penalty = self.alpha * np.sum(self.weights ** 2)\n",
    "        return logistic_cost + l2_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred_proba):\n",
    "        \"\"\"Compute gradients with L2 regularization\"\"\"\n",
    "        dw, db = super().compute_gradients(X, y_true, y_pred_proba)\n",
    "        dw += 2 * self.alpha * self.weights\n",
    "        return dw, db\n",
    "\n",
    "print(\"âœ… RidgeLogisticRegression class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso Logistic Regression (L1 Regularization)\n",
    "\n",
    "**Cost Function:**\n",
    "$$J_{lasso}(w, b) = J(w, b) + \\lambda \\sum_{j=1}^{n} |w_j|$$\n",
    "\n",
    "Where $J(w, b)$ is the standard logistic regression cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, max_iterations, tolerance)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred_proba):\n",
    "        \"\"\"Compute cost with L1 regularization\"\"\"\n",
    "        logistic_cost = super().compute_cost(y_true, y_pred_proba)\n",
    "        l1_penalty = self.alpha * np.sum(np.abs(self.weights))\n",
    "        return logistic_cost + l1_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred_proba):\n",
    "        \"\"\"Compute gradients with L1 regularization\"\"\"\n",
    "        dw, db = super().compute_gradients(X, y_true, y_pred_proba)\n",
    "        l1_gradient = np.where(self.weights > 0, 1, \n",
    "                              np.where(self.weights < 0, -1, 0))\n",
    "        dw += self.alpha * l1_gradient\n",
    "        return dw, db\n",
    "\n",
    "print(\"âœ… LassoLogisticRegression class implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization techniques\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features to demonstrate overfitting\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Train different models\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(learning_rate=0.01, max_iterations=1000),\n",
    "    'Ridge': RidgeLogisticRegression(alpha=0.1, learning_rate=0.01, max_iterations=1000),\n",
    "    'Lasso': LassoLogisticRegression(alpha=0.01, learning_rate=0.01, max_iterations=1000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    train_metrics = model.evaluate(X_train_poly, y_train)\n",
    "    test_metrics = model.evaluate(X_test_poly, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_acc': train_metrics['accuracy'],\n",
    "        'test_acc': test_metrics['accuracy'],\n",
    "        'weights_norm': np.linalg.norm(model.weights),\n",
    "        'non_zero_weights': np.sum(np.abs(model.weights) > 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"Train Acc: {train_metrics['accuracy']:.4f}, Test Acc: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Weights norm: {results[name]['weights_norm']:.4f}\")\n",
    "    print(f\"Non-zero weights: {results[name]['non_zero_weights']}/{len(model.weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Classification requires different evaluation metrics than regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "**Accuracy:**\n",
    "$$\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n",
    "\n",
    "**Precision:**\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$\n",
    "\n",
    "**Recall (Sensitivity):**\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n",
    "\n",
    "**F1-Score:**\n",
    "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}$$\n",
    "\n",
    "**Specificity:**\n",
    "$$\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Compute comprehensive classification metrics\"\"\"\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': np.array([[tn, fp], [fn, tp]])\n",
    "    }\n",
    "    \n",
    "    # Add log loss if probabilities provided\n",
    "    if y_pred_proba is not None:\n",
    "        epsilon = 1e-15\n",
    "        y_pred_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "        log_loss = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                           (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        metrics['log_loss'] = log_loss\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate our best model\n",
    "best_model = models['Ridge']  # Assuming Ridge performed best\n",
    "y_pred = best_model.predict(X_test_poly)\n",
    "y_pred_proba = best_model.predict_proba(X_test_poly)\n",
    "\n",
    "metrics = compute_classification_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "print(\"ðŸ“Š Comprehensive Model Evaluation:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"[[TN={metrics['confusion_matrix'][0,0]}, FP={metrics['confusion_matrix'][0,1]}]\")\n",
    "print(f\" [FN={metrics['confusion_matrix'][1,0]}, TP={metrics['confusion_matrix'][1,1]}]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve plots True Positive Rate vs False Positive Rate at various threshold settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"Compute ROC curve and AUC\"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    tpr_list = []  # True Positive Rate\n",
    "    fpr_list = []  # False Positive Rate\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    \n",
    "    # Compute AUC using trapezoidal rule\n",
    "    auc = np.trapz(tpr_list, fpr_list)\n",
    "    \n",
    "    return np.array(fpr_list), np.array(tpr_list), auc\n",
    "\n",
    "# Compute and plot ROC curve\n",
    "fpr, tpr, auc = compute_roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = metrics['confusion_matrix']\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Benign', 'Malignant'])\n",
    "plt.yticks(tick_marks, ['Benign', 'Malignant'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(f\"AUC Interpretation:\")\n",
    "print(f\"  0.9-1.0: Excellent\")\n",
    "print(f\"  0.8-0.9: Good\")\n",
    "print(f\"  0.7-0.8: Fair\")\n",
    "print(f\"  0.6-0.7: Poor\")\n",
    "print(f\"  0.5-0.6: Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "\n",
    "### 1. Medical Diagnosis\n",
    "- **Binary**: Disease/No Disease\n",
    "- **Features**: Symptoms, test results, demographics\n",
    "- **Considerations**: High recall for serious conditions, interpretability for doctors\n",
    "\n",
    "### 2. Marketing and Customer Analytics\n",
    "- **Binary**: Purchase/No Purchase, Click/No Click\n",
    "- **Features**: Demographics, behavior, history\n",
    "- **Considerations**: Probability calibration, interpretability for business decisions\n",
    "\n",
    "### 3. Fraud Detection\n",
    "- **Binary**: Fraud/Legitimate\n",
    "- **Features**: Transaction patterns, user behavior\n",
    "- **Considerations**: Imbalanced data, real-time prediction, false positive costs\n",
    "\n",
    "### 4. Spam Detection\n",
    "- **Binary**: Spam/Not Spam\n",
    "- **Features**: Text features, sender information\n",
    "- **Considerations**: Feature engineering from text, concept drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Data Preparation\n",
    "- **Handle Missing Values**: Imputation or removal\n",
    "- **Feature Scaling**: Standardization recommended\n",
    "- **Outlier Detection**: Can significantly impact results\n",
    "- **Class Balance**: Address imbalanced datasets\n",
    "\n",
    "### 2. Model Development\n",
    "- **Start Simple**: Basic logistic regression first\n",
    "- **Feature Engineering**: Create meaningful features\n",
    "- **Regularization**: Use when overfitting occurs\n",
    "- **Cross-Validation**: Proper model selection\n",
    "\n",
    "### 3. Model Evaluation\n",
    "- **Multiple Metrics**: Don't rely on accuracy alone\n",
    "- **ROC/PR Curves**: Understand performance across thresholds\n",
    "- **Confusion Matrix**: Understand error types\n",
    "- **Business Context**: Align metrics with business goals\n",
    "\n",
    "### 4. Production Considerations\n",
    "- **Probability Calibration**: Ensure probabilities are well-calibrated\n",
    "- **Monitoring**: Track model performance over time\n",
    "- **Interpretability**: Maintain explainability\n",
    "- **Threshold Optimization**: Optimize for business metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Logistic regression is a powerful and interpretable classification algorithm that forms the foundation of many machine learning applications. Its probabilistic nature, mathematical elegance, and interpretability make it an excellent choice for many real-world problems.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Mathematical Foundation**: Understanding sigmoid function and cross-entropy loss is crucial\n",
    "- **Implementation Skills**: Building from scratch provides deep insights into the algorithm\n",
    "- **Regularization**: Essential for preventing overfitting and feature selection\n",
    "- **Evaluation**: Comprehensive evaluation beyond accuracy is critical\n",
    "- **Practical Applications**: Wide applicability across domains with proper considerations\n",
    "\n",
    "### Next Steps:\n",
    "1. Practice with different classification datasets\n",
    "2. Experiment with feature engineering techniques\n",
    "3. Compare with other classification algorithms\n",
    "4. Study advanced topics like multinomial logistic regression\n",
    "5. Apply to your own classification problems\n",
    "\n",
    "This comprehensive guide provides the theoretical knowledge and practical skills needed to effectively apply logistic regression to real-world classification problems while understanding its assumptions, limitations, and best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}