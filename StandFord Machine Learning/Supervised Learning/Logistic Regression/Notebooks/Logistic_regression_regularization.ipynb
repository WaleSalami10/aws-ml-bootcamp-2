{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Regularization\n",
    "\n",
    "This notebook implements logistic regression with L1 (Lasso) and L2 (Ridge) regularization from scratch using NumPy.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Standard Logistic Regression Cost Function\n",
    "$$J(w,b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_w(x^{(i)})) + (1-y^{(i)}) \\log(1-h_w(x^{(i)}))]$$\n",
    "\n",
    "where $h_w(x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n",
    "\n",
    "### Ridge Logistic Regression (L2 Regularization)\n",
    "$$J_{ridge}(w,b) = J(w,b) + \\lambda \\sum_{j=1}^n w_j^2$$\n",
    "\n",
    "### Lasso Logistic Regression (L1 Regularization)\n",
    "$$J_{lasso}(w,b) = J(w,b) + \\lambda \\sum_{j=1}^n |w_j|$$\n",
    "\n",
    "### Elastic Net Logistic Regression (L1 + L2)\n",
    "$$J_{elastic}(w,b) = J(w,b) + \\lambda_1 \\sum_{j=1}^n |w_j| + \\lambda_2 \\sum_{j=1}^n w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üìö Logistic Regression with Regularization\")\n",
    "print(\"Implementing Ridge, Lasso, and Elastic Net from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll use the same tumor dataset from the original logistic regression notebook for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tumor dataset (same as original logistic regression notebook)\n",
    "data = pd.read_csv(\"../../../Data/tumor_data.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(data['malignant'].value_counts())\n",
    "print(f\"\\nTarget proportions:\")\n",
    "print(data['malignant'].value_counts(normalize=True))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll split the data and standardize the features (same approach as the original notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "X = data.drop('malignant', axis=1).values\n",
    "y = data['malignant'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training set: {X_train.shape} ({X_train.shape[0]/len(y)*100:.0f}%)\")\n",
    "print(f\"Validation set: {X_val.shape} ({X_val.shape[0]/len(y)*100:.0f}%)\")\n",
    "print(f\"Test set: {X_test.shape} ({X_test.shape[0]/len(y)*100:.0f}%)\")\n",
    "\n",
    "# Standardize the features (same as original notebook)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed\")\n",
    "print(f\"Training features mean: {X_train_scaled.mean(axis=0)[:3]:.3f}...\")  # Show first 3\n",
    "print(f\"Training features std: {X_train_scaled.std(axis=0)[:3]:.3f}...\")   # Show first 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression Implementation\n",
    "\n",
    "### Base Class with Common Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLogisticRegression:\n",
    "    \"\"\"\n",
    "    Base class for regularized logistic regression\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.train_cost_history = []\n",
    "        self.val_cost_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function with numerical stability\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    def compute_logistic_cost(self, y_true, y_pred_proba):\n",
    "        \"\"\"Compute logistic regression cost (cross-entropy)\"\"\"\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Compute total cost (logistic + regularization) - to be overridden\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        return self.compute_logistic_cost(y, y_pred_proba)\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Compute gradients - to be overridden\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        \n",
    "        dw = (1/m) * np.dot(X.T, (y_pred_proba - y))\n",
    "        db = (1/m) * np.sum(y_pred_proba - y)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Initialize parameters\n",
    "        self.initialize_parameters(X_train.shape[1])\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(self.max_iterations):\n",
    "            # Compute gradients and update parameters\n",
    "            dw, db = self.compute_gradients(X_train, y_train)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Compute costs\n",
    "            train_cost = self.compute_cost(X_train, y_train)\n",
    "            self.train_cost_history.append(train_cost)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_cost = self.compute_cost(X_val, y_val)\n",
    "                self.val_cost_history.append(val_cost)\n",
    "            \n",
    "            # Progress reporting\n",
    "            if i % 100 == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Iteration {i}: Train Cost = {train_cost:.6f}, Val Cost = {val_cost:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Iteration {i}: Cost = {train_cost:.6f}\")\n",
    "            \n",
    "            # Early stopping based on cost change\n",
    "            if i > 0 and abs(self.train_cost_history[-2] - train_cost) < self.tolerance:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        \n",
    "        # Log loss\n",
    "        log_loss = self.compute_logistic_cost(y, y_pred_proba)\n",
    "        \n",
    "        return {'accuracy': accuracy, 'log_loss': log_loss}\n",
    "\n",
    "print(\"‚úÖ Base RegularizedLogisticRegression class implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression (L2 Regularization)\n",
    "\n",
    "Ridge logistic regression adds a penalty term proportional to the sum of squares of the weights:\n",
    "\n",
    "$$\\text{Penalty} = \\lambda \\sum_{j=1}^n w_j^2$$\n",
    "\n",
    "**Gradient for Ridge:**\n",
    "$$\\frac{\\partial J_{ridge}}{\\partial w_j} = \\frac{\\partial J}{\\partial w_j} + 2\\lambda w_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeLogisticRegression(RegularizedLogisticRegression):\n",
    "    \"\"\"\n",
    "    Ridge Logistic Regression (L2 Regularization)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, max_iterations, tolerance)\n",
    "        self.alpha = alpha  # Regularization strength\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Compute cost with L2 regularization\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        logistic_cost = self.compute_logistic_cost(y, y_pred_proba)\n",
    "        \n",
    "        # L2 regularization term (don't regularize bias)\n",
    "        l2_penalty = self.alpha * np.sum(self.weights ** 2)\n",
    "        \n",
    "        return logistic_cost + l2_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Compute gradients with L2 regularization\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        \n",
    "        # Standard gradients\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred_proba - y))\n",
    "        db = (1/m) * np.sum(y_pred_proba - y)\n",
    "        \n",
    "        # Add L2 regularization to weight gradients\n",
    "        dw += 2 * self.alpha * self.weights\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "print(\"‚úÖ RidgeLogisticRegression class implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Logistic Regression (L1 Regularization)\n",
    "\n",
    "Lasso logistic regression adds a penalty term proportional to the sum of absolute values of weights:\n",
    "\n",
    "$$\\text{Penalty} = \\lambda \\sum_{j=1}^n |w_j|$$\n",
    "\n",
    "**Gradient for Lasso:**\n",
    "$$\\frac{\\partial J_{lasso}}{\\partial w_j} = \\frac{\\partial J}{\\partial w_j} + \\lambda \\cdot \\text{sign}(w_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoLogisticRegression(RegularizedLogisticRegression):\n",
    "    \"\"\"\n",
    "    Lasso Logistic Regression (L1 Regularization)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, max_iterations, tolerance)\n",
    "        self.alpha = alpha  # Regularization strength\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Compute cost with L1 regularization\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        logistic_cost = self.compute_logistic_cost(y, y_pred_proba)\n",
    "        \n",
    "        # L1 regularization term (don't regularize bias)\n",
    "        l1_penalty = self.alpha * np.sum(np.abs(self.weights))\n",
    "        \n",
    "        return logistic_cost + l1_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Compute gradients with L1 regularization\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        \n",
    "        # Standard gradients\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred_proba - y))\n",
    "        db = (1/m) * np.sum(y_pred_proba - y)\n",
    "        \n",
    "        # Add L1 regularization to weight gradients\n",
    "        # Use sign function, but handle zero weights carefully\n",
    "        l1_gradient = np.where(self.weights > 0, 1, \n",
    "                              np.where(self.weights < 0, -1, 0))\n",
    "        dw += self.alpha * l1_gradient\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "print(\"‚úÖ LassoLogisticRegression class implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Logistic Regression (L1 + L2)\n",
    "\n",
    "Elastic Net combines both L1 and L2 regularization:\n",
    "\n",
    "$$\\text{Penalty} = \\lambda_1 \\sum_{j=1}^n |w_j| + \\lambda_2 \\sum_{j=1}^n w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNetLogisticRegression(RegularizedLogisticRegression):\n",
    "    \"\"\"\n",
    "    Elastic Net Logistic Regression (L1 + L2 Regularization)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, l1_ratio=0.5, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, max_iterations, tolerance)\n",
    "        self.alpha = alpha  # Total regularization strength\n",
    "        self.l1_ratio = l1_ratio  # Ratio of L1 to total regularization (0=Ridge, 1=Lasso)\n",
    "        \n",
    "        # Split alpha between L1 and L2\n",
    "        self.alpha_l1 = alpha * l1_ratio\n",
    "        self.alpha_l2 = alpha * (1 - l1_ratio)\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Compute cost with L1 + L2 regularization\"\"\"\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        logistic_cost = self.compute_logistic_cost(y, y_pred_proba)\n",
    "        \n",
    "        # L1 and L2 regularization terms\n",
    "        l1_penalty = self.alpha_l1 * np.sum(np.abs(self.weights))\n",
    "        l2_penalty = self.alpha_l2 * np.sum(self.weights ** 2)\n",
    "        \n",
    "        return logistic_cost + l1_penalty + l2_penalty\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Compute gradients with L1 + L2 regularization\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        \n",
    "        # Standard gradients\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred_proba - y))\n",
    "        db = (1/m) * np.sum(y_pred_proba - y)\n",
    "        \n",
    "        # Add L1 regularization\n",
    "        l1_gradient = np.where(self.weights > 0, 1, \n",
    "                              np.where(self.weights < 0, -1, 0))\n",
    "        dw += self.alpha_l1 * l1_gradient\n",
    "        \n",
    "        # Add L2 regularization\n",
    "        dw += 2 * self.alpha_l2 * self.weights\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "print(\"‚úÖ ElasticNetLogisticRegression class implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Comparison\n",
    "\n",
    "Let's train all models and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic (No Regularization)': RegularizedLogisticRegression(learning_rate=0.01, max_iterations=2000),\n",
    "    'Ridge (Œ±=0.01)': RidgeLogisticRegression(alpha=0.01, learning_rate=0.01, max_iterations=2000),\n",
    "    'Ridge (Œ±=0.1)': RidgeLogisticRegression(alpha=0.1, learning_rate=0.01, max_iterations=2000),\n",
    "    'Ridge (Œ±=1.0)': RidgeLogisticRegression(alpha=1.0, learning_rate=0.01, max_iterations=2000),\n",
    "    'Lasso (Œ±=0.01)': LassoLogisticRegression(alpha=0.01, learning_rate=0.01, max_iterations=2000),\n",
    "    'Lasso (Œ±=0.1)': LassoLogisticRegression(alpha=0.1, learning_rate=0.01, max_iterations=2000),\n",
    "    'Elastic Net (Œ±=0.1)': ElasticNetLogisticRegression(alpha=0.1, l1_ratio=0.5, learning_rate=0.01, max_iterations=2000)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"üöÄ Training all models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "    \n",
    "    # Evaluate on training, validation, and test sets\n",
    "    train_metrics = model.evaluate(X_train_scaled, y_train)\n",
    "    val_metrics = model.evaluate(X_val_scaled, y_val)\n",
    "    test_metrics = model.evaluate(X_test_scaled, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[name] = model\n",
    "    results[name] = {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics,\n",
    "        'weights_norm': np.linalg.norm(model.weights),\n",
    "        'non_zero_weights': np.sum(np.abs(model.weights) > 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"Train Acc: {train_metrics['accuracy']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, Test Acc: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Weights norm: {results[name]['weights_norm']:.4f}\")\n",
    "    print(f\"Non-zero weights: {results[name]['non_zero_weights']}/{len(model.weights)}\\n\")\n",
    "\n",
    "print(\"‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train Accuracy': [results[name]['train']['accuracy'] for name in results.keys()],\n",
    "    'Val Accuracy': [results[name]['val']['accuracy'] for name in results.keys()],\n",
    "    'Test Accuracy': [results[name]['test']['accuracy'] for name in results.keys()],\n",
    "    'Train Log Loss': [results[name]['train']['log_loss'] for name in results.keys()],\n",
    "    'Val Log Loss': [results[name]['val']['log_loss'] for name in results.keys()],\n",
    "    'Test Log Loss': [results[name]['test']['log_loss'] for name in results.keys()],\n",
    "    'Weights Norm': [results[name]['weights_norm'] for name in results.keys()],\n",
    "    'Non-zero Weights': [results[name]['non_zero_weights'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "# Calculate overfitting (difference between train and validation performance)\n",
    "results_df['Overfitting (Train-Val Acc)'] = results_df['Train Accuracy'] - results_df['Val Accuracy']\n",
    "\n",
    "print(\"üìä Model Comparison Results:\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find best model based on validation performance\n",
    "best_model_name = results_df.loc[results_df['Val Accuracy'].idxmax(), 'Model']\n",
    "print(f\"\\nüèÜ Best model based on validation accuracy: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Training and Validation Loss Curves\n",
    "plt.subplot(2, 4, 1)\n",
    "for name, model in trained_models.items():\n",
    "    if len(model.val_cost_history) > 0:\n",
    "        plt.plot(model.train_cost_history, label=f'{name} (Train)', alpha=0.7)\n",
    "        plt.plot(model.val_cost_history, label=f'{name} (Val)', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Accuracy Comparison\n",
    "plt.subplot(2, 4, 2)\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x_pos - width, results_df['Train Accuracy'], width, label='Train', alpha=0.8)\n",
    "plt.bar(x_pos, results_df['Val Accuracy'], width, label='Validation', alpha=0.8)\n",
    "plt.bar(x_pos + width, results_df['Test Accuracy'], width, label='Test', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xticks(x_pos, [name.split('(')[0].strip() for name in results_df['Model']], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Overfitting Analysis\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.bar(range(len(results_df)), results_df['Overfitting (Train-Val Acc)'], alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Train Acc - Val Acc')\n",
    "plt.title('Overfitting Analysis')\n",
    "plt.xticks(range(len(results_df)), [name.split('(')[0].strip() for name in results_df['Model']], rotation=45)\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Weights Magnitude\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.bar(range(len(results_df)), results_df['Weights Norm'], alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('L2 Norm of Weights')\n",
    "plt.title('Weights Magnitude')\n",
    "plt.xticks(range(len(results_df)), [name.split('(')[0].strip() for name in results_df['Model']], rotation=45)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Feature Selection (Non-zero weights)\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.bar(range(len(results_df)), results_df['Non-zero Weights'], alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Number of Non-zero Weights')\n",
    "plt.title('Feature Selection Effect')\n",
    "plt.xticks(range(len(results_df)), [name.split('(')[0].strip() for name in results_df['Model']], rotation=45)\n",
    "plt.axhline(y=X_train_scaled.shape[1], color='red', linestyle='--', alpha=0.5, label='Total Features')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Log Loss Comparison\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.bar(x_pos - width, results_df['Train Log Loss'], width, label='Train', alpha=0.8)\n",
    "plt.bar(x_pos, results_df['Val Log Loss'], width, label='Validation', alpha=0.8)\n",
    "plt.bar(x_pos + width, results_df['Test Log Loss'], width, label='Test', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Log Loss Comparison')\n",
    "plt.xticks(x_pos, [name.split('(')[0].strip() for name in results_df['Model']], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Confusion Matrix for Best Model\n",
    "plt.subplot(2, 4, 7)\n",
    "best_model = trained_models[best_model_name]\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix\\n({best_model_name})')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Benign', 'Malignant'])\n",
    "plt.yticks(tick_marks, ['Benign', 'Malignant'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Plot 8: Weight Distribution for Best Model\n",
    "plt.subplot(2, 4, 8)\n",
    "weights = best_model.weights\n",
    "plt.hist(weights, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Weight Distribution\\n({best_model_name})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Path Analysis\n",
    "\n",
    "Let's analyze how different regularization strengths affect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_regularization_path(RegularizationClass, alphas, model_name):\n",
    "    \"\"\"\n",
    "    Analyze how regularization strength affects model performance\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        # Train model\n",
    "        model = RegularizationClass(alpha=alpha, learning_rate=0.01, max_iterations=1000)\n",
    "        model.fit(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_metrics = model.evaluate(X_train_scaled, y_train)\n",
    "        val_metrics = model.evaluate(X_val_scaled, y_val)\n",
    "        \n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'weights_norm': np.linalg.norm(model.weights),\n",
    "            'non_zero_weights': np.sum(np.abs(model.weights) > 1e-6)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define alpha ranges\n",
    "alphas_ridge = np.logspace(-4, 1, 15)  # 0.0001 to 10\n",
    "alphas_lasso = np.logspace(-4, 0, 15)  # 0.0001 to 1\n",
    "\n",
    "print(\"üîç Analyzing regularization paths...\")\n",
    "\n",
    "# Analyze Ridge regularization path\n",
    "ridge_path = analyze_regularization_path(RidgeLogisticRegression, alphas_ridge, 'Ridge')\n",
    "\n",
    "# Analyze Lasso regularization path\n",
    "lasso_path = analyze_regularization_path(LassoLogisticRegression, alphas_lasso, 'Lasso')\n",
    "\n",
    "print(\"‚úÖ Regularization path analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization paths\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Ridge Regularization Path\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.semilogx(ridge_path['alpha'], ridge_path['train_accuracy'], 'b-', label='Train Accuracy', marker='o')\n",
    "plt.semilogx(ridge_path['alpha'], ridge_path['val_accuracy'], 'r-', label='Val Accuracy', marker='s')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Ridge: Accuracy vs Regularization Strength')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.loglog(ridge_path['alpha'], ridge_path['weights_norm'], 'g-', marker='o')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Weights L2 Norm')\n",
    "plt.title('Ridge: Weights Magnitude vs Œ±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.semilogx(ridge_path['alpha'], ridge_path['non_zero_weights'], 'purple', marker='o')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Non-zero Weights')\n",
    "plt.title('Ridge: Feature Selection vs Œ±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso Regularization Path\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.semilogx(lasso_path['alpha'], lasso_path['train_accuracy'], 'b-', label='Train Accuracy', marker='o')\n",
    "plt.semilogx(lasso_path['alpha'], lasso_path['val_accuracy'], 'r-', label='Val Accuracy', marker='s')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Lasso: Accuracy vs Regularization Strength')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.loglog(lasso_path['alpha'], lasso_path['weights_norm'], 'g-', marker='o')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Weights L2 Norm')\n",
    "plt.title('Lasso: Weights Magnitude vs Œ±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.semilogx(lasso_path['alpha'], lasso_path['non_zero_weights'], 'purple', marker='o')\n",
    "plt.xlabel('Regularization Strength (Œ±)')\n",
    "plt.ylabel('Non-zero Weights')\n",
    "plt.title('Lasso: Feature Selection vs Œ±')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal alpha values\n",
    "optimal_ridge_alpha = ridge_path.loc[ridge_path['val_accuracy'].idxmax(), 'alpha']\n",
    "optimal_lasso_alpha = lasso_path.loc[lasso_path['val_accuracy'].idxmax(), 'alpha']\n",
    "\n",
    "print(f\"üéØ Optimal Ridge Œ±: {optimal_ridge_alpha:.6f}\")\n",
    "print(f\"üéØ Optimal Lasso Œ±: {optimal_lasso_alpha:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Conclusions\n",
    "\n",
    "### Regularization Effects in Logistic Regression:\n",
    "\n",
    "1. **Ridge Logistic Regression (L2)**:\n",
    "   - Shrinks weights towards zero but doesn't eliminate them\n",
    "   - Helps prevent overfitting by penalizing large weights\n",
    "   - Maintains all features but reduces their impact\n",
    "\n",
    "2. **Lasso Logistic Regression (L1)**:\n",
    "   - Can set weights exactly to zero (automatic feature selection)\n",
    "   - Produces sparse models by eliminating irrelevant features\n",
    "   - Useful when you suspect many features are irrelevant\n",
    "\n",
    "3. **Elastic Net Logistic Regression**:\n",
    "   - Combines benefits of both Ridge and Lasso\n",
    "   - Good balance between feature selection and weight shrinkage\n",
    "   - Handles correlated features better than pure Lasso\n",
    "\n",
    "### When to Use Each:\n",
    "- **Ridge**: When you believe most features are relevant but want to prevent overfitting\n",
    "- **Lasso**: When you want automatic feature selection and suspect many features are irrelevant\n",
    "- **Elastic Net**: When you want both feature selection and handling of correlated features\n",
    "- **No Regularization**: When you have few features relative to samples and overfitting isn't a concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üìã LOGISTIC REGRESSION REGULARIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: Tumor classification with {X_train_scaled.shape[0]} training samples, {X_train_scaled.shape[1]} features\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Best validation accuracy: {results_df['Val Accuracy'].max():.4f}\")\n",
    "print(f\"Best test accuracy: {results_df.loc[results_df['Val Accuracy'].idxmax(), 'Test Accuracy']:.4f}\")\n",
    "print(f\"Optimal Ridge Œ±: {optimal_ridge_alpha:.6f}\")\n",
    "print(f\"Optimal Lasso Œ±: {optimal_lasso_alpha:.6f}\")\n",
    "\n",
    "print(\"\\nüéì Key Learnings:\")\n",
    "print(\"‚Ä¢ Regularization helps prevent overfitting in logistic regression\")\n",
    "print(\"‚Ä¢ L1 (Lasso) provides automatic feature selection for classification\")\n",
    "print(\"‚Ä¢ L2 (Ridge) shrinks weights but keeps all features\")\n",
    "print(\"‚Ä¢ Elastic Net combines benefits of both L1 and L2\")\n",
    "print(\"‚Ä¢ Cross-validation is crucial for selecting optimal regularization strength\")\n",
    "print(\"‚Ä¢ Medical diagnosis benefits from regularization to avoid overfitting to training data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}