{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Neural Network Implementation\n",
    "\n",
    "This notebook contains the core implementation of the 4‚Üí6‚Üí4‚Üí1 advanced neural network.\n",
    "Run the previous notebook first to get the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this first - copy from the previous notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "np.random.seed(42)\n",
    "\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        s = ActivationFunctions.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Advanced Neural Network Class\n",
    "\n",
    "Here's the complete implementation of our 4‚Üí6‚Üí4‚Üí1 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNeuralNetwork:\n",
    "    def __init__(self, layer_sizes=[4, 6, 4, 1], activations=['relu', 'relu', 'sigmoid']):\n",
    "        \"\"\"\n",
    "        Initialize advanced neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of neurons in each layer [input, hidden1, hidden2, output]\n",
    "            activations: List of activation functions for each layer transition\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.activations = activations\n",
    "        \n",
    "        print(f\"üèóÔ∏è Building {' ‚Üí '.join(map(str, layer_sizes))} neural network\")\n",
    "        print(f\"üìä Activations: {' ‚Üí '.join(activations)}\")\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Advanced weight initialization (Xavier/Glorot initialization)\n",
    "        for i in range(self.num_layers - 1):\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i + 1]\n",
    "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "            \n",
    "            weight_matrix = np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
    "            bias_vector = np.zeros((1, fan_out))\n",
    "            \n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "            \n",
    "            print(f\"Layer {i+1}: {fan_in} ‚Üí {fan_out}, weights shape: {weight_matrix.shape}\")\n",
    "        \n",
    "        print(\"‚úÖ Advanced neural network initialized!\")\n",
    "    \n",
    "    def get_activation_function(self, name):\n",
    "        \"\"\"Get activation function and its derivative\"\"\"\n",
    "        functions = {\n",
    "            'sigmoid': (ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n",
    "            'relu': (ActivationFunctions.relu, ActivationFunctions.relu_derivative),\n",
    "            'tanh': (ActivationFunctions.tanh, ActivationFunctions.tanh_derivative)\n",
    "        }\n",
    "        return functions[name]\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"Advanced forward pass through all layers\"\"\"\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = inputs.reshape(1, -1)\n",
    "        \n",
    "        # Store all intermediate values for backpropagation\n",
    "        activations = [inputs]  # Store input as first activation\n",
    "        z_values = []  # Store pre-activation values\n",
    "        \n",
    "        current_input = inputs\n",
    "        \n",
    "        # Forward pass through each layer\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Linear transformation: z = W*a + b\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            # Apply activation function\n",
    "            activation_func, _ = self.get_activation_function(self.activations[i])\n",
    "            a = activation_func(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "            current_input = a\n",
    "        \n",
    "        return {\n",
    "            'activations': activations,\n",
    "            'z_values': z_values,\n",
    "            'final_output': activations[-1]\n",
    "        }\n",
    "    \n",
    "    def calculate_loss(self, predictions, targets):\n",
    "        \"\"\"Calculate Mean Squared Error loss\"\"\"\n",
    "        if targets.ndim == 1:\n",
    "            targets = targets.reshape(-1, 1)\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        \n",
    "        mse = np.mean((predictions - targets) ** 2)\n",
    "        return mse\n",
    "\n",
    "# Create the advanced network\n",
    "network = AdvancedNeuralNetwork(\n",
    "    layer_sizes=[4, 6, 4, 1],\n",
    "    activations=['relu', 'relu', 'sigmoid']\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = np.array([0.5, 0.8, 0.3, 0.9])\n",
    "result = network.forward_pass(test_input)\n",
    "print(f\"\\nüß™ Test forward pass:\")\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Output: {result['final_output'][0][0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}