{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Basics for Beginners\n",
    "\n",
    "## üéØ Learning Goals\n",
    "- Understand what tensors are and how they relate to NumPy arrays\n",
    "- Learn basic TensorFlow operations\n",
    "- Compare TensorFlow concepts with your existing NumPy knowledge\n",
    "- Prepare for building neural networks with TensorFlow\n",
    "\n",
    "## üìö Prerequisites\n",
    "You should understand:\n",
    "- NumPy arrays and matrix operations\n",
    "- Basic neural network concepts (from your existing notebooks)\n",
    "- Python programming fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Check if GPU is available (optional)\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ What are Tensors?\n",
    "\n",
    "**Simple Answer**: Tensors are just multi-dimensional arrays, like NumPy arrays but with superpowers!\n",
    "\n",
    "### Tensor Dimensions:\n",
    "- **0D Tensor (Scalar)**: Just a number ‚Üí `5`\n",
    "- **1D Tensor (Vector)**: Array of numbers ‚Üí `[1, 2, 3]`\n",
    "- **2D Tensor (Matrix)**: Your familiar matrices ‚Üí `[[1, 2], [3, 4]]`\n",
    "- **3D+ Tensors**: Higher dimensions for images, videos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors - compare with NumPy\n",
    "print(\"=== NUMPY vs TENSORFLOW COMPARISON ===\")\n",
    "\n",
    "# NumPy arrays (what you're familiar with)\n",
    "numpy_scalar = np.array(5.0)\n",
    "numpy_vector = np.array([1.0, 2.0, 3.0])\n",
    "numpy_matrix = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(\"NumPy Arrays:\")\n",
    "print(f\"Scalar: {numpy_scalar}, shape: {numpy_scalar.shape}\")\n",
    "print(f\"Vector: {numpy_vector}, shape: {numpy_vector.shape}\")\n",
    "print(f\"Matrix: \\n{numpy_matrix}, shape: {numpy_matrix.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# TensorFlow tensors (the new way)\n",
    "tf_scalar = tf.constant(5.0)\n",
    "tf_vector = tf.constant([1.0, 2.0, 3.0])\n",
    "tf_matrix = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(\"TensorFlow Tensors:\")\n",
    "print(f\"Scalar: {tf_scalar}, shape: {tf_scalar.shape}\")\n",
    "print(f\"Vector: {tf_vector}, shape: {tf_vector.shape}\")\n",
    "print(f\"Matrix: \\n{tf_matrix}, shape: {tf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Converting Between NumPy and TensorFlow\n",
    "\n",
    "**Good News**: You can easily convert between NumPy arrays and TensorFlow tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between NumPy and TensorFlow\n",
    "print(\"=== CONVERSION EXAMPLES ===\")\n",
    "\n",
    "# Start with NumPy (your comfort zone)\n",
    "numpy_data = np.array([[0.5, 0.3], [0.2, 0.8]])\n",
    "print(f\"Original NumPy array:\\n{numpy_data}\")\n",
    "print(f\"Type: {type(numpy_data)}\")\n",
    "\n",
    "# Convert to TensorFlow\n",
    "tf_data = tf.constant(numpy_data)\n",
    "print(f\"\\nConverted to TensorFlow:\\n{tf_data}\")\n",
    "print(f\"Type: {type(tf_data)}\")\n",
    "\n",
    "# Convert back to NumPy\n",
    "back_to_numpy = tf_data.numpy()\n",
    "print(f\"\\nBack to NumPy:\\n{back_to_numpy}\")\n",
    "print(f\"Type: {type(back_to_numpy)}\")\n",
    "\n",
    "# Check if they're the same\n",
    "print(f\"\\nAre they equal? {np.array_equal(numpy_data, back_to_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Basic Operations - NumPy vs TensorFlow\n",
    "\n",
    "Let's compare the operations you already know in NumPy with their TensorFlow equivalents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations comparison\n",
    "print(\"=== MATRIX OPERATIONS COMPARISON ===\")\n",
    "\n",
    "# Create test matrices\n",
    "a_np = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "b_np = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "\n",
    "a_tf = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b_tf = tf.constant([[0.5, 0.5], [0.5, 0.5]])\n",
    "\n",
    "print(\"Input matrices:\")\n",
    "print(f\"A = \\n{a_np}\")\n",
    "print(f\"B = \\n{b_np}\")\n",
    "\n",
    "print(\"\\n=== ADDITION ===\")\n",
    "add_np = a_np + b_np\n",
    "add_tf = a_tf + b_tf\n",
    "print(f\"NumPy: A + B = \\n{add_np}\")\n",
    "print(f\"TensorFlow: A + B = \\n{add_tf}\")\n",
    "\n",
    "print(\"\\n=== MATRIX MULTIPLICATION ===\")\n",
    "mult_np = np.dot(a_np, b_np)\n",
    "mult_tf = tf.matmul(a_tf, b_tf)\n",
    "print(f\"NumPy: np.dot(A, B) = \\n{mult_np}\")\n",
    "print(f\"TensorFlow: tf.matmul(A, B) = \\n{mult_tf}\")\n",
    "\n",
    "print(\"\\n=== ELEMENT-WISE MULTIPLICATION ===\")\n",
    "elem_np = a_np * b_np\n",
    "elem_tf = a_tf * b_tf\n",
    "print(f\"NumPy: A * B = \\n{elem_np}\")\n",
    "print(f\"TensorFlow: A * B = \\n{elem_tf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Activation Functions - Sigmoid Comparison\n",
    "\n",
    "Remember your sigmoid function from the NumPy implementation? Let's compare it with TensorFlow's built-in version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function comparison\n",
    "print(\"=== SIGMOID FUNCTION COMPARISON ===\")\n",
    "\n",
    "# Your NumPy sigmoid (from your existing code)\n",
    "def sigmoid_numpy(x):\n",
    "    \"\"\"Your familiar NumPy sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Test values\n",
    "test_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "test_values_tf = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Calculate sigmoid with both methods\n",
    "sigmoid_np_result = sigmoid_numpy(test_values)\n",
    "sigmoid_tf_result = tf.nn.sigmoid(test_values_tf)\n",
    "\n",
    "print(f\"Input values: {test_values}\")\n",
    "print(f\"NumPy sigmoid: {sigmoid_np_result}\")\n",
    "print(f\"TensorFlow sigmoid: {sigmoid_tf_result.numpy()}\")\n",
    "print(f\"Are they equal? {np.allclose(sigmoid_np_result, sigmoid_tf_result.numpy())}\")\n",
    "\n",
    "# Visualize both\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y_numpy = sigmoid_numpy(x)\n",
    "y_tf = tf.nn.sigmoid(tf.constant(x)).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y_numpy, 'b-', label='NumPy Sigmoid', linewidth=2)\n",
    "plt.plot(x, y_tf, 'r--', label='TensorFlow Sigmoid', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Sigmoid(x)')\n",
    "plt.title('Sigmoid Function: NumPy vs TensorFlow')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Both produce identical results!\")\n",
    "print(\"   TensorFlow's version is optimized and GPU-accelerated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Variables vs Constants\n",
    "\n",
    "In your NumPy neural network, you manually update weights. TensorFlow has a special concept called **Variables** for parameters that change during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables vs Constants\n",
    "print(\"=== VARIABLES vs CONSTANTS ===\")\n",
    "\n",
    "# Constants (like your fixed weights for demonstration)\n",
    "fixed_weights = tf.constant([[0.5, 0.3], [0.2, 0.8]])\n",
    "print(f\"Constant weights (cannot change):\\n{fixed_weights}\")\n",
    "\n",
    "# Variables (like your trainable weights)\n",
    "trainable_weights = tf.Variable([[0.5, 0.3], [0.2, 0.8]])\n",
    "print(f\"\\nVariable weights (can change):\\n{trainable_weights}\")\n",
    "\n",
    "# Try to modify them\n",
    "print(\"\\n=== MODIFICATION TEST ===\")\n",
    "\n",
    "# This would cause an error with constants:\n",
    "# fixed_weights[0, 0].assign(0.9)  # ‚ùå Error!\n",
    "\n",
    "# But works with variables:\n",
    "print(f\"Before update: {trainable_weights[0, 0]}\")\n",
    "trainable_weights[0, 0].assign(0.9)  # ‚úÖ Works!\n",
    "print(f\"After update: {trainable_weights[0, 0]}\")\n",
    "print(f\"Full matrix after update:\\n{trainable_weights}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Use tf.Variable for weights that need to be trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Automatic Differentiation - The Magic!\n",
    "\n",
    "Remember how you manually calculated gradients in your backpropagation? TensorFlow can do this automatically with **GradientTape**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic differentiation example\n",
    "print(\"=== AUTOMATIC DIFFERENTIATION ===\")\n",
    "\n",
    "# Simple function: f(x) = x^2\n",
    "# We know the derivative is: f'(x) = 2x\n",
    "\n",
    "x = tf.Variable(3.0)  # Input value\n",
    "print(f\"Input x = {x.numpy()}\")\n",
    "\n",
    "# Use GradientTape to automatically calculate gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2  # f(x) = x^2\n",
    "    print(f\"f(x) = x^2 = {y.numpy()}\")\n",
    "\n",
    "# Get the gradient automatically!\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"Automatic gradient df/dx = {gradient.numpy()}\")\n",
    "print(f\"Manual calculation: 2 * {x.numpy()} = {2 * x.numpy()}\")\n",
    "print(f\"Match? {np.isclose(gradient.numpy(), 2 * x.numpy())}\")\n",
    "\n",
    "print(\"\\n=== MORE COMPLEX EXAMPLE ===\")\n",
    "# Simulate a simple loss function like in your neural network\n",
    "# Loss = (prediction - target)^2\n",
    "\n",
    "weight = tf.Variable(0.5)\n",
    "input_val = tf.constant(2.0)\n",
    "target = tf.constant(1.5)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = weight * input_val  # Simple linear model\n",
    "    loss = (prediction - target) ** 2  # Mean squared error\n",
    "    \n",
    "print(f\"Weight: {weight.numpy()}\")\n",
    "print(f\"Input: {input_val.numpy()}\")\n",
    "print(f\"Target: {target.numpy()}\")\n",
    "print(f\"Prediction: {prediction.numpy()}\")\n",
    "print(f\"Loss: {loss.numpy()}\")\n",
    "\n",
    "# Get gradient of loss with respect to weight\n",
    "gradient = tape.gradient(loss, weight)\n",
    "print(f\"\\nGradient dL/dw = {gradient.numpy()}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: No more manual backpropagation calculations!\")\n",
    "print(\"   TensorFlow computes gradients automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Recreating Your Neural Network Forward Pass\n",
    "\n",
    "Let's recreate the forward pass from your NumPy neural network using TensorFlow operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate your 2‚Üí2‚Üí1 network in TensorFlow\n",
    "print(\"=== NEURAL NETWORK FORWARD PASS COMPARISON ===\")\n",
    "\n",
    "# Your original NumPy weights (from your existing network)\n",
    "weights_input_to_hidden_np = np.array([[0.5, 0.3], [0.2, 0.8]])\n",
    "weights_hidden_to_output_np = np.array([[0.6], [0.4]])\n",
    "\n",
    "# Same weights in TensorFlow\n",
    "weights_input_to_hidden_tf = tf.constant([[0.5, 0.3], [0.2, 0.8]])\n",
    "weights_hidden_to_output_tf = tf.constant([[0.6], [0.4]])\n",
    "\n",
    "# Test input\n",
    "test_input = np.array([1.0, 0.5])\n",
    "test_input_tf = tf.constant([1.0, 0.5])\n",
    "\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Weights (input‚Üíhidden):\\n{weights_input_to_hidden_np}\")\n",
    "print(f\"Weights (hidden‚Üíoutput):\\n{weights_hidden_to_output_np.flatten()}\")\n",
    "\n",
    "# NumPy forward pass (your existing method)\n",
    "def forward_pass_numpy(inputs, w1, w2):\n",
    "    hidden_input = np.dot(inputs, w1)\n",
    "    hidden_output = sigmoid_numpy(hidden_input)\n",
    "    output_input = np.dot(hidden_output, w2)\n",
    "    final_output = sigmoid_numpy(output_input)\n",
    "    return hidden_output, final_output\n",
    "\n",
    "# TensorFlow forward pass\n",
    "def forward_pass_tensorflow(inputs, w1, w2):\n",
    "    hidden_input = tf.matmul(tf.expand_dims(inputs, 0), w1)\n",
    "    hidden_output = tf.nn.sigmoid(hidden_input)\n",
    "    output_input = tf.matmul(hidden_output, w2)\n",
    "    final_output = tf.nn.sigmoid(output_input)\n",
    "    return hidden_output, final_output\n",
    "\n",
    "# Run both versions\n",
    "hidden_np, output_np = forward_pass_numpy(test_input, weights_input_to_hidden_np, weights_hidden_to_output_np)\n",
    "hidden_tf, output_tf = forward_pass_tensorflow(test_input_tf, weights_input_to_hidden_tf, weights_hidden_to_output_tf)\n",
    "\n",
    "print(\"\\n=== RESULTS COMPARISON ===\")\n",
    "print(f\"NumPy - Hidden layer: {hidden_np}\")\n",
    "print(f\"TensorFlow - Hidden layer: {hidden_tf.numpy().flatten()}\")\n",
    "print(f\"NumPy - Final output: {output_np[0]}\")\n",
    "print(f\"TensorFlow - Final output: {output_tf.numpy()[0][0]}\")\n",
    "\n",
    "# Check if results match\n",
    "hidden_match = np.allclose(hidden_np, hidden_tf.numpy().flatten())\n",
    "output_match = np.allclose(output_np[0], output_tf.numpy()[0][0])\n",
    "\n",
    "print(f\"\\nHidden layers match? {hidden_match}\")\n",
    "print(f\"Outputs match? {output_match}\")\n",
    "\n",
    "if hidden_match and output_match:\n",
    "    print(\"\\nüéâ Perfect! Both implementations produce identical results!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Small differences detected (likely due to numerical precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Comparison\n",
    "\n",
    "Let's see how NumPy and TensorFlow compare in terms of speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance comparison\n",
    "print(\"=== PERFORMANCE COMPARISON ===\")\n",
    "\n",
    "# Create larger test data\n",
    "num_samples = 1000\n",
    "test_inputs_np = np.random.random((num_samples, 2))\n",
    "test_inputs_tf = tf.constant(test_inputs_np)\n",
    "\n",
    "# Time NumPy version\n",
    "start_time = time.time()\n",
    "for i in range(num_samples):\n",
    "    _, _ = forward_pass_numpy(test_inputs_np[i], weights_input_to_hidden_np, weights_hidden_to_output_np)\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "# Time TensorFlow version (batch processing)\n",
    "start_time = time.time()\n",
    "hidden_batch = tf.matmul(test_inputs_tf, weights_input_to_hidden_tf)\n",
    "hidden_batch = tf.nn.sigmoid(hidden_batch)\n",
    "output_batch = tf.matmul(hidden_batch, weights_hidden_to_output_tf)\n",
    "output_batch = tf.nn.sigmoid(output_batch)\n",
    "tensorflow_time = time.time() - start_time\n",
    "\n",
    "print(f\"NumPy time (1000 samples): {numpy_time:.4f} seconds\")\n",
    "print(f\"TensorFlow time (1000 samples): {tensorflow_time:.4f} seconds\")\n",
    "print(f\"TensorFlow is {numpy_time/tensorflow_time:.1f}x faster\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - TensorFlow is optimized for batch operations\")\n",
    "print(\"   - NumPy is great for learning and understanding\")\n",
    "print(\"   - TensorFlow shines with larger datasets and GPU acceleration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary and Next Steps\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Tensors are like NumPy arrays** with extra features\n",
    "2. **Easy conversion** between NumPy and TensorFlow\n",
    "3. **Similar operations** but with different syntax\n",
    "4. **Variables vs Constants** for trainable parameters\n",
    "5. **Automatic differentiation** eliminates manual gradient calculations\n",
    "6. **Performance benefits** especially for larger datasets\n",
    "\n",
    "### Key Comparisons:\n",
    "\n",
    "| Aspect | NumPy (Your Current Approach) | TensorFlow |\n",
    "|--------|-------------------------------|------------|\n",
    "| **Learning** | ‚úÖ Excellent for understanding | ‚ö†Ô∏è Abstracts details |\n",
    "| **Control** | ‚úÖ Full control over every step | ‚ö†Ô∏è Less granular control |\n",
    "| **Speed** | ‚ö†Ô∏è Slower for large datasets | ‚úÖ Optimized and GPU-accelerated |\n",
    "| **Gradients** | ‚ö†Ô∏è Manual backpropagation | ‚úÖ Automatic differentiation |\n",
    "| **Production** | ‚ö†Ô∏è Not ideal for deployment | ‚úÖ Industry standard |\n",
    "| **Debugging** | ‚úÖ Easy to trace every step | ‚ö†Ô∏è Can be harder to debug |\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Continue with your NumPy foundation** - it's excellent for learning!\n",
    "2. **Try the next notebook**: `02_NumPy_vs_TensorFlow_Comparison.ipynb`\n",
    "3. **Build the same 2‚Üí2‚Üí1 network** using TensorFlow's high-level Keras API\n",
    "4. **Compare training processes** between manual and automatic approaches\n",
    "\n",
    "### üí° Recommendation:\n",
    "\n",
    "Keep your NumPy implementations as the foundation for understanding, and use TensorFlow to see how the same concepts work in a production environment. This dual approach will make you a stronger ML practitioner!\n",
    "\n",
    "**Ready for the next level?** Let's build your first complete TensorFlow neural network! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}