{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Cat vs Non-Cat Image Classification\n",
    "\n",
    "## Overview\n",
    "This notebook implements a logistic regression model from scratch for binary image classification. We'll classify images as either containing a cat (1) or not containing a cat (0).\n",
    "\n",
    "## What is Logistic Regression?\n",
    "Logistic regression is a binary classification algorithm that:\n",
    "1. Takes input features (flattened image pixels)\n",
    "2. Computes a weighted sum: z = w^T * x + b\n",
    "3. Applies sigmoid activation: a = σ(z) = 1/(1 + e^(-z))\n",
    "4. Outputs a probability between 0 and 1\n",
    "\n",
    "## Model Architecture\n",
    "- **Input**: Flattened image (64x64x3 = 12,288 features)\n",
    "- **Parameters**: Weight vector w (12,288,) and bias b (scalar)\n",
    "- **Output**: Probability of being a cat (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Dummy Dataset\n",
    "\n",
    "We'll create synthetic image data to simulate cat and non-cat images:\n",
    "- **Image dimensions**: 64x64 pixels with 3 color channels (RGB)\n",
    "- **Training set**: 200 examples (100 cats, 100 non-cats)\n",
    "- **Test set**: 50 examples (25 cats, 25 non-cats)\n",
    "\n",
    "### Dataset characteristics:\n",
    "- **Cat images**: Higher intensity in certain pixel regions (simulating cat features)\n",
    "- **Non-cat images**: Random patterns with different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_cat_dataset(n_samples=200, img_height=64, img_width=64, n_channels=3, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Create a dummy dataset of cat and non-cat images.\n",
    "    \n",
    "    Arguments:\n",
    "    n_samples -- total number of samples to generate\n",
    "    img_height -- height of each image\n",
    "    img_width -- width of each image\n",
    "    n_channels -- number of color channels (3 for RGB)\n",
    "    test_ratio -- proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "    train_set_x -- training images (n_train, img_height, img_width, n_channels)\n",
    "    train_set_y -- training labels (1, n_train)\n",
    "    test_set_x -- test images (n_test, img_height, img_width, n_channels)\n",
    "    test_set_y -- test labels (1, n_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate number of samples for each class\n",
    "    n_cats = n_samples // 2\n",
    "    n_non_cats = n_samples // 2\n",
    "    \n",
    "    # Generate CAT images (label = 1)\n",
    "    # Cats have higher intensity in center region (simulating cat face/body)\n",
    "    cat_images = np.random.rand(n_cats, img_height, img_width, n_channels) * 100\n",
    "    # Add brighter central region for cats\n",
    "    center_h_start, center_h_end = img_height // 4, 3 * img_height // 4\n",
    "    center_w_start, center_w_end = img_width // 4, 3 * img_width // 4\n",
    "    cat_images[:, center_h_start:center_h_end, center_w_start:center_w_end, :] += 100\n",
    "    cat_images = np.clip(cat_images, 0, 255)\n",
    "    \n",
    "    # Generate NON-CAT images (label = 0)\n",
    "    # Non-cats have more random patterns with lower overall intensity\n",
    "    non_cat_images = np.random.rand(n_non_cats, img_height, img_width, n_channels) * 150\n",
    "    non_cat_images = np.clip(non_cat_images, 0, 255)\n",
    "    \n",
    "    # Combine and create labels\n",
    "    all_images = np.vstack([cat_images, non_cat_images])\n",
    "    all_labels = np.hstack([np.ones(n_cats), np.zeros(n_non_cats)])\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    all_images = all_images[indices]\n",
    "    all_labels = all_labels[indices]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    n_test = int(n_samples * test_ratio)\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    train_set_x = all_images[:n_train]\n",
    "    train_set_y = all_labels[:n_train].reshape(1, n_train)\n",
    "    \n",
    "    test_set_x = all_images[n_train:]\n",
    "    test_set_y = all_labels[n_train:].reshape(1, n_test)\n",
    "    \n",
    "    return train_set_x, train_set_y, test_set_x, test_set_y\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y = create_dummy_cat_dataset(\n",
    "    n_samples=200, img_height=64, img_width=64, n_channels=3, test_ratio=0.25\n",
    ")\n",
    "\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"Training set: {train_set_x_orig.shape[0]} examples\")\n",
    "print(f\"Test set: {test_set_x_orig.shape[0]} examples\")\n",
    "print(f\"Image shape: {train_set_x_orig.shape[1:]}\")\n",
    "print(f\"Train labels shape: {train_set_y.shape}\")\n",
    "print(f\"Number of cats in training: {int(np.sum(train_set_y))}\")\n",
    "print(f\"Number of non-cats in training: {int(train_set_y.shape[1] - np.sum(train_set_y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize Sample Images\n",
    "\n",
    "Let's visualize some examples from our dataset to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "def visualize_samples(X, y, n_samples=6):\n",
    "    \"\"\"\n",
    "    Visualize sample images from the dataset.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- images array\n",
    "    y -- labels array\n",
    "    n_samples -- number of samples to display\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        axes[i].imshow(X[i].astype('uint8'))\n",
    "        label = \"Cat\" if y[0, i] == 1 else \"Non-Cat\"\n",
    "        axes[i].set_title(f\"{label}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_set_x_orig, train_set_y, n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Data\n",
    "\n",
    "### Preprocessing steps:\n",
    "1. **Flatten images**: Convert from (height, width, channels) to a single vector\n",
    "   - Original shape: (64, 64, 3)\n",
    "   - Flattened shape: (12288,) where 12288 = 64 × 64 × 3\n",
    "\n",
    "2. **Normalize pixel values**: Scale from [0, 255] to [0, 1]\n",
    "   - Helps gradient descent converge faster\n",
    "   - Prevents numerical instability\n",
    "\n",
    "3. **Transpose**: Shape becomes (n_features, n_examples)\n",
    "   - Enables efficient vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_x, test_x):\n",
    "    \"\"\"\n",
    "    Flatten and normalize image data.\n",
    "    \n",
    "    Arguments:\n",
    "    train_x -- training images (m_train, height, width, channels)\n",
    "    test_x -- test images (m_test, height, width, channels)\n",
    "    \n",
    "    Returns:\n",
    "    train_x_flatten -- flattened training images (n_features, m_train)\n",
    "    test_x_flatten -- flattened test images (n_features, m_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the images\n",
    "    # reshape(m, -1) flattens each image: (m, height, width, channels) -> (m, height*width*channels)\n",
    "    train_x_flatten = train_x.reshape(train_x.shape[0], -1).T\n",
    "    test_x_flatten = test_x.reshape(test_x.shape[0], -1).T\n",
    "    \n",
    "    # Normalize pixel values from [0, 255] to [0, 1]\n",
    "    train_x_flatten = train_x_flatten / 255.0\n",
    "    test_x_flatten = test_x_flatten / 255.0\n",
    "    \n",
    "    return train_x_flatten, test_x_flatten\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "train_set_x, test_set_x = preprocess_data(train_set_x_orig, test_set_x_orig)\n",
    "\n",
    "print(\"Data preprocessed successfully!\")\n",
    "print(f\"Training set shape: {train_set_x.shape}\")\n",
    "print(f\"Test set shape: {test_set_x.shape}\")\n",
    "print(f\"Training labels shape: {train_set_y.shape}\")\n",
    "print(f\"Test labels shape: {test_set_y.shape}\")\n",
    "print(f\"\\nEach image is now a vector of size: {train_set_x.shape[0]}\")\n",
    "print(f\"Number of training examples: {train_set_x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Helper Functions\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "The sigmoid function σ(z) = 1 / (1 + e^(-z)) maps any real number to a value between 0 and 1.\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (0, 1)\n",
    "- σ(0) = 0.5\n",
    "- As z → ∞, σ(z) → 1\n",
    "- As z → -∞, σ(z) → 0\n",
    "\n",
    "**Why sigmoid for binary classification?**\n",
    "- Outputs can be interpreted as probabilities\n",
    "- Smooth and differentiable (needed for gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z.\n",
    "    \n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "    \n",
    "    Returns:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "# Test sigmoid function\n",
    "print(\"Testing sigmoid function:\")\n",
    "print(f\"sigmoid(0) = {sigmoid(0):.4f} (should be 0.5)\")\n",
    "print(f\"sigmoid(5) = {sigmoid(5):.4f} (should be close to 1)\")\n",
    "print(f\"sigmoid(-5) = {sigmoid(-5):.4f} (should be close to 0)\")\n",
    "\n",
    "# Visualize sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(z, sigmoid(z), linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z', fontsize=12)\n",
    "plt.ylabel('sigmoid(z)', fontsize=12)\n",
    "plt.title('Sigmoid Activation Function', fontsize=14)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='y=0.5')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.5, label='z=0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Parameters\n",
    "\n",
    "We need to initialize:\n",
    "- **Weight vector (w)**: Shape (n_features, 1) - initialized to zeros\n",
    "- **Bias (b)**: Scalar - initialized to zero\n",
    "\n",
    "**Why initialize to zeros for logistic regression?**\n",
    "- Unlike neural networks, logistic regression doesn't suffer from symmetry problems\n",
    "- Zero initialization is simple and works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dim):\n",
    "    \"\"\"\n",
    "    Initialize weights and bias to zeros.\n",
    "    \n",
    "    Arguments:\n",
    "    dim -- size of the w vector (number of features)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized weight vector of shape (dim, 1)\n",
    "    b -- initialized bias (scalar)\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "\n",
    "# Test initialization\n",
    "dim = train_set_x.shape[0]\n",
    "w, b = initialize_parameters(dim)\n",
    "print(f\"Weight vector shape: {w.shape}\")\n",
    "print(f\"Bias value: {b}\")\n",
    "print(f\"Number of parameters: {w.shape[0] + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Forward and Backward Propagation\n",
    "\n",
    "### Forward Propagation\n",
    "Compute the predictions and cost:\n",
    "\n",
    "1. **Linear transformation**: Z = w^T X + b\n",
    "2. **Activation**: A = σ(Z)\n",
    "3. **Cost function** (Binary Cross-Entropy):\n",
    "   \n",
    "   J = -1/m ∑[y log(a) + (1-y) log(1-a)]\n",
    "\n",
    "### Backward Propagation\n",
    "Compute gradients:\n",
    "\n",
    "- **dw** = ∂J/∂w = 1/m X(A-Y)^T\n",
    "- **db** = ∂J/∂b = 1/m ∑(A-Y)\n",
    "\n",
    "**Why Binary Cross-Entropy?**\n",
    "- Penalizes confident wrong predictions heavily\n",
    "- Convex for logistic regression (guarantees finding global minimum)\n",
    "- Derives from maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement forward and backward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, numpy array of shape (n_features, 1)\n",
    "    b -- bias, scalar\n",
    "    X -- input data of shape (n_features, m_examples)\n",
    "    Y -- true labels of shape (1, m_examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- binary cross-entropy cost\n",
    "    dw -- gradient of loss with respect to w\n",
    "    db -- gradient of loss with respect to b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]  # number of examples\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    # Compute activation\n",
    "    Z = np.dot(w.T, X) + b  # Shape: (1, m)\n",
    "    A = sigmoid(Z)           # Shape: (1, m)\n",
    "    \n",
    "    # Compute cost\n",
    "    # Add small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "    cost = -1/m * np.sum(Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "    \n",
    "    # BACKWARD PROPAGATION\n",
    "    dZ = A - Y              # Shape: (1, m)\n",
    "    dw = 1/m * np.dot(X, dZ.T)  # Shape: (n_features, 1)\n",
    "    db = 1/m * np.sum(dZ)       # Scalar\n",
    "    \n",
    "    cost = np.squeeze(cost)  # Remove unnecessary dimensions\n",
    "    \n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"db\": db\n",
    "    }\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "# Test propagation\n",
    "w_test, b_test = initialize_parameters(train_set_x.shape[0])\n",
    "grads, cost = propagate(w_test, b_test, train_set_x[:, :5], train_set_y[:, :5])\n",
    "print(f\"Initial cost (random initialization): {cost:.4f}\")\n",
    "print(f\"Gradient dw shape: {grads['dw'].shape}\")\n",
    "print(f\"Gradient db: {grads['db']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Optimization using Gradient Descent\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "Iteratively update parameters to minimize cost:\n",
    "\n",
    "**Update rules:**\n",
    "- w = w - α × dw\n",
    "- b = b - α × db\n",
    "\n",
    "Where α is the learning rate.\n",
    "\n",
    "**Learning rate (α):**\n",
    "- Too large: May overshoot minimum, diverge\n",
    "- Too small: Slow convergence\n",
    "- Typical values: 0.001 to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n",
    "    \"\"\"\n",
    "    Optimize w and b by running gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, numpy array of shape (n_features, 1)\n",
    "    b -- bias, scalar\n",
    "    X -- input data of shape (n_features, m_examples)\n",
    "    Y -- true labels of shape (1, m_examples)\n",
    "    num_iterations -- number of iterations for gradient descent\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    print_cost -- if True, print cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing weights w and bias b\n",
    "    grads -- dictionary containing gradients\n",
    "    costs -- list of costs computed during optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward and backward propagation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve gradients\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record cost every 10 iterations\n",
    "        if i % 10 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "    \n",
    "    params = {\n",
    "        \"w\": w,\n",
    "        \"b\": b\n",
    "    }\n",
    "    \n",
    "    grads = {\n",
    "        \"dw\": dw,\n",
    "        \"db\": db\n",
    "    }\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "print(\"Optimization function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prediction Function\n",
    "\n",
    "Convert probabilities to binary predictions:\n",
    "- If A > 0.5: predict class 1 (cat)\n",
    "- If A ≤ 0.5: predict class 0 (non-cat)\n",
    "\n",
    "The threshold 0.5 is standard but can be adjusted based on:\n",
    "- Cost of false positives vs false negatives\n",
    "- Class imbalance\n",
    "- Business requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    Predict labels using learned parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, numpy array of shape (n_features, 1)\n",
    "    b -- bias, scalar\n",
    "    X -- input data of shape (n_features, m_examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- predictions for the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    # Compute predictions\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    Y_prediction = (A > 0.5).astype(int)\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "print(\"Prediction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Complete Model\n",
    "\n",
    "Combine all components into a single model function that:\n",
    "1. Initializes parameters\n",
    "2. Runs gradient descent\n",
    "3. Makes predictions on train and test sets\n",
    "4. Returns learned parameters and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.005, print_cost=True):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set of shape (n_features, m_train)\n",
    "    Y_train -- training labels of shape (1, m_train)\n",
    "    X_test -- test set of shape (n_features, m_test)\n",
    "    Y_test -- test labels of shape (1, m_test)\n",
    "    num_iterations -- number of iterations for optimization\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    print_cost -- if True, print cost during training\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w, b = initialize_parameters(X_train.shape[0])\n",
    "    \n",
    "    # Gradient descent\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
    "    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "    \n",
    "    print(f\"\\nTrain accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    d = {\n",
    "        \"costs\": costs,\n",
    "        \"Y_prediction_test\": Y_prediction_test,\n",
    "        \"Y_prediction_train\": Y_prediction_train,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_iterations\": num_iterations,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    }\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "print(\"Model function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Train the Model\n",
    "\n",
    "Now let's train our logistic regression model on the cat vs non-cat dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training logistic regression model...\\n\")\n",
    "model_results = model(\n",
    "    train_set_x, \n",
    "    train_set_y, \n",
    "    test_set_x, \n",
    "    test_set_y, \n",
    "    num_iterations=2000, \n",
    "    learning_rate=0.005, \n",
    "    print_cost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Learning Curve\n",
    "\n",
    "The learning curve shows how the cost decreases over iterations:\n",
    "- **Decreasing cost**: Model is learning\n",
    "- **Flat cost**: Model has converged\n",
    "- **Increasing cost**: Learning rate too high or numerical issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "costs = np.squeeze(model_results['costs'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(costs, linewidth=2)\n",
    "plt.ylabel('Cost', fontsize=12)\n",
    "plt.xlabel('Iterations (per 10)', fontsize=12)\n",
    "plt.title(f\"Learning Curve (Learning Rate = {model_results['learning_rate']})\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final cost: {costs[-1]:.6f}\")\n",
    "print(f\"Initial cost: {costs[0]:.6f}\")\n",
    "print(f\"Cost reduction: {((costs[0] - costs[-1]) / costs[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Analyze Model Performance\n",
    "\n",
    "Let's examine prediction examples and compute detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test set\n",
    "def visualize_predictions(X_orig, y_true, y_pred, n_samples=6):\n",
    "    \"\"\"\n",
    "    Visualize predictions vs true labels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        axes[i].imshow(X_orig[i].astype('uint8'))\n",
    "        true_label = \"Cat\" if y_true[0, i] == 1 else \"Non-Cat\"\n",
    "        pred_label = \"Cat\" if y_pred[0, i] == 1 else \"Non-Cat\"\n",
    "        \n",
    "        # Color code: green for correct, red for incorrect\n",
    "        color = 'green' if y_true[0, i] == y_pred[0, i] else 'red'\n",
    "        axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=color, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(\n",
    "    test_set_x_orig, \n",
    "    test_set_y, \n",
    "    model_results['Y_prediction_test'], \n",
    "    n_samples=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Confusion Matrix and Metrics\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- True Positives (TP): Correctly predicted cats\n",
    "- True Negatives (TN): Correctly predicted non-cats\n",
    "- False Positives (FP): Non-cats predicted as cats\n",
    "- False Negatives (FN): Cats predicted as non-cats\n",
    "\n",
    "**Metrics:**\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "- F1-Score = 2 × (Precision × Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix and classification metrics.\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrix': np.array([[TN, FP], [FN, TP]]),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "\n",
    "# Compute metrics for test set\n",
    "metrics = compute_metrics(test_set_y, model_results['Y_prediction_test'])\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(f\"Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"Precision: {metrics['precision']*100:.2f}%\")\n",
    "print(f\"Recall:    {metrics['recall']*100:.2f}%\")\n",
    "print(f\"F1-Score:  {metrics['f1_score']*100:.2f}%\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Cat', 'Cat'], yticklabels=['Non-Cat', 'Cat'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Experiment with Different Learning Rates\n",
    "\n",
    "Let's see how different learning rates affect model performance and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05]\n",
    "models = {}\n",
    "\n",
    "print(\"Training models with different learning rates...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    models[lr] = model(\n",
    "        train_set_x, \n",
    "        train_set_y, \n",
    "        test_set_x, \n",
    "        test_set_y, \n",
    "        num_iterations=1500, \n",
    "        learning_rate=lr,\n",
    "        print_cost=False\n",
    "    )\n",
    "\n",
    "# Plot learning curves for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for lr in learning_rates:\n",
    "    costs = np.squeeze(models[lr]['costs'])\n",
    "    plt.plot(costs, label=f'LR = {lr}')\n",
    "\n",
    "plt.ylabel('Cost', fontsize=12)\n",
    "plt.xlabel('Iterations (per 10)', fontsize=12)\n",
    "plt.title('Learning Curves for Different Learning Rates', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'Learning Rate':<15} {'Train Accuracy':<20} {'Test Accuracy':<20}\")\n",
    "print(\"=\"*70)\n",
    "for lr in learning_rates:\n",
    "    train_acc = models[lr]['train_accuracy']\n",
    "    test_acc = models[lr]['test_accuracy']\n",
    "    print(f\"{lr:<15} {train_acc:<20.2f}% {test_acc:<20.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Implemented:\n",
    "1. **Data Creation**: Generated synthetic cat/non-cat images\n",
    "2. **Preprocessing**: Flattened and normalized images\n",
    "3. **Model Components**:\n",
    "   - Sigmoid activation function\n",
    "   - Forward propagation (prediction)\n",
    "   - Cost function (binary cross-entropy)\n",
    "   - Backward propagation (gradients)\n",
    "   - Gradient descent optimization\n",
    "4. **Evaluation**: Accuracy, precision, recall, F1-score\n",
    "\n",
    "### Mathematical Foundation:\n",
    "- **Model**: ŷ = σ(w^T x + b)\n",
    "- **Cost**: J = -1/m ∑[y log(ŷ) + (1-y) log(1-ŷ)]\n",
    "- **Gradients**: ∂J/∂w, ∂J/∂b\n",
    "- **Update**: w = w - α ∂J/∂w\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Vectorization**: Efficient computation using NumPy\n",
    "2. **Feature Engineering**: Flattening images into vectors\n",
    "3. **Normalization**: Scaling features for better convergence\n",
    "4. **Hyperparameters**: Learning rate, iterations\n",
    "5. **Evaluation**: Multiple metrics for comprehensive assessment\n",
    "\n",
    "### Limitations:\n",
    "- Linear decision boundary (can't learn complex patterns)\n",
    "- No feature learning (uses raw pixels)\n",
    "- Sensitive to learning rate\n",
    "\n",
    "### Next Steps:\n",
    "- Try neural networks for better feature learning\n",
    "- Experiment with regularization (L2/L1)\n",
    "- Use real cat image datasets\n",
    "- Implement mini-batch gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
