{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5-Layer Neural Network (Synced)\n",
        "\n",
        "This notebook runs the code from `five_layer_nn.py` so it stays in sync with the script.\n",
        "\n",
        "- Normalization helpers\n",
        "- Initialization, regularization, dropout\n",
        "- Optimizers, mini-batches, learning rate decay\n",
        "- Gradient checking and experiment comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_repo_file(filename, max_levels=6):\n",
        "    current = Path.cwd().resolve()\n",
        "    for _ in range(max_levels + 1):\n",
        "        candidate = current / filename\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "        current = current.parent\n",
        "    raise FileNotFoundError(f\"Could not find {filename} from {Path.cwd()}\")\n",
        "\n",
        "\n",
        "script_path = find_repo_file(\"five_layer_nn.py\")\n",
        "print(f\"Loading: {script_path}\")\n",
        "code = script_path.read_text()\n",
        "exec(code, globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5-Layer Neural Network\n",
        "\n",
        "A neural network with configurable layer sizes.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input (X) -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> Layer5 (Output)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FiveLayerNN:\n",
        "    \"\"\"\n",
        "    5-Layer Neural Network with configurable layer sizes.\n",
        "\n",
        "    Architecture:\n",
        "    Input (X) -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> Layer5 (Output)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_dims, learning_rate=0.01, initialization='he',\n",
        "                 lambd=0.0, keep_prob=1.0, optimizer='gd', beta=0.0,\n",
        "                 beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
        "                 decay_rate=0.0, time_interval=1000):\n",
        "        \"\"\"\n",
        "        Initialize the neural network.\n",
        "\n",
        "        Args:\n",
        "            layer_dims: list of 6 integers [n_x, n1, n2, n3, n4, n5]\n",
        "                        where n_x is input size, n5 is output size\n",
        "            learning_rate: learning rate for gradient descent\n",
        "            initialization: weight initialization method ('zeros', 'random', 'xavier', 'he')\n",
        "            lambd: L2 regularization hyperparameter (0 = no regularization)\n",
        "            keep_prob: dropout keep probability (1.0 = no dropout)\n",
        "            optimizer: optimization algorithm ('gd', 'momentum', 'rmsprop', 'adam')\n",
        "            beta: momentum hyperparameter for 'momentum' optimizer (typical: 0.9)\n",
        "            beta1: exponential decay rate for first moment (Adam), default 0.9\n",
        "            beta2: exponential decay rate for second moment (Adam/RMSprop), default 0.999\n",
        "            epsilon: small constant for numerical stability (Adam/RMSprop), default 1e-8\n",
        "            decay_rate: learning rate decay rate (0 = no decay)\n",
        "            time_interval: number of epochs between learning rate updates (for scheduled decay)\n",
        "        \"\"\"\n",
        "        self.layer_dims = layer_dims\n",
        "        self.learning_rate = learning_rate\n",
        "        self.learning_rate0 = learning_rate  # Store initial learning rate for decay\n",
        "        self.initialization = initialization\n",
        "        self.lambd = lambd\n",
        "        self.keep_prob = keep_prob\n",
        "        self.optimizer = optimizer.lower()\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "        self.time_interval = time_interval\n",
        "        self.t = 0  # Adam iteration counter for bias correction\n",
        "        self.parameters = {}\n",
        "        self.cache = {}\n",
        "        self.gradients = {}\n",
        "        self.dropout_masks = {}\n",
        "        self.velocity = {}  # First moment (momentum/Adam)\n",
        "        self.squared = {}   # Second moment (RMSprop/Adam)\n",
        "\n",
        "        self._initialize_parameters()\n",
        "        self._initialize_optimizer()\n",
        "\n",
        "    # ==================== Normalization Methods ====================\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_minmax(X, X_min=None, X_max=None):\n",
        "        \"\"\"\n",
        "        Min-Max normalization: scales features to range [0, 1].\n",
        "\n",
        "        Formula: X_norm = (X - X_min) / (X_max - X_min)\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_features, m_examples)\n",
        "            X_min: Minimum values per feature. If None, computed from X.\n",
        "            X_max: Maximum values per feature. If None, computed from X.\n",
        "\n",
        "        Returns:\n",
        "            X_norm: Normalized data\n",
        "            X_min: Minimum values (save for test set normalization)\n",
        "            X_max: Maximum values (save for test set normalization)\n",
        "        \"\"\"\n",
        "        if X_min is None:\n",
        "            X_min = np.min(X, axis=1, keepdims=True)\n",
        "        if X_max is None:\n",
        "            X_max = np.max(X, axis=1, keepdims=True)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        range_vals = X_max - X_min\n",
        "        range_vals[range_vals == 0] = 1\n",
        "\n",
        "        X_norm = (X - X_min) / range_vals\n",
        "        return X_norm, X_min, X_max\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_zscore(X, mean=None, std=None):\n",
        "        \"\"\"\n",
        "        Z-score standardization: transforms to mean=0, std=1.\n",
        "\n",
        "        Formula: X_norm = (X - mean) / std\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_features, m_examples)\n",
        "            mean: Mean per feature. If None, computed from X.\n",
        "            std: Std deviation per feature. If None, computed from X.\n",
        "\n",
        "        Returns:\n",
        "            X_norm: Normalized data\n",
        "            mean: Mean values (save for test set normalization)\n",
        "            std: Std values (save for test set normalization)\n",
        "        \"\"\"\n",
        "        if mean is None:\n",
        "            mean = np.mean(X, axis=1, keepdims=True)\n",
        "        if std is None:\n",
        "            std = np.std(X, axis=1, keepdims=True)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        std[std == 0] = 1\n",
        "\n",
        "        X_norm = (X - mean) / std\n",
        "        return X_norm, mean, std\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_mean(X, mean=None):\n",
        "        \"\"\"\n",
        "        Mean normalization: centers data around zero.\n",
        "\n",
        "        Formula: X_norm = X - mean\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_features, m_examples)\n",
        "            mean: Mean per feature. If None, computed from X.\n",
        "\n",
        "        Returns:\n",
        "            X_norm: Normalized data\n",
        "            mean: Mean values (save for test set normalization)\n",
        "        \"\"\"\n",
        "        if mean is None:\n",
        "            mean = np.mean(X, axis=1, keepdims=True)\n",
        "\n",
        "        X_norm = X - mean\n",
        "        return X_norm, mean\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_l2(X):\n",
        "        \"\"\"\n",
        "        L2 normalization: scales each sample to unit norm.\n",
        "\n",
        "        Formula: X_norm = X / ||X||_2\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_features, m_examples)\n",
        "\n",
        "        Returns:\n",
        "            X_norm: Normalized data (each column has L2 norm = 1)\n",
        "        \"\"\"\n",
        "        norms = np.linalg.norm(X, axis=0, keepdims=True)\n",
        "        norms[norms == 0] = 1  # Avoid division by zero\n",
        "        X_norm = X / norms\n",
        "        return X_norm\n",
        "\n",
        "    # ==================== Initialization Methods ====================\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        \"\"\"Initialize W and b using the specified initialization method.\"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for l in range(1, 6):\n",
        "            if self.initialization == 'zeros':\n",
        "                # Zero initialization (bad - causes symmetry problem)\n",
        "                self.parameters[f'W{l}'] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n",
        "\n",
        "            elif self.initialization == 'random':\n",
        "                # Random initialization with large values (can cause vanishing/exploding gradients)\n",
        "                self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * 10\n",
        "\n",
        "            elif self.initialization == 'xavier':\n",
        "                # Xavier/Glorot initialization (good for tanh/sigmoid)\n",
        "                # W = randn * sqrt(1 / n_prev) or sqrt(2 / (n_prev + n_curr))\n",
        "                self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(1 / self.layer_dims[l-1])\n",
        "\n",
        "            elif self.initialization == 'he':\n",
        "                # He initialization (good for ReLU)\n",
        "                # W = randn * sqrt(2 / n_prev)\n",
        "                self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2 / self.layer_dims[l-1])\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown initialization: {self.initialization}. Use 'zeros', 'random', 'xavier', or 'he'\")\n",
        "\n",
        "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
        "\n",
        "    def _initialize_optimizer(self):\n",
        "        \"\"\"\n",
        "        Initialize optimizer state variables.\n",
        "\n",
        "        For Momentum:\n",
        "            - velocity (v): exponentially weighted average of gradients\n",
        "\n",
        "        For RMSprop:\n",
        "            - squared (s): exponentially weighted average of squared gradients\n",
        "\n",
        "        For Adam:\n",
        "            - velocity (v): first moment estimate (like momentum)\n",
        "            - squared (s): second moment estimate (like RMSprop)\n",
        "\n",
        "        All initialized to zeros.\n",
        "        \"\"\"\n",
        "        for l in range(1, 6):\n",
        "            # First moment (velocity) - used by momentum and Adam\n",
        "            self.velocity[f'dW{l}'] = np.zeros_like(self.parameters[f'W{l}'])\n",
        "            self.velocity[f'db{l}'] = np.zeros_like(self.parameters[f'b{l}'])\n",
        "\n",
        "            # Second moment (squared gradients) - used by RMSprop and Adam\n",
        "            self.squared[f'dW{l}'] = np.zeros_like(self.parameters[f'W{l}'])\n",
        "            self.squared[f'db{l}'] = np.zeros_like(self.parameters[f'b{l}'])\n",
        "\n",
        "    # ==================== Activation Functions ====================\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        s = self.sigmoid(Z)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "\n",
        "    # ==================== Forward Propagation ====================\n",
        "\n",
        "    def forward_propagation(self, X, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass through all 5 layers.\n",
        "\n",
        "        Layers 1-4: ReLU activation (with optional dropout)\n",
        "        Layer 5: Sigmoid activation (binary) or Softmax (multiclass)\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "            training: If True, apply dropout. If False (inference), no dropout.\n",
        "        \"\"\"\n",
        "        self.cache['A0'] = X\n",
        "        A = X\n",
        "\n",
        "        # Layers 1-4: Linear -> ReLU -> Dropout (optional)\n",
        "        for l in range(1, 5):\n",
        "            W = self.parameters[f'W{l}']\n",
        "            b = self.parameters[f'b{l}']\n",
        "\n",
        "            Z = np.dot(W, A) + b\n",
        "            A = self.relu(Z)\n",
        "\n",
        "            # Apply dropout during training (not on output layer)\n",
        "            if training and self.keep_prob < 1.0:\n",
        "                D = np.random.rand(A.shape[0], A.shape[1])\n",
        "                D = (D < self.keep_prob).astype(int)\n",
        "                A = A * D\n",
        "                A = A / self.keep_prob  # Inverted dropout scaling\n",
        "                self.dropout_masks[f'D{l}'] = D\n",
        "\n",
        "            self.cache[f'Z{l}'] = Z\n",
        "            self.cache[f'A{l}'] = A\n",
        "\n",
        "        # Layer 5: Linear -> Sigmoid (output) - no dropout on output layer\n",
        "        W5 = self.parameters['W5']\n",
        "        b5 = self.parameters['b5']\n",
        "\n",
        "        Z5 = np.dot(W5, A) + b5\n",
        "        A5 = self.sigmoid(Z5)\n",
        "\n",
        "        self.cache['Z5'] = Z5\n",
        "        self.cache['A5'] = A5\n",
        "\n",
        "        return A5\n",
        "\n",
        "    # ==================== Backward Propagation ====================\n",
        "\n",
        "    def backward_propagation(self, Y):\n",
        "        \"\"\"\n",
        "        Backward pass computing dZ, dW, db for all 5 layers.\n",
        "        Supports L2 regularization and dropout.\n",
        "\n",
        "        Layer 5 (Output):\n",
        "            dZ\u0015 = A\u0015 - Y\n",
        "            dW\u0015 = (1/m) * dZ\u0015 \u0000 (A\u0014)\u0000 + (\u0003bb/m) * W\u0015\n",
        "            db\u0015 = (1/m) * \u0006 dZ\u0015\n",
        "\n",
        "        Layers 4-1:\n",
        "            dA\u0014 = (W\u0015\u0000)\u0000 \u0000 dZ\u0015\n",
        "            dA\u0014 = dA\u0014 * D\u0014 / keep_prob  (if dropout)\n",
        "            dZ\u0014 = dA\u0014 * g'\u0014(Z\u0014)\n",
        "            dW\u0014 = (1/m) * dZ\u0014 \u0000 (A\u0013)\u0000 + (\u0003bb/m) * W\u0014\n",
        "            db\u0014 = (1/m) * \u0006 dZ\u0014\n",
        "        \"\"\"\n",
        "        m = Y.shape[1]\n",
        "\n",
        "        # ============ Layer 5 (Output Layer) ============\n",
        "        dZ5 = self.cache['A5'] - Y\n",
        "        dW5 = (1/m) * np.dot(dZ5, self.cache['A4'].T)\n",
        "        # Add L2 regularization gradient\n",
        "        if self.lambd > 0:\n",
        "            dW5 += (self.lambd / m) * self.parameters['W5']\n",
        "        db5 = (1/m) * np.sum(dZ5, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients['dZ5'] = dZ5\n",
        "        self.gradients['dW5'] = dW5\n",
        "        self.gradients['db5'] = db5\n",
        "\n",
        "        # ============ Layer 4 ============\n",
        "        dA4 = np.dot(self.parameters['W5'].T, dZ5)\n",
        "        # Apply dropout mask if dropout was used\n",
        "        if self.keep_prob < 1.0 and f'D4' in self.dropout_masks:\n",
        "            dA4 = dA4 * self.dropout_masks['D4']\n",
        "            dA4 = dA4 / self.keep_prob\n",
        "        dZ4 = dA4 * self.relu_derivative(self.cache['Z4'])\n",
        "        dW4 = (1/m) * np.dot(dZ4, self.cache['A3'].T)\n",
        "        if self.lambd > 0:\n",
        "            dW4 += (self.lambd / m) * self.parameters['W4']\n",
        "        db4 = (1/m) * np.sum(dZ4, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients['dZ4'] = dZ4\n",
        "        self.gradients['dW4'] = dW4\n",
        "        self.gradients['db4'] = db4\n",
        "\n",
        "        # ============ Layer 3 ============\n",
        "        dA3 = np.dot(self.parameters['W4'].T, dZ4)\n",
        "        if self.keep_prob < 1.0 and f'D3' in self.dropout_masks:\n",
        "            dA3 = dA3 * self.dropout_masks['D3']\n",
        "            dA3 = dA3 / self.keep_prob\n",
        "        dZ3 = dA3 * self.relu_derivative(self.cache['Z3'])\n",
        "        dW3 = (1/m) * np.dot(dZ3, self.cache['A2'].T)\n",
        "        if self.lambd > 0:\n",
        "            dW3 += (self.lambd / m) * self.parameters['W3']\n",
        "        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients['dZ3'] = dZ3\n",
        "        self.gradients['dW3'] = dW3\n",
        "        self.gradients['db3'] = db3\n",
        "\n",
        "        # ============ Layer 2 ============\n",
        "        dA2 = np.dot(self.parameters['W3'].T, dZ3)\n",
        "        if self.keep_prob < 1.0 and f'D2' in self.dropout_masks:\n",
        "            dA2 = dA2 * self.dropout_masks['D2']\n",
        "            dA2 = dA2 / self.keep_prob\n",
        "        dZ2 = dA2 * self.relu_derivative(self.cache['Z2'])\n",
        "        dW2 = (1/m) * np.dot(dZ2, self.cache['A1'].T)\n",
        "        if self.lambd > 0:\n",
        "            dW2 += (self.lambd / m) * self.parameters['W2']\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients['dZ2'] = dZ2\n",
        "        self.gradients['dW2'] = dW2\n",
        "        self.gradients['db2'] = db2\n",
        "\n",
        "        # ============ Layer 1 ============\n",
        "        dA1 = np.dot(self.parameters['W2'].T, dZ2)\n",
        "        if self.keep_prob < 1.0 and f'D1' in self.dropout_masks:\n",
        "            dA1 = dA1 * self.dropout_masks['D1']\n",
        "            dA1 = dA1 / self.keep_prob\n",
        "        dZ1 = dA1 * self.relu_derivative(self.cache['Z1'])\n",
        "        dW1 = (1/m) * np.dot(dZ1, self.cache['A0'].T)\n",
        "        if self.lambd > 0:\n",
        "            dW1 += (self.lambd / m) * self.parameters['W1']\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients['dZ1'] = dZ1\n",
        "        self.gradients['dW1'] = dW1\n",
        "        self.gradients['db1'] = db1\n",
        "\n",
        "    # ==================== Update Parameters ====================\n",
        "\n",
        "    def update_parameters(self):\n",
        "        \"\"\"\n",
        "        Update W and b using the specified optimizer.\n",
        "\n",
        "        Supported optimizers:\n",
        "        1. 'gd' - Standard Gradient Descent:\n",
        "            W = W - \u0003b1 * dW\n",
        "\n",
        "        2. 'momentum' - Gradient Descent with Momentum:\n",
        "            v = \u0003b2 * v + (1-\u0003b2) * dW\n",
        "            W = W - \u0003b1 * v\n",
        "            Smooths gradients, accelerates in consistent directions.\n",
        "\n",
        "        3. 'rmsprop' - Root Mean Square Propagation:\n",
        "            s = \u0003b2\n",
        " * s + (1-\u0003b2\n",
        ") * dW\n",
        "            W = W - \u0003b1 * dW / (\n",
        "s + \u0003b5)\n",
        "            Adapts learning rate per parameter, divides by running average of gradient magnitudes.\n",
        "\n",
        "        4. 'adam' - Adaptive Moment Estimation:\n",
        "            v = \u0003b2\n",
        " * v + (1-\u0003b2\n",
        ") * dW     (first moment - momentum)\n",
        "            s = \u0003b2\n",
        " * s + (1-\u0003b2\n",
        ") * dW\n",
        "    (second moment - RMSprop)\n",
        "            v_corrected = v / (1 - \u0003b2\n",
        "\u0011)  (bias correction)\n",
        "            s_corrected = s / (1 - \u0003b2\n",
        "\u0011)\n",
        "            W = W - \u0003b1 * v_corrected / (\n",
        "s_corrected + \u0003b5)\n",
        "            Combines benefits of momentum and RMSprop with bias correction.\n",
        "        \"\"\"\n",
        "        if self.optimizer == 'adam':\n",
        "            self.t += 1  # Increment timestep for bias correction\n",
        "\n",
        "        for l in range(1, 6):\n",
        "            dW = self.gradients[f'dW{l}']\n",
        "            db = self.gradients[f'db{l}']\n",
        "\n",
        "            if self.optimizer == 'gd':\n",
        "                # Standard gradient descent\n",
        "                self.parameters[f'W{l}'] -= self.learning_rate * dW\n",
        "                self.parameters[f'b{l}'] -= self.learning_rate * db\n",
        "\n",
        "            elif self.optimizer == 'momentum':\n",
        "                # Momentum: v = \u0003b2*v + (1-\u0003b2)*dW, W = W - \u0003b1*v\n",
        "                self.velocity[f'dW{l}'] = self.beta * self.velocity[f'dW{l}'] + (1 - self.beta) * dW\n",
        "                self.velocity[f'db{l}'] = self.beta * self.velocity[f'db{l}'] + (1 - self.beta) * db\n",
        "\n",
        "                self.parameters[f'W{l}'] -= self.learning_rate * self.velocity[f'dW{l}']\n",
        "                self.parameters[f'b{l}'] -= self.learning_rate * self.velocity[f'db{l}']\n",
        "\n",
        "            elif self.optimizer == 'rmsprop':\n",
        "                # RMSprop: s = \u0003b2\n",
        "*s + (1-\u0003b2\n",
        ")*dW\n",
        ", W = W - \u0003b1*dW/(\n",
        "s + \u0003b5)\n",
        "                self.squared[f'dW{l}'] = self.beta2 * self.squared[f'dW{l}'] + (1 - self.beta2) * np.square(dW)\n",
        "                self.squared[f'db{l}'] = self.beta2 * self.squared[f'db{l}'] + (1 - self.beta2) * np.square(db)\n",
        "\n",
        "                self.parameters[f'W{l}'] -= self.learning_rate * dW / (np.sqrt(self.squared[f'dW{l}']) + self.epsilon)\n",
        "                self.parameters[f'b{l}'] -= self.learning_rate * db / (np.sqrt(self.squared[f'db{l}']) + self.epsilon)\n",
        "\n",
        "            elif self.optimizer == 'adam':\n",
        "                # Adam: combines momentum and RMSprop with bias correction\n",
        "                # Update first moment (momentum)\n",
        "                self.velocity[f'dW{l}'] = self.beta1 * self.velocity[f'dW{l}'] + (1 - self.beta1) * dW\n",
        "                self.velocity[f'db{l}'] = self.beta1 * self.velocity[f'db{l}'] + (1 - self.beta1) * db\n",
        "\n",
        "                # Update second moment (RMSprop)\n",
        "                self.squared[f'dW{l}'] = self.beta2 * self.squared[f'dW{l}'] + (1 - self.beta2) * np.square(dW)\n",
        "                self.squared[f'db{l}'] = self.beta2 * self.squared[f'db{l}'] + (1 - self.beta2) * np.square(db)\n",
        "\n",
        "                # Bias correction\n",
        "                v_dW_corrected = self.velocity[f'dW{l}'] / (1 - self.beta1 ** self.t)\n",
        "                v_db_corrected = self.velocity[f'db{l}'] / (1 - self.beta1 ** self.t)\n",
        "                s_dW_corrected = self.squared[f'dW{l}'] / (1 - self.beta2 ** self.t)\n",
        "                s_db_corrected = self.squared[f'db{l}'] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                # Update parameters\n",
        "                self.parameters[f'W{l}'] -= self.learning_rate * v_dW_corrected / (np.sqrt(s_dW_corrected) + self.epsilon)\n",
        "                self.parameters[f'b{l}'] -= self.learning_rate * v_db_corrected / (np.sqrt(s_db_corrected) + self.epsilon)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown optimizer: {self.optimizer}. Use 'gd', 'momentum', 'rmsprop', or 'adam'\")\n",
        "\n",
        "    # ==================== Learning Rate Decay ====================\n",
        "\n",
        "    def update_learning_rate(self, epoch_num):\n",
        "        \"\"\"\n",
        "        Update learning rate using inverse time decay formula.\n",
        "\n",
        "        Formula: learning_rate = learning_rate0 / (1 + decay_rate * epoch_num)\n",
        "\n",
        "        Args:\n",
        "            epoch_num: Current epoch number\n",
        "\n",
        "        Returns:\n",
        "            learning_rate: Updated learning rate\n",
        "        \"\"\"\n",
        "        if self.decay_rate > 0:\n",
        "            self.learning_rate = self.learning_rate0 / (1 + self.decay_rate * epoch_num)\n",
        "        return self.learning_rate\n",
        "\n",
        "    def schedule_lr_decay(self, epoch_num):\n",
        "        \"\"\"\n",
        "        Update learning rate using scheduled decay (step decay).\n",
        "\n",
        "        The learning rate is reduced at fixed intervals (time_interval epochs).\n",
        "\n",
        "        Formula: learning_rate = learning_rate0 / (1 + decay_rate * floor(epoch_num / time_interval))\n",
        "\n",
        "        Args:\n",
        "            epoch_num: Current epoch number\n",
        "\n",
        "        Returns:\n",
        "            learning_rate: Updated learning rate\n",
        "        \"\"\"\n",
        "        if self.decay_rate > 0:\n",
        "            self.learning_rate = self.learning_rate0 / (1 + self.decay_rate * np.floor(epoch_num / self.time_interval))\n",
        "        return self.learning_rate\n",
        "\n",
        "    # ==================== Loss Function ====================\n",
        "\n",
        "    def compute_loss(self, Y):\n",
        "        \"\"\"Binary cross-entropy loss with optional L2 regularization.\"\"\"\n",
        "        m = Y.shape[1]\n",
        "        A5 = self.cache['A5']\n",
        "\n",
        "        # Clip to prevent log(0)\n",
        "        A5 = np.clip(A5, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        # Cross-entropy loss\n",
        "        cross_entropy_loss = -(1/m) * np.sum(Y * np.log(A5) + (1 - Y) * np.log(1 - A5))\n",
        "\n",
        "        # L2 regularization cost\n",
        "        L2_cost = 0\n",
        "        if self.lambd > 0:\n",
        "            for l in range(1, 6):\n",
        "                L2_cost += np.sum(np.square(self.parameters[f'W{l}']))\n",
        "            L2_cost = (self.lambd / (2 * m)) * L2_cost\n",
        "\n",
        "        return cross_entropy_loss + L2_cost\n",
        "\n",
        "    # ==================== Training ====================\n",
        "\n",
        "    @staticmethod\n",
        "    def create_mini_batches(X, Y, mini_batch_size, seed=None):\n",
        "        \"\"\"\n",
        "        Create mini-batches from the training data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_x, m)\n",
        "            Y: Labels, shape (1, m)\n",
        "            mini_batch_size: Size of each mini-batch\n",
        "            seed: Random seed for shuffling (optional)\n",
        "\n",
        "        Returns:\n",
        "            mini_batches: List of (mini_batch_X, mini_batch_Y) tuples\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        m = X.shape[1]\n",
        "        mini_batches = []\n",
        "\n",
        "        # Shuffle training data\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[:, permutation]\n",
        "        Y_shuffled = Y[:, permutation]\n",
        "\n",
        "        # Partition into mini-batches\n",
        "        num_complete_batches = m // mini_batch_size\n",
        "\n",
        "        for k in range(num_complete_batches):\n",
        "            start = k * mini_batch_size\n",
        "            end = (k + 1) * mini_batch_size\n",
        "            mini_batch_X = X_shuffled[:, start:end]\n",
        "            mini_batch_Y = Y_shuffled[:, start:end]\n",
        "            mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "        # Handle the remaining examples (last mini-batch)\n",
        "        if m % mini_batch_size != 0:\n",
        "            mini_batch_X = X_shuffled[:, num_complete_batches * mini_batch_size:]\n",
        "            mini_batch_Y = Y_shuffled[:, num_complete_batches * mini_batch_size:]\n",
        "            mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "        return mini_batches\n",
        "\n",
        "    def train(self, X, Y, epochs=1000, print_loss=True, mini_batch_size=None, decay_type='scheduled'):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_x, m)\n",
        "            Y: Labels, shape (1, m)\n",
        "            epochs: Number of training epochs\n",
        "            print_loss: Whether to print loss every 100 epochs\n",
        "            mini_batch_size: Size of mini-batches. If None, use full batch (batch GD).\n",
        "                            Common values: 32, 64, 128, 256\n",
        "            decay_type: Type of learning rate decay ('continuous' or 'scheduled')\n",
        "                       - 'continuous': Decay every epoch using update_learning_rate()\n",
        "                       - 'scheduled': Decay at intervals using schedule_lr_decay()\n",
        "\n",
        "        Returns:\n",
        "            losses: List of loss values (one per epoch)\n",
        "\n",
        "        Mini-batch Gradient Descent:\n",
        "        - mini_batch_size = m: Batch gradient descent (smooth but slow)\n",
        "        - mini_batch_size = 1: Stochastic gradient descent (noisy but fast)\n",
        "        - mini_batch_size = 32-256: Mini-batch (good balance)\n",
        "        \"\"\"\n",
        "        losses = []\n",
        "        learning_rates = []  # Track learning rate over epochs\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # Reset Adam timestep counter at start of training\n",
        "        self.t = 0\n",
        "\n",
        "        # Reset learning rate to initial value\n",
        "        self.learning_rate = self.learning_rate0\n",
        "\n",
        "        # Use full batch if mini_batch_size not specified\n",
        "        if mini_batch_size is None or mini_batch_size >= m:\n",
        "            mini_batch_size = m\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_cost = 0\n",
        "\n",
        "            # Apply learning rate decay at the start of each epoch\n",
        "            if self.decay_rate > 0:\n",
        "                if decay_type == 'continuous':\n",
        "                    self.update_learning_rate(epoch)\n",
        "                else:  # 'scheduled'\n",
        "                    self.schedule_lr_decay(epoch)\n",
        "\n",
        "            learning_rates.append(self.learning_rate)\n",
        "\n",
        "            # Create mini-batches (shuffle each epoch for mini-batch GD)\n",
        "            if mini_batch_size < m:\n",
        "                mini_batches = self.create_mini_batches(X, Y, mini_batch_size, seed=epoch)\n",
        "            else:\n",
        "                mini_batches = [(X, Y)]\n",
        "\n",
        "            num_batches = len(mini_batches)\n",
        "\n",
        "            for mini_batch_X, mini_batch_Y in mini_batches:\n",
        "                # Forward propagation (with dropout if enabled)\n",
        "                self.forward_propagation(mini_batch_X, training=True)\n",
        "\n",
        "                # Compute loss for this mini-batch\n",
        "                batch_cost = self.compute_loss(mini_batch_Y)\n",
        "                epoch_cost += batch_cost\n",
        "\n",
        "                # Backward propagation\n",
        "                self.backward_propagation(mini_batch_Y)\n",
        "\n",
        "                # Update parameters\n",
        "                self.update_parameters()\n",
        "\n",
        "            # Average cost over all mini-batches\n",
        "            epoch_cost /= num_batches\n",
        "            losses.append(epoch_cost)\n",
        "\n",
        "            if print_loss and epoch % 100 == 0:\n",
        "                lr_info = f\" | LR: {self.learning_rate:.6f}\" if self.decay_rate > 0 else \"\"\n",
        "                print(f\"Epoch {epoch:4d} | Loss: {epoch_cost:.6f}{lr_info}\")\n",
        "\n",
        "        # Store learning rate history for analysis\n",
        "        self.learning_rate_history = learning_rates\n",
        "\n",
        "        return losses\n",
        "\n",
        "    # ==================== Prediction ====================\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions (no dropout during inference).\"\"\"\n",
        "        A5 = self.forward_propagation(X, training=False)\n",
        "        return (A5 > 0.5).astype(int)\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"Compute accuracy.\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return np.mean(predictions == Y) * 100\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return model configuration as a string.\"\"\"\n",
        "        config = f\"init={self.initialization}\"\n",
        "        if self.lambd > 0:\n",
        "            config += f\", L2={self.lambd}\"\n",
        "        if self.keep_prob < 1.0:\n",
        "            config += f\", dropout={1-self.keep_prob:.1f}\"\n",
        "        if self.optimizer != 'gd':\n",
        "            config += f\", optimizer={self.optimizer}\"\n",
        "            if self.optimizer == 'momentum':\n",
        "                config += f\"(\u0003b2={self.beta})\"\n",
        "            elif self.optimizer == 'rmsprop':\n",
        "                config += f\"(\u0003b2\n",
        "={self.beta2})\"\n",
        "            elif self.optimizer == 'adam':\n",
        "                config += f\"(\u0003b2\n",
        "={self.beta1},\u0003b2\n",
        "={self.beta2})\"\n",
        "        if self.decay_rate > 0:\n",
        "            config += f\", lr_decay={self.decay_rate}(interval={self.time_interval})\"\n",
        "        return config\n",
        "\n",
        "    # ==================== Gradient Checking ====================\n",
        "\n",
        "    def _parameters_to_vector(self):\n",
        "        \"\"\"Flatten all parameters into a single vector.\"\"\"\n",
        "        params = []\n",
        "        for l in range(1, 6):\n",
        "            params.append(self.parameters[f'W{l}'].flatten())\n",
        "            params.append(self.parameters[f'b{l}'].flatten())\n",
        "        return np.concatenate(params)\n",
        "\n",
        "    def _vector_to_parameters(self, theta):\n",
        "        \"\"\"Reshape vector back into parameters dictionary.\"\"\"\n",
        "        parameters = {}\n",
        "        idx = 0\n",
        "        for l in range(1, 6):\n",
        "            W_shape = (self.layer_dims[l], self.layer_dims[l-1])\n",
        "            b_shape = (self.layer_dims[l], 1)\n",
        "\n",
        "            W_size = W_shape[0] * W_shape[1]\n",
        "            b_size = b_shape[0]\n",
        "\n",
        "            parameters[f'W{l}'] = theta[idx:idx + W_size].reshape(W_shape)\n",
        "            idx += W_size\n",
        "            parameters[f'b{l}'] = theta[idx:idx + b_size].reshape(b_shape)\n",
        "            idx += b_size\n",
        "\n",
        "        return parameters\n",
        "\n",
        "    def _gradients_to_vector(self):\n",
        "        \"\"\"Flatten all gradients into a single vector.\"\"\"\n",
        "        grads = []\n",
        "        for l in range(1, 6):\n",
        "            grads.append(self.gradients[f'dW{l}'].flatten())\n",
        "            grads.append(self.gradients[f'db{l}'].flatten())\n",
        "        return np.concatenate(grads)\n",
        "\n",
        "    def gradient_check(self, X, Y, epsilon=1e-7):\n",
        "        \"\"\"\n",
        "        Perform gradient checking to verify backpropagation.\n",
        "\n",
        "        Compares analytical gradients (from backprop) with numerical gradients\n",
        "        (using two-sided finite difference approximation).\n",
        "\n",
        "        IMPORTANT: Dropout must be disabled during gradient checking because\n",
        "        random masks would be regenerated on each forward pass, causing\n",
        "        inconsistent loss values and incorrect numerical gradients.\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_x, m)\n",
        "            Y: Labels, shape (1, m)\n",
        "            epsilon: Small perturbation for numerical gradient\n",
        "\n",
        "        Returns:\n",
        "            difference: Relative difference between analytical and numerical gradients\n",
        "                       (should be < 1e-7 if backprop is correct)\n",
        "        \"\"\"\n",
        "        # Save original keep_prob and temporarily disable dropout for gradient checking\n",
        "        original_keep_prob = self.keep_prob\n",
        "        self.keep_prob = 1.0  # Disable dropout\n",
        "\n",
        "        # Compute analytical gradients via backprop (no dropout)\n",
        "        self.forward_propagation(X, training=False)\n",
        "        self.backward_propagation(Y)\n",
        "        analytical_grads = self._gradients_to_vector()\n",
        "\n",
        "        # Store original parameters\n",
        "        original_params = self._parameters_to_vector()\n",
        "        num_parameters = len(original_params)\n",
        "\n",
        "        # Compute numerical gradients\n",
        "        numerical_grads = np.zeros(num_parameters)\n",
        "\n",
        "        for i in range(num_parameters):\n",
        "            # Compute J(theta + epsilon)\n",
        "            theta_plus = original_params.copy()\n",
        "            theta_plus[i] += epsilon\n",
        "            self.parameters = self._vector_to_parameters(theta_plus)\n",
        "            self.forward_propagation(X, training=False)\n",
        "            J_plus = self.compute_loss(Y)\n",
        "\n",
        "            # Compute J(theta - epsilon)\n",
        "            theta_minus = original_params.copy()\n",
        "            theta_minus[i] -= epsilon\n",
        "            self.parameters = self._vector_to_parameters(theta_minus)\n",
        "            self.forward_propagation(X, training=False)\n",
        "            J_minus = self.compute_loss(Y)\n",
        "\n",
        "            # Two-sided numerical gradient\n",
        "            numerical_grads[i] = (J_plus - J_minus) / (2 * epsilon)\n",
        "\n",
        "        # Restore original parameters and keep_prob\n",
        "        self.parameters = self._vector_to_parameters(original_params)\n",
        "        self.keep_prob = original_keep_prob  # Restore dropout setting\n",
        "\n",
        "        # Compute relative difference\n",
        "        numerator = np.linalg.norm(analytical_grads - numerical_grads)\n",
        "        denominator = np.linalg.norm(analytical_grads) + np.linalg.norm(numerical_grads)\n",
        "        difference = numerator / (denominator + 1e-8)\n",
        "\n",
        "        return difference, analytical_grads, numerical_grads\n",
        "\n",
        "\n",
        "# ==================== Demo ====================\n",
        "\n",
        "def compare_initializations(X_train, Y_train, X_test, Y_test, layer_dims, epochs=1500):\n",
        "    \"\"\"Compare different initialization methods.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"INITIALIZATION COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    initializations = ['zeros', 'random', 'xavier', 'he']\n",
        "    results = {}\n",
        "\n",
        "    for init in initializations:\n",
        "        print(f\"\\n--- Training with {init.upper()} initialization ---\")\n",
        "        np.random.seed(42)\n",
        "\n",
        "        nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization=init)\n",
        "        losses = nn.train(X_train, Y_train, epochs=epochs, print_loss=False)\n",
        "\n",
        "        train_acc = nn.accuracy(X_train, Y_train)\n",
        "        test_acc = nn.accuracy(X_test, Y_test)\n",
        "\n",
        "        results[init] = {\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'final_loss': losses[-1] if losses else float('inf'),\n",
        "            'losses': losses\n",
        "        }\n",
        "\n",
        "        print(f\"  Final Loss: {losses[-1]:.6f}\" if losses else \"  Final Loss: N/A\")\n",
        "        print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"INITIALIZATION SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Method':<12} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for init, res in results.items():\n",
        "        print(f\"{init:<12} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_normalizations(X_train_raw, Y_train, X_test_raw, Y_test, layer_dims, epochs=1500):\n",
        "    \"\"\"Compare different normalization methods.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"NORMALIZATION COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. No normalization\n",
        "    print(\"\\n--- Training with NO normalization ---\")\n",
        "    np.random.seed(42)\n",
        "    nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization='he')\n",
        "    losses = nn.train(X_train_raw, Y_train, epochs=epochs, print_loss=False)\n",
        "    train_acc = nn.accuracy(X_train_raw, Y_train)\n",
        "    test_acc = nn.accuracy(X_test_raw, Y_test)\n",
        "    results['None'] = {'train_acc': train_acc, 'test_acc': test_acc, 'final_loss': losses[-1]}\n",
        "    print(f\"  Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 2. Min-Max normalization\n",
        "    print(\"\\n--- Training with MIN-MAX normalization ---\")\n",
        "    X_train_mm, X_min, X_max = FiveLayerNN.normalize_minmax(X_train_raw)\n",
        "    X_test_mm, _, _ = FiveLayerNN.normalize_minmax(X_test_raw, X_min, X_max)\n",
        "    np.random.seed(42)\n",
        "    nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization='he')\n",
        "    losses = nn.train(X_train_mm, Y_train, epochs=epochs, print_loss=False)\n",
        "    train_acc = nn.accuracy(X_train_mm, Y_train)\n",
        "    test_acc = nn.accuracy(X_test_mm, Y_test)\n",
        "    results['Min-Max'] = {'train_acc': train_acc, 'test_acc': test_acc, 'final_loss': losses[-1]}\n",
        "    print(f\"  Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 3. Z-score standardization\n",
        "    print(\"\\n--- Training with Z-SCORE standardization ---\")\n",
        "    X_train_zs, mean, std = FiveLayerNN.normalize_zscore(X_train_raw)\n",
        "    X_test_zs, _, _ = FiveLayerNN.normalize_zscore(X_test_raw, mean, std)\n",
        "    np.random.seed(42)\n",
        "    nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization='he')\n",
        "    losses = nn.train(X_train_zs, Y_train, epochs=epochs, print_loss=False)\n",
        "    train_acc = nn.accuracy(X_train_zs, Y_train)\n",
        "    test_acc = nn.accuracy(X_test_zs, Y_test)\n",
        "    results['Z-Score'] = {'train_acc': train_acc, 'test_acc': test_acc, 'final_loss': losses[-1]}\n",
        "    print(f\"  Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 4. Mean normalization\n",
        "    print(\"\\n--- Training with MEAN normalization ---\")\n",
        "    X_train_mn, mean = FiveLayerNN.normalize_mean(X_train_raw)\n",
        "    X_test_mn, _ = FiveLayerNN.normalize_mean(X_test_raw, mean)\n",
        "    np.random.seed(42)\n",
        "    nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization='he')\n",
        "    losses = nn.train(X_train_mn, Y_train, epochs=epochs, print_loss=False)\n",
        "    train_acc = nn.accuracy(X_train_mn, Y_train)\n",
        "    test_acc = nn.accuracy(X_test_mn, Y_test)\n",
        "    results['Mean'] = {'train_acc': train_acc, 'test_acc': test_acc, 'final_loss': losses[-1]}\n",
        "    print(f\"  Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # 5. L2 normalization\n",
        "    print(\"\\n--- Training with L2 normalization ---\")\n",
        "    X_train_l2 = FiveLayerNN.normalize_l2(X_train_raw)\n",
        "    X_test_l2 = FiveLayerNN.normalize_l2(X_test_raw)\n",
        "    np.random.seed(42)\n",
        "    nn = FiveLayerNN(layer_dims, learning_rate=0.1, initialization='he')\n",
        "    losses = nn.train(X_train_l2, Y_train, epochs=epochs, print_loss=False)\n",
        "    train_acc = nn.accuracy(X_train_l2, Y_train)\n",
        "    test_acc = nn.accuracy(X_test_l2, Y_test)\n",
        "    results['L2'] = {'train_acc': train_acc, 'test_acc': test_acc, 'final_loss': losses[-1]}\n",
        "    print(f\"  Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"NORMALIZATION SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Method':<12} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for method, res in results.items():\n",
        "        print(f\"{method:<12} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_regularizations(X_train, Y_train, X_test, Y_test, layer_dims, epochs=1500):\n",
        "    \"\"\"Compare different regularization methods.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"REGULARIZATION COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    configs = [\n",
        "        {'name': 'No Regularization', 'lambd': 0.0, 'keep_prob': 1.0},\n",
        "        {'name': 'L2 (\u0003bb=0.1)', 'lambd': 0.1, 'keep_prob': 1.0},\n",
        "        {'name': 'L2 (\u0003bb=0.5)', 'lambd': 0.5, 'keep_prob': 1.0},\n",
        "        {'name': 'L2 (\u0003bb=1.0)', 'lambd': 1.0, 'keep_prob': 1.0},\n",
        "        {'name': 'Dropout (0.2)', 'lambd': 0.0, 'keep_prob': 0.8},\n",
        "        {'name': 'Dropout (0.4)', 'lambd': 0.0, 'keep_prob': 0.6},\n",
        "        {'name': 'L2 + Dropout', 'lambd': 0.3, 'keep_prob': 0.8},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for cfg in configs:\n",
        "        print(f\"\\n--- Training with {cfg['name']} ---\")\n",
        "        np.random.seed(42)\n",
        "\n",
        "        nn = FiveLayerNN(\n",
        "            layer_dims,\n",
        "            learning_rate=0.1,\n",
        "            initialization='he',\n",
        "            lambd=cfg['lambd'],\n",
        "            keep_prob=cfg['keep_prob']\n",
        "        )\n",
        "        losses = nn.train(X_train, Y_train, epochs=epochs, print_loss=False)\n",
        "\n",
        "        train_acc = nn.accuracy(X_train, Y_train)\n",
        "        test_acc = nn.accuracy(X_test, Y_test)\n",
        "\n",
        "        results[cfg['name']] = {\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'final_loss': losses[-1],\n",
        "            'losses': losses,\n",
        "            'config': cfg\n",
        "        }\n",
        "\n",
        "        print(f\"  Final Loss: {losses[-1]:.6f}\")\n",
        "        print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"  Overfitting Gap: {train_acc - test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"REGULARIZATION SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Method':<20} {'Train Acc':<12} {'Test Acc':<12} {'Gap':<10} {'Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for name, res in results.items():\n",
        "        gap = res['train_acc'] - res['test_acc']\n",
        "        print(f\"{name:<20} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {gap:<10.2f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_optimizers(X_train, Y_train, X_test, Y_test, layer_dims, epochs=1500):\n",
        "    \"\"\"Compare different optimization algorithms.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"OPTIMIZER COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    configs = [\n",
        "        {'name': 'Gradient Descent', 'optimizer': 'gd', 'lr': 0.1},\n",
        "        {'name': 'Momentum (\u0003b2=0.9)', 'optimizer': 'momentum', 'beta': 0.9, 'lr': 0.1},\n",
        "        {'name': 'RMSprop', 'optimizer': 'rmsprop', 'beta2': 0.999, 'lr': 0.01},\n",
        "        {'name': 'Adam', 'optimizer': 'adam', 'beta1': 0.9, 'beta2': 0.999, 'lr': 0.01},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for cfg in configs:\n",
        "        print(f\"\\n--- Training with {cfg['name']} ---\")\n",
        "        np.random.seed(42)\n",
        "\n",
        "        nn = FiveLayerNN(\n",
        "            layer_dims,\n",
        "            learning_rate=cfg.get('lr', 0.01),\n",
        "            initialization='he',\n",
        "            optimizer=cfg['optimizer'],\n",
        "            beta=cfg.get('beta', 0.9),\n",
        "            beta1=cfg.get('beta1', 0.9),\n",
        "            beta2=cfg.get('beta2', 0.999)\n",
        "        )\n",
        "        losses = nn.train(X_train, Y_train, epochs=epochs, print_loss=False)\n",
        "\n",
        "        train_acc = nn.accuracy(X_train, Y_train)\n",
        "        test_acc = nn.accuracy(X_test, Y_test)\n",
        "\n",
        "        results[cfg['name']] = {\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'final_loss': losses[-1],\n",
        "            'losses': losses,\n",
        "            'config': cfg\n",
        "        }\n",
        "\n",
        "        print(f\"  Final Loss: {losses[-1]:.6f}\")\n",
        "        print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"OPTIMIZER SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Optimizer':<20} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:<20} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_mini_batch_sizes(X_train, Y_train, X_test, Y_test, layer_dims, epochs=500):\n",
        "    \"\"\"Compare different mini-batch sizes.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MINI-BATCH SIZE COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    m = X_train.shape[1]\n",
        "    batch_sizes = [1, 32, 64, 128, m]  # SGD, mini-batch sizes, full batch\n",
        "    batch_names = ['SGD (1)', 'Mini-batch (32)', 'Mini-batch (64)', 'Mini-batch (128)', f'Batch GD ({m})']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for batch_size, name in zip(batch_sizes, batch_names):\n",
        "        print(f\"\\n--- Training with {name} ---\")\n",
        "        np.random.seed(42)\n",
        "\n",
        "        nn = FiveLayerNN(\n",
        "            layer_dims,\n",
        "            learning_rate=0.01,\n",
        "            initialization='he',\n",
        "            optimizer='adam'\n",
        "        )\n",
        "        losses = nn.train(X_train, Y_train, epochs=epochs, print_loss=False,\n",
        "                         mini_batch_size=batch_size)\n",
        "\n",
        "        train_acc = nn.accuracy(X_train, Y_train)\n",
        "        test_acc = nn.accuracy(X_test, Y_test)\n",
        "\n",
        "        results[name] = {\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'final_loss': losses[-1],\n",
        "            'losses': losses,\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "        print(f\"  Final Loss: {losses[-1]:.6f}\")\n",
        "        print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"MINI-BATCH SIZE SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Batch Size':<20} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:<20} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_learning_rate_decay(X_train, Y_train, X_test, Y_test, layer_dims, epochs=2500):\n",
        "    \"\"\"Compare different learning rate decay strategies.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LEARNING RATE DECAY COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    configs = [\n",
        "        {'name': 'No Decay', 'decay_rate': 0.0, 'time_interval': 1000, 'decay_type': 'scheduled'},\n",
        "        {'name': 'Continuous (rate=0.01)', 'decay_rate': 0.01, 'time_interval': 1000, 'decay_type': 'continuous'},\n",
        "        {'name': 'Continuous (rate=0.1)', 'decay_rate': 0.1, 'time_interval': 1000, 'decay_type': 'continuous'},\n",
        "        {'name': 'Scheduled (rate=1, int=500)', 'decay_rate': 1.0, 'time_interval': 500, 'decay_type': 'scheduled'},\n",
        "        {'name': 'Scheduled (rate=1, int=1000)', 'decay_rate': 1.0, 'time_interval': 1000, 'decay_type': 'scheduled'},\n",
        "        {'name': 'Scheduled (rate=0.5, int=500)', 'decay_rate': 0.5, 'time_interval': 500, 'decay_type': 'scheduled'},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for cfg in configs:\n",
        "        print(f\"\\n--- Training with {cfg['name']} ---\")\n",
        "        np.random.seed(42)\n",
        "\n",
        "        nn = FiveLayerNN(\n",
        "            layer_dims,\n",
        "            learning_rate=0.1,\n",
        "            initialization='he',\n",
        "            optimizer='gd',\n",
        "            decay_rate=cfg['decay_rate'],\n",
        "            time_interval=cfg['time_interval']\n",
        "        )\n",
        "        losses = nn.train(X_train, Y_train, epochs=epochs, print_loss=False,\n",
        "                         decay_type=cfg['decay_type'])\n",
        "\n",
        "        train_acc = nn.accuracy(X_train, Y_train)\n",
        "        test_acc = nn.accuracy(X_test, Y_test)\n",
        "\n",
        "        # Get final learning rate\n",
        "        final_lr = nn.learning_rate\n",
        "\n",
        "        results[cfg['name']] = {\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'final_loss': losses[-1],\n",
        "            'final_lr': final_lr,\n",
        "            'losses': losses,\n",
        "            'lr_history': nn.learning_rate_history if hasattr(nn, 'learning_rate_history') else [],\n",
        "            'config': cfg\n",
        "        }\n",
        "\n",
        "        print(f\"  Final Loss: {losses[-1]:.6f}\")\n",
        "        print(f\"  Final LR: {final_lr:.6f}\")\n",
        "        print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"LEARNING RATE DECAY SUMMARY\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Decay Strategy':<30} {'Train Acc':<12} {'Test Acc':<12} {'Final LR':<12} {'Loss':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:<30} {res['train_acc']:<12.2f} {res['test_acc']:<12.2f} {res['final_lr']:<12.6f} {res['final_loss']:<12.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate sample data (XOR-like problem with noise for overfitting demo)\n",
        "    np.random.seed(1)\n",
        "    m_train = 300  # Small training set to show overfitting\n",
        "    m_test = 200\n",
        "\n",
        "    # Training data\n",
        "    X_train = np.random.randn(2, m_train)\n",
        "    Y_train = ((X_train[0, :] * X_train[1, :]) > 0).astype(int).reshape(1, m_train)\n",
        "    # Add some noise\n",
        "    noise_idx = np.random.choice(m_train, size=int(m_train * 0.05), replace=False)\n",
        "    Y_train[0, noise_idx] = 1 - Y_train[0, noise_idx]\n",
        "\n",
        "    # Test data (clean)\n",
        "    X_test = np.random.randn(2, m_test)\n",
        "    Y_test = ((X_test[0, :] * X_test[1, :]) > 0).astype(int).reshape(1, m_test)\n",
        "\n",
        "    # Network architecture\n",
        "    layer_dims = [2, 20, 15, 10, 5, 1]  # Larger network to show overfitting\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"5-LAYER NEURAL NETWORK\")\n",
        "    print(\"Initialization & Regularization Comparison\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Architecture: {layer_dims}\")\n",
        "    print(f\"Training samples: {m_train}\")\n",
        "    print(f\"Test samples: {m_test}\")\n",
        "\n",
        "    # ============ Compare Normalizations ============\n",
        "    norm_results = compare_normalizations(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Compare Initializations ============\n",
        "    init_results = compare_initializations(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Compare Regularizations ============\n",
        "    reg_results = compare_regularizations(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Compare Optimizers ============\n",
        "    opt_results = compare_optimizers(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Compare Mini-Batch Sizes ============\n",
        "    batch_results = compare_mini_batch_sizes(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Compare Learning Rate Decay ============\n",
        "    lr_decay_results = compare_learning_rate_decay(X_train, Y_train, X_test, Y_test, layer_dims)\n",
        "\n",
        "    # ============ Gradient Checking ============\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GRADIENT CHECKING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Create a network for gradient checking\n",
        "    # Note: Use enough neurons per layer to avoid \"dead ReLU\" problem\n",
        "    # where all activations become zero (making gradients zero too)\n",
        "    grad_check_dims = [2, 10, 8, 6, 4, 1]\n",
        "    np.random.seed(1)\n",
        "    grad_check_nn = FiveLayerNN(grad_check_dims, learning_rate=0.1, initialization='he')\n",
        "\n",
        "    # Use small subset of data\n",
        "    X_small = X_train[:, :5]\n",
        "    Y_small = Y_train[:, :5]\n",
        "\n",
        "    diff, analytical, numerical = grad_check_nn.gradient_check(X_small, Y_small)\n",
        "\n",
        "    print(f\"Relative difference: {diff:.2e}\")\n",
        "    if diff < 1e-7:\n",
        "        print(\"Gradient check PASSED! Backpropagation is correct.\")\n",
        "    elif diff < 1e-5:\n",
        "        print(\"Gradient check WARNING: Small discrepancy detected.\")\n",
        "    else:\n",
        "        print(\"Gradient check FAILED! There may be a bug in backpropagation.\")\n",
        "\n",
        "    # ============ Final Summary ============\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"KEY TAKEAWAYS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\"\"\n",
        "    NORMALIZATION:\n",
        "    - Min-Max: Scales to [0, 1] range - good for bounded features\n",
        "    - Z-Score: Mean=0, Std=1 - most common, handles outliers\n",
        "    - Mean: Centers around zero - simple but effective\n",
        "    - L2: Unit norm per sample - for direction-based similarity\n",
        "\n",
        "    INITIALIZATION:\n",
        "    - Zeros: All neurons learn the same thing (symmetry problem)\n",
        "    - Random (large): Can cause exploding/vanishing gradients\n",
        "    - Xavier: Good for tanh/sigmoid activations\n",
        "    - He: Best for ReLU activations (used in this network)\n",
        "\n",
        "    REGULARIZATION:\n",
        "    - L2 (Weight Decay): Penalizes large weights, reduces overfitting\n",
        "    - Dropout: Randomly drops neurons, prevents co-adaptation\n",
        "    - Combined: Often gives best results\n",
        "\n",
        "    OPTIMIZATION ALGORITHMS:\n",
        "    1. Gradient Descent (GD):\n",
        "       - W = W - \u0003b1*dW\n",
        "       - Simple but can be slow\n",
        "\n",
        "    2. Momentum:\n",
        "       - v = \u0003b2*v + (1-\u0003b2)*dW, W = W - \u0003b1*v\n",
        "       - Accelerates in consistent directions, dampens oscillations\n",
        "       - \u0003b2=0.9 is typical (~10 gradients averaged)\n",
        "\n",
        "    3. RMSprop:\n",
        "       - s = \u0003b2\n",
        "*s + (1-\u0003b2\n",
        ")*dW\n",
        ", W = W - \u0003b1*dW/\n",
        "s+ \u0003b5\n",
        "       - Adapts learning rate per parameter\n",
        "       - Good for non-stationary problems\n",
        "\n",
        "    4. Adam (Recommended):\n",
        "       - Combines Momentum + RMSprop with bias correction\n",
        "       - v = \u0003b2\n",
        "*v + (1-\u0003b2\n",
        ")*dW (first moment)\n",
        "       - s = \u0003b2\n",
        "*s + (1-\u0003b2\n",
        ")*dW\n",
        " (second moment)\n",
        "       - Works well in most cases, less sensitive to hyperparameters\n",
        "\n",
        "    MINI-BATCH GRADIENT DESCENT:\n",
        "    - Batch GD (batch=m): Smooth but slow, memory intensive\n",
        "    - SGD (batch=1): Noisy but fast, can escape local minima\n",
        "    - Mini-batch (32-256): Best of both worlds (recommended)\n",
        "\n",
        "    LEARNING RATE DECAY:\n",
        "    - Helps fine-tune convergence in later training stages\n",
        "    - Two types:\n",
        "      1. Continuous decay: \u0003b1 = \u0003b1\u0000 / (1 + decay_rate * epoch)\n",
        "         - Smoothly decreases every epoch\n",
        "      2. Scheduled decay: \u0003b1 = \u0003b1\u0000 / (1 + decay_rate * floor(epoch / interval))\n",
        "         - Steps down at fixed intervals (e.g., every 500 or 1000 epochs)\n",
        "    - Benefits:\n",
        "      - Large steps early for fast initial progress\n",
        "      - Small steps later for fine-tuning near minimum\n",
        "    - Typical values: decay_rate=0.01-1.0, interval=500-1000\n",
        "\n",
        "    SIGNS OF OVERFITTING:\n",
        "    - High training accuracy, low test accuracy\n",
        "    - Large gap between train and test performance\n",
        "\n",
        "    RECOMMENDATIONS:\n",
        "    - Always normalize input data (Z-score is most common)\n",
        "    - Use He initialization for ReLU networks\n",
        "    - Use Adam optimizer with mini-batches (64 or 128)\n",
        "    - Start with small L2 (\u0003bb=0.01-0.1) or dropout (keep_prob=0.8-0.9)\n",
        "    - Increase regularization if overfitting persists\n",
        "    \"\"\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FiveLayerNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FiveLayerNN:\n",
        "    \"\"\"\n",
        "    5-Layer Neural Network with configurable layer sizes.\n",
        "\n",
        "    Architecture:\n",
        "    Input (X) -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> Layer5 (Output)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_dims, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the neural network.\n",
        "\n",
        "        Args:\n",
        "            layer_dims: list of 6 integers [n_x, n1, n2, n3, n4, n5]\n",
        "                        where n_x is input size, n5 is output size\n",
        "            learning_rate: learning rate for gradient descent\n",
        "        \"\"\"\n",
        "        self.layer_dims = layer_dims\n",
        "        self.learning_rate = learning_rate\n",
        "        self.parameters = {}\n",
        "        self.cache = {}\n",
        "        self.gradients = {}\n",
        "\n",
        "        self._initialize_parameters()\n",
        "\n",
        "    def _initialize_parameters(self):\n",
        "        \"\"\"Initialize W and b for all 5 layers using He initialization.\"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for l in range(1, 6):\n",
        "            #* np.sqrt(2 / ...)\tHe initialization  make numbers not too big, not too small!\n",
        "            self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2 / self.layer_dims[l-1])\n",
        "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add activation functions to the class\n",
        "def relu(self, Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(self, Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "def sigmoid(self, Z):\n",
        "    return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(self, Z):\n",
        "    s = self.sigmoid(Z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def softmax(self, Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "\n",
        "# Attach methods to class\n",
        "FiveLayerNN.relu = relu\n",
        "FiveLayerNN.relu_derivative = relu_derivative\n",
        "FiveLayerNN.sigmoid = sigmoid\n",
        "FiveLayerNN.sigmoid_derivative = sigmoid_derivative\n",
        "FiveLayerNN.softmax = softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_propagation(self, X):\n",
        "    \"\"\"\n",
        "    Forward pass through all 5 layers.\n",
        "\n",
        "    Layers 1-4: ReLU activation\n",
        "    Layer 5: Sigmoid activation (binary) or Softmax (multiclass)\n",
        "    \"\"\"\n",
        "    self.cache['A0'] = X\n",
        "    A = X\n",
        "\n",
        "    # Layers 1-4: Linear -> ReLU\n",
        "    for l in range(1, 5):\n",
        "        W = self.parameters[f'W{l}']\n",
        "        b = self.parameters[f'b{l}']\n",
        "\n",
        "        Z = np.dot(W, A) + b\n",
        "        A = self.relu(Z)\n",
        "\n",
        "        self.cache[f'Z{l}'] = Z\n",
        "        self.cache[f'A{l}'] = A\n",
        "\n",
        "    # Layer 5: Linear -> Sigmoid (output)\n",
        "    W5 = self.parameters['W5']\n",
        "    b5 = self.parameters['b5']\n",
        "\n",
        "    Z5 = np.dot(W5, A) + b5\n",
        "    A5 = self.sigmoid(Z5)\n",
        "\n",
        "    self.cache['Z5'] = Z5\n",
        "    self.cache['A5'] = A5\n",
        "\n",
        "    return A5\n",
        "\n",
        "FiveLayerNN.forward_propagation = forward_propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward Propagation\n",
        "\n",
        "**Layer 5 (Output):**\n",
        "- dZ = A - Y\n",
        "- dW = (1/m) * dZ  (A)\n",
        "- db = (1/m) *  dZ\n",
        "\n",
        "**Layers 4-1:**\n",
        "- dZ = (W)  dZ * g'(Z)\n",
        "- dW = (1/m) * dZ  (A)\n",
        "- db = (1/m) *  dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_propagation(self, Y):\n",
        "    \"\"\"\n",
        "    Backward pass computing dZ, dW, db for all 5 layers.\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # ============ Layer 5 (Output Layer) ============\n",
        "    # dZ = A - Y\n",
        "    dZ5 = self.cache['A5'] - Y\n",
        "\n",
        "    # dW = (1/m) * dZ  (A)\n",
        "    dW5 = (1/m) * np.dot(dZ5, self.cache['A4'].T)\n",
        "\n",
        "    # db = (1/m) *  dZ\n",
        "    db5 = (1/m) * np.sum(dZ5, axis=1, keepdims=True)\n",
        "\n",
        "    self.gradients['dZ5'] = dZ5\n",
        "    self.gradients['dW5'] = dW5\n",
        "    self.gradients['db5'] = db5\n",
        "\n",
        "    # ============ Layer 4 ============\n",
        "    # dZ = (W)  dZ * g'(Z)\n",
        "    dZ4 = np.dot(self.parameters['W5'].T, dZ5) * self.relu_derivative(self.cache['Z4'])\n",
        "\n",
        "    # dW = (1/m) * dZ  (A)\n",
        "    dW4 = (1/m) * np.dot(dZ4, self.cache['A3'].T)\n",
        "\n",
        "    # db = (1/m) *  dZ\n",
        "    db4 = (1/m) * np.sum(dZ4, axis=1, keepdims=True)\n",
        "\n",
        "    self.gradients['dZ4'] = dZ4\n",
        "    self.gradients['dW4'] = dW4\n",
        "    self.gradients['db4'] = db4\n",
        "\n",
        "    # ============ Layer 3 ============\n",
        "    # dZ = (W)  dZ * g'(Z)\n",
        "    dZ3 = np.dot(self.parameters['W4'].T, dZ4) * self.relu_derivative(self.cache['Z3'])\n",
        "\n",
        "    # dW = (1/m) * dZ  (A)\n",
        "    dW3 = (1/m) * np.dot(dZ3, self.cache['A2'].T)\n",
        "\n",
        "    # db = (1/m) *  dZ\n",
        "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "    self.gradients['dZ3'] = dZ3\n",
        "    self.gradients['dW3'] = dW3\n",
        "    self.gradients['db3'] = db3\n",
        "\n",
        "    # ============ Layer 2 ============\n",
        "    # dZ = (W)  dZ * g'(Z)\n",
        "    dZ2 = np.dot(self.parameters['W3'].T, dZ3) * self.relu_derivative(self.cache['Z2'])\n",
        "\n",
        "    # dW = (1/m) * dZ  (A)\n",
        "    dW2 = (1/m) * np.dot(dZ2, self.cache['A1'].T)\n",
        "\n",
        "    # db = (1/m) *  dZ\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    self.gradients['dZ2'] = dZ2\n",
        "    self.gradients['dW2'] = dW2\n",
        "    self.gradients['db2'] = db2\n",
        "\n",
        "    # ============ Layer 1 ============\n",
        "    # dZ = (W)  dZ * g'(Z)\n",
        "    dZ1 = np.dot(self.parameters['W2'].T, dZ2) * self.relu_derivative(self.cache['Z1'])\n",
        "\n",
        "    # dW = (1/m) * dZ  (X)\n",
        "    dW1 = (1/m) * np.dot(dZ1, self.cache['A0'].T)\n",
        "\n",
        "    # db = (1/m) *  dZ\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    self.gradients['dZ1'] = dZ1\n",
        "    self.gradients['dW1'] = dW1\n",
        "    self.gradients['db1'] = db1\n",
        "\n",
        "FiveLayerNN.backward_propagation = backward_propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_parameters(self):\n",
        "    \"\"\"Update W and b using gradient descent.\"\"\"\n",
        "    for l in range(1, 6):\n",
        "        self.parameters[f'W{l}'] -= self.learning_rate * self.gradients[f'dW{l}']\n",
        "        self.parameters[f'b{l}'] -= self.learning_rate * self.gradients[f'db{l}']\n",
        "\n",
        "FiveLayerNN.update_parameters = update_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(self, Y):\n",
        "    \"\"\"Binary cross-entropy loss.\"\"\"\n",
        "    m = Y.shape[1]\n",
        "    A5 = self.cache['A5']\n",
        "\n",
        "    # Clip to prevent log(0)\n",
        "    A5 = np.clip(A5, 1e-15, 1 - 1e-15)\n",
        "\n",
        "    loss = -(1/m) * np.sum(Y * np.log(A5) + (1 - Y) * np.log(1 - A5))\n",
        "    return loss\n",
        "\n",
        "FiveLayerNN.compute_loss = compute_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(self, X, Y, epochs=1000, print_loss=True):\n",
        "    \"\"\"Train the neural network.\"\"\"\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        self.forward_propagation(X)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self.compute_loss(Y)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Backward propagation\n",
        "        self.backward_propagation(Y)\n",
        "\n",
        "        # Update parameters\n",
        "        self.update_parameters()\n",
        "\n",
        "        if print_loss and epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch:4d} | Loss: {loss:.6f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "FiveLayerNN.train = train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(self, X):\n",
        "    \"\"\"Make predictions.\"\"\"\n",
        "    A5 = self.forward_propagation(X)\n",
        "    return (A5 > 0.5).astype(int)\n",
        "\n",
        "def accuracy(self, X, Y):\n",
        "    \"\"\"Compute accuracy.\"\"\"\n",
        "    predictions = self.predict(X)\n",
        "    return np.mean(predictions == Y) * 100\n",
        "\n",
        "FiveLayerNN.predict = predict\n",
        "FiveLayerNN.accuracy = accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo - XOR-like Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data (XOR-like problem)\n",
        "np.random.seed(1)\n",
        "m = 1000  # number of samples\n",
        "\n",
        "X = np.random.randn(2, m)\n",
        "Y = ((X[0, :] * X[1, :]) > 0).astype(int).reshape(1, m)\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Network architecture: [input, layer1, layer2, layer3, layer4, output]\n",
        "layer_dims = [2, 8, 8, 6, 4, 1]\n",
        "\n",
        "# Create and train network\n",
        "nn = FiveLayerNN(layer_dims, learning_rate=0.1)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"5-Layer Neural Network Training\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Architecture: {layer_dims}\")\n",
        "print(f\"Training samples: {m}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the network\n",
        "losses = nn.train(X, Y, epochs=1000, print_loss=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(f\"Final Accuracy: {nn.accuracy(X, Y):.2f}%\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print gradient shapes for verification\n",
        "print(\"\\nGradient Shapes:\")\n",
        "for l in range(1, 6):\n",
        "    print(f\"  Layer {l}: dW{l}{nn.gradients[f'dW{l}'].shape}, \"\n",
        "          f\"db{l}{nn.gradients[f'db{l}'].shape}, \"\n",
        "          f\"dZ{l}{nn.gradients[f'dZ{l}'].shape}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
