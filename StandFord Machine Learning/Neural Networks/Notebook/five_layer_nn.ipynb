{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Layer Neural Network\n",
    "\n",
    "A neural network with configurable layer sizes.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (X) -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> Layer5 (Output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FiveLayerNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveLayerNN:\n",
    "    \"\"\"\n",
    "    5-Layer Neural Network with configurable layer sizes.\n",
    "\n",
    "    Architecture:\n",
    "    Input (X) -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> Layer5 (Output)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_dims, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "\n",
    "        Args:\n",
    "            layer_dims: list of 6 integers [n_x, n1, n2, n3, n4, n5]\n",
    "                        where n_x is input size, n5 is output size\n",
    "            learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize W and b for all 5 layers using He initialization.\"\"\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for l in range(1, 6):\n",
    "            #* np.sqrt(2 / ...)\tHe initialization — make numbers not too big, not too small!\n",
    "            self.parameters[f'W{l}'] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * np.sqrt(2 / self.layer_dims[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add activation functions to the class\n",
    "def relu(self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(self, Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid(self, Z):\n",
    "    return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(self, Z):\n",
    "    s = self.sigmoid(Z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(self, Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "# Attach methods to class\n",
    "FiveLayerNN.relu = relu\n",
    "FiveLayerNN.relu_derivative = relu_derivative\n",
    "FiveLayerNN.sigmoid = sigmoid\n",
    "FiveLayerNN.sigmoid_derivative = sigmoid_derivative\n",
    "FiveLayerNN.softmax = softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, X):\n",
    "    \"\"\"\n",
    "    Forward pass through all 5 layers.\n",
    "\n",
    "    Layers 1-4: ReLU activation\n",
    "    Layer 5: Sigmoid activation (binary) or Softmax (multiclass)\n",
    "    \"\"\"\n",
    "    self.cache['A0'] = X\n",
    "    A = X\n",
    "\n",
    "    # Layers 1-4: Linear -> ReLU\n",
    "    for l in range(1, 5):\n",
    "        W = self.parameters[f'W{l}']\n",
    "        b = self.parameters[f'b{l}']\n",
    "\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = self.relu(Z)\n",
    "\n",
    "        self.cache[f'Z{l}'] = Z\n",
    "        self.cache[f'A{l}'] = A\n",
    "\n",
    "    # Layer 5: Linear -> Sigmoid (output)\n",
    "    W5 = self.parameters['W5']\n",
    "    b5 = self.parameters['b5']\n",
    "\n",
    "    Z5 = np.dot(W5, A) + b5\n",
    "    A5 = self.sigmoid(Z5)\n",
    "\n",
    "    self.cache['Z5'] = Z5\n",
    "    self.cache['A5'] = A5\n",
    "\n",
    "    return A5\n",
    "\n",
    "FiveLayerNN.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "\n",
    "**Layer 5 (Output):**\n",
    "- dZ⁵ = A⁵ - Y\n",
    "- dW⁵ = (1/m) * dZ⁵ · (A⁴)ᵀ\n",
    "- db⁵ = (1/m) * Σ dZ⁵\n",
    "\n",
    "**Layers 4-1:**\n",
    "- dZˡ = (Wˡ⁺¹)ᵀ · dZˡ⁺¹ * g'ˡ(Zˡ)\n",
    "- dWˡ = (1/m) * dZˡ · (Aˡ⁻¹)ᵀ\n",
    "- dbˡ = (1/m) * Σ dZˡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(self, Y):\n",
    "    \"\"\"\n",
    "    Backward pass computing dZ, dW, db for all 5 layers.\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # ============ Layer 5 (Output Layer) ============\n",
    "    # dZ⁵ = A⁵ - Y\n",
    "    dZ5 = self.cache['A5'] - Y\n",
    "\n",
    "    # dW⁵ = (1/m) * dZ⁵ · (A⁴)ᵀ\n",
    "    dW5 = (1/m) * np.dot(dZ5, self.cache['A4'].T)\n",
    "\n",
    "    # db⁵ = (1/m) * Σ dZ⁵\n",
    "    db5 = (1/m) * np.sum(dZ5, axis=1, keepdims=True)\n",
    "\n",
    "    self.gradients['dZ5'] = dZ5\n",
    "    self.gradients['dW5'] = dW5\n",
    "    self.gradients['db5'] = db5\n",
    "\n",
    "    # ============ Layer 4 ============\n",
    "    # dZ⁴ = (W⁵)ᵀ · dZ⁵ * g'⁴(Z⁴)\n",
    "    dZ4 = np.dot(self.parameters['W5'].T, dZ5) * self.relu_derivative(self.cache['Z4'])\n",
    "\n",
    "    # dW⁴ = (1/m) * dZ⁴ · (A³)ᵀ\n",
    "    dW4 = (1/m) * np.dot(dZ4, self.cache['A3'].T)\n",
    "\n",
    "    # db⁴ = (1/m) * Σ dZ⁴\n",
    "    db4 = (1/m) * np.sum(dZ4, axis=1, keepdims=True)\n",
    "\n",
    "    self.gradients['dZ4'] = dZ4\n",
    "    self.gradients['dW4'] = dW4\n",
    "    self.gradients['db4'] = db4\n",
    "\n",
    "    # ============ Layer 3 ============\n",
    "    # dZ³ = (W⁴)ᵀ · dZ⁴ * g'³(Z³)\n",
    "    dZ3 = np.dot(self.parameters['W4'].T, dZ4) * self.relu_derivative(self.cache['Z3'])\n",
    "\n",
    "    # dW³ = (1/m) * dZ³ · (A²)ᵀ\n",
    "    dW3 = (1/m) * np.dot(dZ3, self.cache['A2'].T)\n",
    "\n",
    "    # db³ = (1/m) * Σ dZ³\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    self.gradients['dZ3'] = dZ3\n",
    "    self.gradients['dW3'] = dW3\n",
    "    self.gradients['db3'] = db3\n",
    "\n",
    "    # ============ Layer 2 ============\n",
    "    # dZ² = (W³)ᵀ · dZ³ * g'²(Z²)\n",
    "    dZ2 = np.dot(self.parameters['W3'].T, dZ3) * self.relu_derivative(self.cache['Z2'])\n",
    "\n",
    "    # dW² = (1/m) * dZ² · (A¹)ᵀ\n",
    "    dW2 = (1/m) * np.dot(dZ2, self.cache['A1'].T)\n",
    "\n",
    "    # db² = (1/m) * Σ dZ²\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    self.gradients['dZ2'] = dZ2\n",
    "    self.gradients['dW2'] = dW2\n",
    "    self.gradients['db2'] = db2\n",
    "\n",
    "    # ============ Layer 1 ============\n",
    "    # dZ¹ = (W²)ᵀ · dZ² * g'¹(Z¹)\n",
    "    dZ1 = np.dot(self.parameters['W2'].T, dZ2) * self.relu_derivative(self.cache['Z1'])\n",
    "\n",
    "    # dW¹ = (1/m) * dZ¹ · (X)ᵀ\n",
    "    dW1 = (1/m) * np.dot(dZ1, self.cache['A0'].T)\n",
    "\n",
    "    # db¹ = (1/m) * Σ dZ¹\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    self.gradients['dZ1'] = dZ1\n",
    "    self.gradients['dW1'] = dW1\n",
    "    self.gradients['db1'] = db1\n",
    "\n",
    "FiveLayerNN.backward_propagation = backward_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(self):\n",
    "    \"\"\"Update W and b using gradient descent.\"\"\"\n",
    "    for l in range(1, 6):\n",
    "        self.parameters[f'W{l}'] -= self.learning_rate * self.gradients[f'dW{l}']\n",
    "        self.parameters[f'b{l}'] -= self.learning_rate * self.gradients[f'db{l}']\n",
    "\n",
    "FiveLayerNN.update_parameters = update_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, Y):\n",
    "    \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "    m = Y.shape[1]\n",
    "    A5 = self.cache['A5']\n",
    "\n",
    "    # Clip to prevent log(0)\n",
    "    A5 = np.clip(A5, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    loss = -(1/m) * np.sum(Y * np.log(A5) + (1 - Y) * np.log(1 - A5))\n",
    "    return loss\n",
    "\n",
    "FiveLayerNN.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, Y, epochs=1000, print_loss=True):\n",
    "    \"\"\"Train the neural network.\"\"\"\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        self.forward_propagation(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(Y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backward propagation\n",
    "        self.backward_propagation(Y)\n",
    "\n",
    "        # Update parameters\n",
    "        self.update_parameters()\n",
    "\n",
    "        if print_loss and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {loss:.6f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "FiveLayerNN.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"Make predictions.\"\"\"\n",
    "    A5 = self.forward_propagation(X)\n",
    "    return (A5 > 0.5).astype(int)\n",
    "\n",
    "def accuracy(self, X, Y):\n",
    "    \"\"\"Compute accuracy.\"\"\"\n",
    "    predictions = self.predict(X)\n",
    "    return np.mean(predictions == Y) * 100\n",
    "\n",
    "FiveLayerNN.predict = predict\n",
    "FiveLayerNN.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - XOR-like Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data (XOR-like problem)\n",
    "np.random.seed(1)\n",
    "m = 1000  # number of samples\n",
    "\n",
    "X = np.random.randn(2, m)\n",
    "Y = ((X[0, :] * X[1, :]) > 0).astype(int).reshape(1, m)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture: [input, layer1, layer2, layer3, layer4, output]\n",
    "layer_dims = [2, 8, 8, 6, 4, 1]\n",
    "\n",
    "# Create and train network\n",
    "nn = FiveLayerNN(layer_dims, learning_rate=0.1)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"5-Layer Neural Network Training\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Architecture: {layer_dims}\")\n",
    "print(f\"Training samples: {m}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "losses = nn.train(X, Y, epochs=1000, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(f\"Final Accuracy: {nn.accuracy(X, Y):.2f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print gradient shapes for verification\n",
    "print(\"\\nGradient Shapes:\")\n",
    "for l in range(1, 6):\n",
    "    print(f\"  Layer {l}: dW{l}{nn.gradients[f'dW{l}'].shape}, \"\n",
    "          f\"db{l}{nn.gradients[f'db{l}'].shape}, \"\n",
    "          f\"dZ{l}{nn.gradients[f'dZ{l}'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
