{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy vs TensorFlow: Direct Comparison\n",
    "\n",
    "## üéØ Learning Goals\n",
    "- Compare your existing NumPy neural network with TensorFlow equivalent\n",
    "- Understand when to use each approach\n",
    "- See how TensorFlow simplifies complex operations\n",
    "- Learn the trade-offs between manual and automatic implementations\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Completed your NumPy neural network notebooks\n",
    "- Basic understanding of TensorFlow from previous notebook\n",
    "- Familiarity with forward pass and backpropagation concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Your NumPy Implementation (Baseline)\n",
    "\n",
    "Let's start with your existing NumPy neural network implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your NumPy Neural Network (from your existing implementation)\n",
    "class NumPyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with the same structure as your existing network: 2‚Üí2‚Üí1\"\"\"\n",
    "        # Initialize weights randomly (like your trainable version)\n",
    "        self.weights_input_to_hidden = np.random.uniform(-1, 1, (2, 2))\n",
    "        self.weights_hidden_to_output = np.random.uniform(-1, 1, (2, 1))\n",
    "        self.bias_hidden = np.zeros((1, 2))\n",
    "        self.bias_output = np.zeros((1, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Your familiar sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clipping for numerical stability\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid for backpropagation\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = inputs.reshape(1, -1)\n",
    "        \n",
    "        # Input to Hidden Layer\n",
    "        self.hidden_input = np.dot(inputs, self.weights_input_to_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Hidden to Output Layer\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_to_output) + self.bias_output\n",
    "        self.final_output = self.sigmoid(self.output_input)\n",
    "        \n",
    "        return self.final_output\n",
    "    \n",
    "    def backpropagation(self, inputs, target, learning_rate=0.1):\n",
    "        \"\"\"Your manual backpropagation implementation\"\"\"\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = inputs.reshape(1, -1)\n",
    "        if target.ndim == 1:\n",
    "            target = target.reshape(1, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.forward_pass(inputs)\n",
    "        \n",
    "        # Calculate output error\n",
    "        output_error = output - target\n",
    "        \n",
    "        # Calculate gradients\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output_input)\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_to_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_input)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights_hidden_to_output -= learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.weights_input_to_hidden -= learning_rate * np.dot(inputs.T, hidden_delta)\n",
    "        \n",
    "        # Update biases\n",
    "        self.bias_output -= learning_rate * np.mean(output_delta, axis=0, keepdims=True)\n",
    "        self.bias_hidden -= learning_rate * np.mean(hidden_delta, axis=0, keepdims=True)\n",
    "        \n",
    "        return np.mean(output_error ** 2)\n",
    "\n",
    "# Create NumPy network\n",
    "numpy_network = NumPyNeuralNetwork()\n",
    "print(\"‚úÖ NumPy Neural Network created\")\n",
    "print(f\"Input‚ÜíHidden weights shape: {numpy_network.weights_input_to_hidden.shape}\")\n",
    "print(f\"Hidden‚ÜíOutput weights shape: {numpy_network.weights_hidden_to_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ TensorFlow Implementation (Modern Approach)\n",
    "\n",
    "Now let's create the same network using TensorFlow's low-level API (similar to your NumPy approach):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Neural Network (Low-level, similar to your NumPy approach)\n",
    "class TensorFlowNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with same structure: 2‚Üí2‚Üí1\"\"\"\n",
    "        # Initialize weights as Variables (trainable parameters)\n",
    "        self.weights_input_to_hidden = tf.Variable(\n",
    "            tf.random.uniform((2, 2), -1, 1), name='w1'\n",
    "        )\n",
    "        self.weights_hidden_to_output = tf.Variable(\n",
    "            tf.random.uniform((2, 1), -1, 1), name='w2'\n",
    "        )\n",
    "        self.bias_hidden = tf.Variable(tf.zeros((1, 2)), name='b1')\n",
    "        self.bias_output = tf.Variable(tf.zeros((1, 1)), name='b2')\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"Forward pass using TensorFlow operations\"\"\"\n",
    "        if len(inputs.shape) == 1:\n",
    "            inputs = tf.expand_dims(inputs, 0)\n",
    "        \n",
    "        # Input to Hidden Layer\n",
    "        hidden_input = tf.matmul(inputs, self.weights_input_to_hidden) + self.bias_hidden\n",
    "        hidden_output = tf.nn.sigmoid(hidden_input)\n",
    "        \n",
    "        # Hidden to Output Layer\n",
    "        output_input = tf.matmul(hidden_output, self.weights_hidden_to_output) + self.bias_output\n",
    "        final_output = tf.nn.sigmoid(output_input)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def train_step(self, inputs, target, learning_rate=0.1):\n",
    "        \"\"\"Training step with automatic differentiation\"\"\"\n",
    "        if len(inputs.shape) == 1:\n",
    "            inputs = tf.expand_dims(inputs, 0)\n",
    "        if len(target.shape) == 1:\n",
    "            target = tf.expand_dims(target, 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.forward_pass(inputs)\n",
    "            # Calculate loss\n",
    "            loss = tf.reduce_mean(tf.square(predictions - target))\n",
    "        \n",
    "        # Automatic gradient calculation!\n",
    "        gradients = tape.gradient(loss, [\n",
    "            self.weights_input_to_hidden,\n",
    "            self.weights_hidden_to_output,\n",
    "            self.bias_hidden,\n",
    "            self.bias_output\n",
    "        ])\n",
    "        \n",
    "        # Manual weight updates (like your NumPy version)\n",
    "        self.weights_input_to_hidden.assign_sub(learning_rate * gradients[0])\n",
    "        self.weights_hidden_to_output.assign_sub(learning_rate * gradients[1])\n",
    "        self.bias_hidden.assign_sub(learning_rate * gradients[2])\n",
    "        self.bias_output.assign_sub(learning_rate * gradients[3])\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "# Create TensorFlow network\n",
    "tf_network = TensorFlowNeuralNetwork()\n",
    "print(\"‚úÖ TensorFlow Neural Network created\")\n",
    "print(f\"Input‚ÜíHidden weights shape: {tf_network.weights_input_to_hidden.shape}\")\n",
    "print(f\"Hidden‚ÜíOutput weights shape: {tf_network.weights_hidden_to_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Keras Implementation (High-Level Approach)\n",
    "\n",
    "Now let's see how simple this becomes with Keras (TensorFlow's high-level API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Neural Network (High-level, super simple!)\n",
    "def create_keras_network():\n",
    "    \"\"\"Create the same 2‚Üí2‚Üí1 network with Keras\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)),  # Hidden layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')                     # Output layer\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "        loss='mse',  # Mean Squared Error\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create Keras network\n",
    "keras_network = create_keras_network()\n",
    "print(\"‚úÖ Keras Neural Network created\")\n",
    "print(\"\\nModel Summary:\")\n",
    "keras_network.summary()\n",
    "\n",
    "print(\"\\nüí° Notice how much simpler the Keras version is!\")\n",
    "print(\"   - No manual forward pass implementation\")\n",
    "print(\"   - No manual backpropagation\")\n",
    "print(\"   - No manual weight updates\")\n",
    "print(\"   - Everything is handled automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Data Setup\n",
    "\n",
    "Let's create some training data to compare all three approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data (same as your existing examples)\n",
    "training_inputs = np.array([\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0]\n",
    "])\n",
    "\n",
    "training_outputs = np.array([\n",
    "    [1.0],\n",
    "    [0.0],\n",
    "    [0.5],\n",
    "    [0.5]\n",
    "])\n",
    "\n",
    "print(\"Training Data:\")\n",
    "for i in range(len(training_inputs)):\n",
    "    print(f\"Input: {training_inputs[i]} ‚Üí Target: {training_outputs[i][0]}\")\n",
    "\n",
    "# Convert to TensorFlow tensors for TF networks\n",
    "training_inputs_tf = tf.constant(training_inputs, dtype=tf.float32)\n",
    "training_outputs_tf = tf.constant(training_outputs, dtype=tf.float32)\n",
    "\n",
    "print(f\"\\nDataset size: {len(training_inputs)} samples\")\n",
    "print(f\"Input shape: {training_inputs.shape}\")\n",
    "print(f\"Output shape: {training_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Training Comparison\n",
    "\n",
    "Now let's train all three networks and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Storage for loss tracking\n",
    "numpy_losses = []\n",
    "tf_losses = []\n",
    "keras_losses = []\n",
    "\n",
    "print(\"üöÄ Starting training comparison...\")\n",
    "print(f\"Training for {epochs} epochs with learning rate {learning_rate}\")\n",
    "\n",
    "# Time the training processes\n",
    "start_time = time.time()\n",
    "\n",
    "# Train NumPy network\n",
    "print(\"\\nüìä Training NumPy Network...\")\n",
    "numpy_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(training_inputs)):\n",
    "        loss = numpy_network.backpropagation(\n",
    "            training_inputs[i], \n",
    "            training_outputs[i], \n",
    "            learning_rate\n",
    "        )\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(training_inputs)\n",
    "    numpy_losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"  Epoch {epoch}: Loss = {avg_loss:.6f}\")\n",
    "\n",
    "numpy_time = time.time() - numpy_start\n",
    "print(f\"NumPy training completed in {numpy_time:.3f} seconds\")\n",
    "\n",
    "# Train TensorFlow network\n",
    "print(\"\\nüî• Training TensorFlow Network...\")\n",
    "tf_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(training_inputs)):\n",
    "        loss = tf_network.train_step(\n",
    "            training_inputs_tf[i], \n",
    "            training_outputs_tf[i], \n",
    "            learning_rate\n",
    "        )\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(training_inputs)\n",
    "    tf_losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"  Epoch {epoch}: Loss = {avg_loss:.6f}\")\n",
    "\n",
    "tf_time = time.time() - tf_start\n",
    "print(f\"TensorFlow training completed in {tf_time:.3f} seconds\")\n",
    "\n",
    "# Train Keras network\n",
    "print(\"\\n‚ö° Training Keras Network...\")\n",
    "keras_start = time.time()\n",
    "history = keras_network.fit(\n",
    "    training_inputs_tf, \n",
    "    training_outputs_tf,\n",
    "    epochs=epochs,\n",
    "    verbose=0  # Silent training\n",
    ")\n",
    "keras_losses = history.history['loss']\n",
    "keras_time = time.time() - keras_start\n",
    "print(f\"Keras training completed in {keras_time:.3f} seconds\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Total comparison time: {total_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Results Visualization\n",
    "\n",
    "Let's visualize the training progress and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(numpy_losses, 'b-', label='NumPy', linewidth=2)\n",
    "plt.plot(tf_losses, 'r--', label='TensorFlow', linewidth=2)\n",
    "plt.plot(keras_losses, 'g:', label='Keras', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see differences better\n",
    "\n",
    "# Training time comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "methods = ['NumPy', 'TensorFlow', 'Keras']\n",
    "times = [numpy_time, tf_time, keras_time]\n",
    "colors = ['blue', 'red', 'green']\n",
    "bars = plt.bar(methods, times, color=colors, alpha=0.7)\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Speed Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add time labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{time_val:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "# Final loss comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "final_losses = [numpy_losses[-1], tf_losses[-1], keras_losses[-1]]\n",
    "bars = plt.bar(methods, final_losses, color=colors, alpha=0.7)\n",
    "plt.ylabel('Final Loss (MSE)')\n",
    "plt.title('Final Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add loss labels on bars\n",
    "for bar, loss_val in zip(bars, final_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n",
    "             f'{loss_val:.6f}', ha='center', va='bottom', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìä TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Method':<12} {'Time (s)':<10} {'Final Loss':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'NumPy':<12} {numpy_time:<10.3f} {numpy_losses[-1]:<12.6f} {'1.0x':<10}\")\n",
    "print(f\"{'TensorFlow':<12} {tf_time:<10.3f} {tf_losses[-1]:<12.6f} {numpy_time/tf_time:<10.1f}x\")\n",
    "print(f\"{'Keras':<12} {keras_time:<10.3f} {keras_losses[-1]:<12.6f} {numpy_time/keras_time:<10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing the Trained Networks\n",
    "\n",
    "Let's test all three networks on the same inputs to see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all networks on the training data\n",
    "print(\"üß™ TESTING TRAINED NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Input':<12} {'Target':<8} {'NumPy':<10} {'TensorFlow':<12} {'Keras':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(len(training_inputs)):\n",
    "    # Get predictions from all networks\n",
    "    numpy_pred = numpy_network.forward_pass(training_inputs[i])[0][0]\n",
    "    tf_pred = tf_network.forward_pass(training_inputs_tf[i]).numpy()[0][0]\n",
    "    keras_pred = keras_network.predict(training_inputs[i:i+1], verbose=0)[0][0]\n",
    "    \n",
    "    target = training_outputs[i][0]\n",
    "    \n",
    "    print(f\"{str(training_inputs[i]):<12} {target:<8.1f} {numpy_pred:<10.4f} {tf_pred:<12.4f} {keras_pred:<10.4f}\")\n",
    "\n",
    "# Test on some new data\n",
    "print(\"\\nüîç TESTING ON NEW DATA\")\n",
    "print(\"=\" * 60)\n",
    "test_inputs = np.array([\n",
    "    [0.5, 0.5],\n",
    "    [0.8, 0.2],\n",
    "    [0.3, 0.7]\n",
    "])\n",
    "\n",
    "print(f\"{'Input':<12} {'NumPy':<10} {'TensorFlow':<12} {'Keras':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(test_inputs)):\n",
    "    numpy_pred = numpy_network.forward_pass(test_inputs[i])[0][0]\n",
    "    tf_pred = tf_network.forward_pass(tf.constant(test_inputs[i])).numpy()[0][0]\n",
    "    keras_pred = keras_network.predict(test_inputs[i:i+1], verbose=0)[0][0]\n",
    "    \n",
    "    print(f\"{str(test_inputs[i]):<12} {numpy_pred:<10.4f} {tf_pred:<12.4f} {keras_pred:<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Code Complexity Comparison\n",
    "\n",
    "Let's analyze the complexity of each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code complexity analysis\n",
    "print(\"üìù CODE COMPLEXITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count lines of code for each implementation\n",
    "numpy_lines = 65  # Approximate lines in NumPy class\n",
    "tf_lines = 45     # Approximate lines in TensorFlow class  \n",
    "keras_lines = 12  # Lines for Keras implementation\n",
    "\n",
    "print(f\"NumPy Implementation:    ~{numpy_lines} lines of code\")\n",
    "print(f\"TensorFlow Implementation: ~{tf_lines} lines of code\")\n",
    "print(f\"Keras Implementation:    ~{keras_lines} lines of code\")\n",
    "\n",
    "print(\"\\nüß† WHAT YOU NEED TO UNDERSTAND:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"NumPy Approach:\")\n",
    "print(\"  ‚úÖ Forward propagation math\")\n",
    "print(\"  ‚úÖ Backpropagation algorithm\")\n",
    "print(\"  ‚úÖ Gradient calculations\")\n",
    "print(\"  ‚úÖ Weight update rules\")\n",
    "print(\"  ‚úÖ Matrix operations\")\n",
    "print(\"  ‚úÖ Activation functions\")\n",
    "\n",
    "print(\"\\nTensorFlow Low-Level:\")\n",
    "print(\"  ‚úÖ Forward propagation\")\n",
    "print(\"  ‚ö° Automatic gradients\")\n",
    "print(\"  ‚úÖ TensorFlow operations\")\n",
    "print(\"  ‚úÖ Variable management\")\n",
    "\n",
    "print(\"\\nKeras High-Level:\")\n",
    "print(\"  ‚ö° Model architecture\")\n",
    "print(\"  ‚ö° Compilation settings\")\n",
    "print(\"  ‚ö° Training process\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ NumPy: {numpy_lines/keras_lines:.1f}x more code, but full understanding\")\n",
    "print(f\"‚Ä¢ TensorFlow: {tf_lines/keras_lines:.1f}x more code, automatic gradients\")\n",
    "print(f\"‚Ä¢ Keras: Simplest, but abstracts away details\")\n",
    "print(\"‚Ä¢ NumPy is best for learning fundamentals\")\n",
    "print(\"‚Ä¢ Keras is best for rapid prototyping\")\n",
    "print(\"‚Ä¢ TensorFlow low-level gives you control + automation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ When to Use Each Approach\n",
    "\n",
    "Let's create a decision guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual decision guide\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Learning curve comparison\n",
    "learning_stages = ['Beginner', 'Intermediate', 'Advanced', 'Expert']\n",
    "numpy_difficulty = [3, 2, 1, 1]  # Gets easier as you understand more\n",
    "tf_difficulty = [4, 3, 2, 1]     # Moderate learning curve\n",
    "keras_difficulty = [1, 1, 2, 3]  # Easy start, harder to customize\n",
    "\n",
    "ax1.plot(learning_stages, numpy_difficulty, 'b-o', label='NumPy', linewidth=2, markersize=8)\n",
    "ax1.plot(learning_stages, tf_difficulty, 'r-s', label='TensorFlow', linewidth=2, markersize=8)\n",
    "ax1.plot(learning_stages, keras_difficulty, 'g-^', label='Keras', linewidth=2, markersize=8)\n",
    "ax1.set_ylabel('Difficulty Level')\n",
    "ax1.set_title('Learning Difficulty by Experience Level')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 5)\n",
    "\n",
    "# Development speed comparison\n",
    "tasks = ['Simple\\nNetwork', 'Complex\\nArchitecture', 'Custom\\nLayers', 'Research\\nIdeas']\n",
    "numpy_speed = [2, 1, 3, 4]    # Slow for simple, good for custom\n",
    "tf_speed = [3, 4, 4, 3]       # Balanced\n",
    "keras_speed = [5, 4, 2, 1]    # Fast for standard, slow for custom\n",
    "\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, numpy_speed, width, label='NumPy', color='blue', alpha=0.7)\n",
    "ax2.bar(x, tf_speed, width, label='TensorFlow', color='red', alpha=0.7)\n",
    "ax2.bar(x + width, keras_speed, width, label='Keras', color='green', alpha=0.7)\n",
    "ax2.set_ylabel('Development Speed')\n",
    "ax2.set_title('Development Speed by Task Type')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(tasks)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "dataset_sizes = ['Small\\n(<1K)', 'Medium\\n(1K-100K)', 'Large\\n(100K+)', 'Huge\\n(1M+)']\n",
    "numpy_perf = [4, 3, 2, 1]     # Good for small, poor for large\n",
    "tf_perf = [3, 4, 5, 5]        # Scales well\n",
    "keras_perf = [3, 4, 5, 5]     # Same as TF (built on TF)\n",
    "\n",
    "ax3.plot(dataset_sizes, numpy_perf, 'b-o', label='NumPy', linewidth=2, markersize=8)\n",
    "ax3.plot(dataset_sizes, tf_perf, 'r-s', label='TensorFlow', linewidth=2, markersize=8)\n",
    "ax3.plot(dataset_sizes, keras_perf, 'g-^', label='Keras', linewidth=2, markersize=8)\n",
    "ax3.set_ylabel('Performance Level')\n",
    "ax3.set_title('Performance by Dataset Size')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 6)\n",
    "\n",
    "# Use case recommendations\n",
    "use_cases = ['Learning\\nML', 'Prototyping', 'Production', 'Research']\n",
    "numpy_fit = [5, 2, 1, 4]      # Excellent for learning\n",
    "tf_fit = [3, 4, 5, 4]         # Balanced\n",
    "keras_fit = [2, 5, 4, 2]      # Great for prototyping\n",
    "\n",
    "ax4.bar(x - width, numpy_fit, width, label='NumPy', color='blue', alpha=0.7)\n",
    "ax4.bar(x, tf_fit, width, label='TensorFlow', color='red', alpha=0.7)\n",
    "ax4.bar(x + width, keras_fit, width, label='Keras', color='green', alpha=0.7)\n",
    "ax4.set_ylabel('Suitability Level')\n",
    "ax4.set_title('Best Use Cases')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(use_cases)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ DECISION GUIDE\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Choose NumPy when:\")\n",
    "print(\"  ‚Ä¢ Learning ML fundamentals\")\n",
    "print(\"  ‚Ä¢ Understanding algorithms deeply\")\n",
    "print(\"  ‚Ä¢ Implementing custom research ideas\")\n",
    "print(\"  ‚Ä¢ Small datasets and simple models\")\n",
    "\n",
    "print(\"\\nChoose TensorFlow Low-Level when:\")\n",
    "print(\"  ‚Ä¢ Need control + automatic gradients\")\n",
    "print(\"  ‚Ä¢ Building custom training loops\")\n",
    "print(\"  ‚Ä¢ Complex model architectures\")\n",
    "print(\"  ‚Ä¢ Performance is critical\")\n",
    "\n",
    "print(\"\\nChoose Keras when:\")\n",
    "print(\"  ‚Ä¢ Rapid prototyping\")\n",
    "print(\"  ‚Ä¢ Standard architectures\")\n",
    "print(\"  ‚Ä¢ Production deployment\")\n",
    "print(\"  ‚Ä¢ Team collaboration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary and Recommendations\n",
    "\n",
    "### üìä What We Learned:\n",
    "\n",
    "1. **All three approaches solve the same problem** but with different trade-offs\n",
    "2. **NumPy gives you complete understanding** of what's happening\n",
    "3. **TensorFlow provides automation** while maintaining control\n",
    "4. **Keras maximizes productivity** for standard use cases\n",
    "\n",
    "### üèÜ Performance Results:\n",
    "- **Speed**: Keras ‚âà TensorFlow > NumPy\n",
    "- **Learning**: NumPy > TensorFlow > Keras\n",
    "- **Productivity**: Keras > TensorFlow > NumPy\n",
    "- **Flexibility**: NumPy ‚âà TensorFlow > Keras\n",
    "\n",
    "### üöÄ Your Learning Path Forward:\n",
    "\n",
    "**Recommended Progression**:\n",
    "1. **Master NumPy first** (you're doing this! ‚úÖ)\n",
    "2. **Learn TensorFlow concepts** (automatic differentiation, variables)\n",
    "3. **Use Keras for real projects** (rapid development)\n",
    "4. **Return to low-level TensorFlow** when you need custom solutions\n",
    "\n",
    "### üí° Key Insights:\n",
    "\n",
    "- **NumPy teaches you the \"why\"** - essential for deep understanding\n",
    "- **TensorFlow teaches you the \"how\"** - industry-standard tools\n",
    "- **Keras teaches you the \"what\"** - practical application\n",
    "\n",
    "**Your NumPy foundation is invaluable!** It makes you a better ML practitioner because you understand what's happening under the hood. Keep building on it while exploring TensorFlow for practical applications.\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. Continue with your NumPy implementations for learning\n",
    "2. Try the next notebook: `03_Keras_Sequential_Models.ipynb`\n",
    "3. Build more complex architectures with both approaches\n",
    "4. Explore real-world datasets with TensorFlow/Keras\n",
    "\n",
    "**Remember**: The best ML engineers understand both the theory (NumPy) and the tools (TensorFlow/Keras)! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}