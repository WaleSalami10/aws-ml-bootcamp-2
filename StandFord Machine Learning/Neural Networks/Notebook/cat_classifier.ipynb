{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat vs Non-Cat Classifier\n",
    "\n",
    "A 5-Layer Deep Neural Network for classifying cat images.\n",
    "\n",
    "**Features:**\n",
    "- Data loading and preprocessing\n",
    "- Training with customizable learning rate\n",
    "- Evaluation metrics (Accuracy, Precision, Recall, F1)\n",
    "- Visualization of cost curves, confusion matrix, and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from five_layer_nn import FiveLayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the cat vs non-cat dataset.\n",
    "\n",
    "    Returns:\n",
    "    train_x_orig -- training images, shape (num_examples, height, width, channels)\n",
    "    train_y -- training labels, shape (1, num_train_examples)\n",
    "    test_x_orig -- test images, shape (num_examples, height, width, channels)\n",
    "    test_y -- test labels, shape (1, num_test_examples)\n",
    "    classes -- list of class names\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    # Load test data\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    # Get class names\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "    # Reshape labels to (1, num_examples)\n",
    "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
    "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
    "\n",
    "    return train_x_orig, train_y, test_x_orig, test_y, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_x_orig, test_x_orig):\n",
    "    \"\"\"\n",
    "    Preprocess images: flatten and normalize.\n",
    "\n",
    "    Arguments:\n",
    "    train_x_orig -- training images, shape (num_examples, height, width, channels)\n",
    "    test_x_orig -- test images, shape (num_examples, height, width, channels)\n",
    "\n",
    "    Returns:\n",
    "    train_x -- flattened and normalized training images, shape (num_features, num_examples)\n",
    "    test_x -- flattened and normalized test images, shape (num_features, num_examples)\n",
    "    \"\"\"\n",
    "    # Flatten images: (num_examples, h, w, c) -> (h*w*c, num_examples)\n",
    "    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "    # Normalize pixel values (0-255 -> 0-1)\n",
    "    train_x = train_x_flatten / 255.\n",
    "    test_x = test_x_flatten / 255.\n",
    "\n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, Y):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 score, and confusion matrix.\n",
    "\n",
    "    Arguments:\n",
    "    predictions -- predicted labels (0 or 1), shape (1, m)\n",
    "    Y -- true labels (0 or 1), shape (1, m)\n",
    "\n",
    "    Returns:\n",
    "    metrics -- dictionary with precision, recall, f1, confusion matrix\n",
    "    \"\"\"\n",
    "    predictions = predictions.flatten()\n",
    "    Y = Y.flatten()\n",
    "\n",
    "    # True Positives, False Positives, True Negatives, False Negatives\n",
    "    TP = np.sum((predictions == 1) & (Y == 1))\n",
    "    FP = np.sum((predictions == 1) & (Y == 0))\n",
    "    TN = np.sum((predictions == 0) & (Y == 0))\n",
    "    FN = np.sum((predictions == 0) & (Y == 1))\n",
    "\n",
    "    # Precision: Of all predicted cats, how many are actually cats?\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    # Recall: Of all actual cats, how many did we correctly identify?\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # F1 Score: Harmonic mean of precision and recall\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(losses, title=\"Cost over Training\"):\n",
    "    \"\"\"Plot the training cost curve.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, 'b-', linewidth=2)\n",
    "    plt.xlabel('Iterations (per 100)', fontsize=12)\n",
    "    plt.ylabel('Cost', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('cost_curve.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"    Saved: cost_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_rate_comparison(train_x, train_y, layer_dims, learning_rates, epochs=2500):\n",
    "    \"\"\"\n",
    "    Train models with different learning rates and compare.\n",
    "\n",
    "    Arguments:\n",
    "    train_x -- training data\n",
    "    train_y -- training labels\n",
    "    layer_dims -- network architecture\n",
    "    learning_rates -- list of learning rates to try\n",
    "    epochs -- number of training epochs\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n    Training with learning_rate = {lr}...\")\n",
    "        nn = FiveLayerNN(layer_dims, learning_rate=lr)\n",
    "        losses = nn.train(train_x, train_y, epochs=epochs, print_loss=False)\n",
    "\n",
    "        # Sample losses every 100 iterations for plotting\n",
    "        sampled_losses = losses[::1]  # Already sampled in train()\n",
    "\n",
    "        results[lr] = {\n",
    "            'losses': sampled_losses,\n",
    "            'final_loss': losses[-1],\n",
    "            'model': nn\n",
    "        }\n",
    "\n",
    "        plt.plot(sampled_losses, label=f'lr = {lr}', linewidth=2)\n",
    "\n",
    "    plt.xlabel('Iterations (per 100)', fontsize=12)\n",
    "    plt.ylabel('Cost', fontsize=12)\n",
    "    plt.title('Learning Rate Comparison', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('learning_rate_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n    Saved: learning_rate_comparison.png\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(metrics, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix as a heatmap.\"\"\"\n",
    "    cm = np.array([[metrics['TN'], metrics['FP']],\n",
    "                   [metrics['FN'], metrics['TP']]])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "\n",
    "    classes = ['Non-Cat', 'Cat']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "\n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontsize=16)\n",
    "\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"    Saved: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(test_x_orig, test_y, predictions, classes, num_samples=10):\n",
    "    \"\"\"Plot sample images with predictions.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    indices = np.random.choice(test_x_orig.shape[0], num_samples, replace=False)\n",
    "\n",
    "    for idx, ax in enumerate(axes):\n",
    "        i = indices[idx]\n",
    "        ax.imshow(test_x_orig[i])\n",
    "\n",
    "        pred_label = classes[int(predictions[0, i])].decode(\"utf-8\")\n",
    "        true_label = classes[int(test_y[0, i])].decode(\"utf-8\")\n",
    "\n",
    "        color = 'green' if predictions[0, i] == test_y[0, i] else 'red'\n",
    "        ax.set_title(f'Pred: {pred_label}\\nTrue: {true_label}', color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"    Saved: sample_predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_bar(train_metrics, test_metrics):\n",
    "    \"\"\"Plot comparison of metrics between train and test.\"\"\"\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    train_values = [train_metrics['accuracy'], train_metrics['precision'],\n",
    "                    train_metrics['recall'], train_metrics['f1']]\n",
    "    test_values = [test_metrics['accuracy'], test_metrics['precision'],\n",
    "                   test_metrics['recall'], test_metrics['f1']]\n",
    "\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, train_values, width, label='Train', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, test_values, width, label='Test', color='coral')\n",
    "\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Model Performance Metrics', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names, fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"    Saved: metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"    CAT VS NON-CAT CLASSIFIER\")\n",
    "print(\"    5-Layer Deep Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] Loading data...\")\n",
    "try:\n",
    "    train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "    print(f\"    Training examples: {train_x_orig.shape[0]}\")\n",
    "    print(f\"    Test examples: {test_x_orig.shape[0]}\")\n",
    "    print(f\"    Image shape: {train_x_orig.shape[1:]}\")\n",
    "    has_real_data = True\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"    Dataset not found! Using synthetic data for demo...\")\n",
    "    np.random.seed(1)\n",
    "    n_x = 12288\n",
    "    m_train = 209\n",
    "    m_test = 50\n",
    "\n",
    "    train_x = np.random.randn(n_x, m_train)\n",
    "    train_y = (np.random.rand(1, m_train) > 0.5).astype(int)\n",
    "    test_x = np.random.randn(n_x, m_test)\n",
    "    test_y = (np.random.rand(1, m_test) > 0.5).astype(int)\n",
    "    classes = np.array([b'non-cat', b'cat'])\n",
    "    train_x_orig = None\n",
    "    test_x_orig = None\n",
    "\n",
    "    print(f\"    Synthetic training examples: {m_train}\")\n",
    "    print(f\"    Synthetic test examples: {m_test}\")\n",
    "    has_real_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2] Preprocessing data...\")\n",
    "\n",
    "if has_real_data:\n",
    "    train_x, test_x = preprocess_data(train_x_orig, test_x_orig)\n",
    "    print(f\"    Flattened image size: {train_x.shape[0]}\")\n",
    "\n",
    "print(f\"    Train shape: {train_x.shape}\")\n",
    "print(f\"    Test shape: {test_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3] Setting up network architecture...\")\n",
    "n_x = train_x.shape[0]\n",
    "layer_dims = [n_x, 20, 7, 5, 3, 1]\n",
    "\n",
    "print(f\"    Architecture: {layer_dims}\")\n",
    "print(f\"    Layer 1: {layer_dims[0]} -> {layer_dims[1]} (ReLU)\")\n",
    "print(f\"    Layer 2: {layer_dims[1]} -> {layer_dims[2]} (ReLU)\")\n",
    "print(f\"    Layer 3: {layer_dims[2]} -> {layer_dims[3]} (ReLU)\")\n",
    "print(f\"    Layer 4: {layer_dims[3]} -> {layer_dims[4]} (ReLU)\")\n",
    "print(f\"    Layer 5: {layer_dims[4]} -> {layer_dims[5]} (Sigmoid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create and Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4] Training network...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "nn = FiveLayerNN(layer_dims, learning_rate=0.0075)\n",
    "losses = nn.train(train_x, train_y, epochs=2500, print_loss=True)\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot Cost Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5] Plotting cost curve...\")\n",
    "plot_cost(losses, title=\"Training Cost - Cat vs Non-Cat Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Performance with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6] Computing evaluation metrics...\")\n",
    "\n",
    "train_predictions = nn.predict(train_x)\n",
    "test_predictions = nn.predict(test_x)\n",
    "\n",
    "train_metrics = compute_metrics(train_predictions, train_y)\n",
    "test_metrics = compute_metrics(test_predictions, test_y)\n",
    "\n",
    "print(\"\\n    === TRAINING SET METRICS ===\")\n",
    "print(f\"    Accuracy:  {train_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"    Precision: {train_metrics['precision']*100:.2f}%\")\n",
    "print(f\"    Recall:    {train_metrics['recall']*100:.2f}%\")\n",
    "print(f\"    F1 Score:  {train_metrics['f1']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n    === TEST SET METRICS ===\")\n",
    "print(f\"    Accuracy:  {test_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"    Precision: {test_metrics['precision']*100:.2f}%\")\n",
    "print(f\"    Recall:    {test_metrics['recall']*100:.2f}%\")\n",
    "print(f\"    F1 Score:  {test_metrics['f1']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[7] Plotting confusion matrix...\")\n",
    "plot_confusion_matrix(test_metrics, title=\"Confusion Matrix (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Plot Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[8] Plotting metrics comparison...\")\n",
    "plot_metrics_bar(train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Learning Rate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[9] Comparing different learning rates...\")\n",
    "learning_rates = [0.001, 0.0075, 0.01, 0.05]\n",
    "lr_results = plot_learning_rate_comparison(train_x, train_y, layer_dims, learning_rates, epochs=2500)\n",
    "\n",
    "print(\"\\n    === LEARNING RATE COMPARISON RESULTS ===\")\n",
    "for lr, result in lr_results.items():\n",
    "    test_acc = result['model'].accuracy(test_x, test_y)\n",
    "    print(f\"    lr = {lr:6.4f} | Final Loss: {result['final_loss']:.6f} | Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Sample Predictions (if real data available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_real_data and test_x_orig is not None:\n",
    "    print(\"\\n[10] Plotting sample predictions...\")\n",
    "    plot_sample_predictions(test_x_orig, test_y, test_predictions, classes)\n",
    "else:\n",
    "    print(\"\\n[10] Skipping sample predictions (no real image data available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"    TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n    Generated Graphs:\")\n",
    "print(\"    - cost_curve.png\")\n",
    "print(\"    - confusion_matrix.png\")\n",
    "print(\"    - metrics_comparison.png\")\n",
    "print(\"    - learning_rate_comparison.png\")\n",
    "if has_real_data:\n",
    "    print(\"    - sample_predictions.png\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
