{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Neural Network Training\n",
    "\n",
    "This notebook contains the training implementation with momentum and learning rate decay.\n",
    "Run the implementation notebook first to get the network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex dataset\n",
    "def generate_complex_dataset(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate a complex 4D dataset that requires a deeper network to learn\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate 4D input features\n",
    "    X = np.random.uniform(-2, 2, (n_samples, 4))\n",
    "    \n",
    "    # Complex non-linear function\n",
    "    y = (\n",
    "        0.3 * np.sin(X[:, 0] * X[:, 1]) +\n",
    "        0.4 * np.cos(X[:, 2]) * X[:, 3] +\n",
    "        0.2 * (X[:, 0] ** 2 + X[:, 1] ** 2) +\n",
    "        0.1 * np.exp(-0.5 * (X[:, 2] ** 2 + X[:, 3] ** 2))\n",
    "    )\n",
    "    \n",
    "    # Normalize output to [0, 1] range\n",
    "    y = (y - y.min()) / (y.max() - y.min())\n",
    "    \n",
    "    return X, y.reshape(-1, 1)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"üìä Generating complex dataset...\")\n",
    "X_train, y_train = generate_complex_dataset(800)\n",
    "X_test, y_test = generate_complex_dataset(200)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Output range: [{y_train.min():.3f}, {y_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Advanced Backpropagation with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_backpropagation_with_momentum(network, forward_result, targets, \n",
    "                                         learning_rate, weight_momentum, \n",
    "                                         bias_momentum, momentum_factor):\n",
    "    \"\"\"\n",
    "    Advanced backpropagation with momentum for faster training\n",
    "    \"\"\"\n",
    "    if targets.ndim == 1:\n",
    "        targets = targets.reshape(-1, 1)\n",
    "    \n",
    "    activations = forward_result['activations']\n",
    "    z_values = forward_result['z_values']\n",
    "    \n",
    "    # Calculate output error\n",
    "    output_error = activations[-1] - targets\n",
    "    \n",
    "    # Initialize lists to store gradients\n",
    "    weight_gradients = []\n",
    "    bias_gradients = []\n",
    "    \n",
    "    # Backpropagate through each layer\n",
    "    current_error = output_error\n",
    "    \n",
    "    for i in reversed(range(network.num_layers - 1)):\n",
    "        # Get activation derivative\n",
    "        _, activation_derivative = network.get_activation_function(network.activations[i])\n",
    "        \n",
    "        # Calculate delta\n",
    "        delta = current_error * activation_derivative(z_values[i])\n",
    "        \n",
    "        # Calculate gradients\n",
    "        weight_grad = np.dot(activations[i].T, delta) / activations[i].shape[0]\n",
    "        bias_grad = np.mean(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        weight_gradients.append(weight_grad)\n",
    "        bias_gradients.append(bias_grad)\n",
    "        \n",
    "        # Calculate error for previous layer\n",
    "        if i > 0:\n",
    "            current_error = np.dot(delta, network.weights[i].T)\n",
    "    \n",
    "    # Reverse gradients\n",
    "    weight_gradients.reverse()\n",
    "    bias_gradients.reverse()\n",
    "    \n",
    "    # Update weights and biases with momentum\n",
    "    for i in range(network.num_layers - 1):\n",
    "        # Update momentum terms\n",
    "        weight_momentum[i] = momentum_factor * weight_momentum[i] + learning_rate * weight_gradients[i]\n",
    "        bias_momentum[i] = momentum_factor * bias_momentum[i] + learning_rate * bias_gradients[i]\n",
    "        \n",
    "        # Update parameters\n",
    "        network.weights[i] -= weight_momentum[i]\n",
    "        network.biases[i] -= bias_momentum[i]\n",
    "    \n",
    "    return np.mean(output_error ** 2)\n",
    "\n",
    "print(\"üéì Advanced backpropagation implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_network(network, X_train, y_train, X_test, y_test, \n",
    "                          epochs=1000, initial_lr=0.01, momentum=0.9, \n",
    "                          lr_decay=0.95, decay_every=200):\n",
    "    \"\"\"\n",
    "    Advanced training with momentum and learning rate decay\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting training for {epochs} epochs\")\n",
    "    print(f\"üìä Learning rate: {initial_lr}, Momentum: {momentum}\")\n",
    "    \n",
    "    # Initialize momentum terms\n",
    "    weight_momentum = [np.zeros_like(w) for w in network.weights]\n",
    "    bias_momentum = [np.zeros_like(b) for b in network.biases]\n",
    "    \n",
    "    current_lr = initial_lr\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass on training data\n",
    "        train_result = network.forward_pass(X_train)\n",
    "        \n",
    "        # Backpropagation with momentum\n",
    "        train_loss = advanced_backpropagation_with_momentum(\n",
    "            network, train_result, y_train, current_lr, \n",
    "            weight_momentum, bias_momentum, momentum\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_result = network.forward_pass(X_test)\n",
    "        test_loss = network.calculate_loss(test_result['final_output'], y_test)\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Learning rate decay\n",
    "        if (epoch + 1) % decay_every == 0:\n",
    "            current_lr *= lr_decay\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch:4d}: Train={train_loss:.6f}, Test={test_loss:.6f}, Time={elapsed:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    return {'train_losses': train_losses, 'test_losses': test_losses}\n",
    "\n",
    "print(\"üöÄ Training function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Train the Network\n",
    "\n",
    "Now let's train our advanced network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network (make sure you've run the implementation notebook first)\n",
    "# network = AdvancedNeuralNetwork([4, 6, 4, 1], ['relu', 'relu', 'sigmoid'])\n",
    "\n",
    "history = train_advanced_network(\n",
    "    network=network,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    epochs=1000,\n",
    "    initial_lr=0.01,\n",
    "    momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(history['test_losses'], 'r-', label='Test Loss', linewidth=2)\n",
    "plt.title('Training Progress')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Predictions vs Actual (Training)\n",
    "plt.subplot(1, 3, 2)\n",
    "train_pred = network.forward_pass(X_train)['final_output']\n",
    "plt.scatter(y_train, train_pred, alpha=0.5, s=10)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', linewidth=2)\n",
    "plt.title('Training: Predicted vs Actual')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual (Test)\n",
    "plt.subplot(1, 3, 3)\n",
    "test_pred = network.forward_pass(X_test)['final_output']\n",
    "plt.scatter(y_test, test_pred, alpha=0.5, s=10, color='red')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "plt.title('Test: Predicted vs Actual')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "train_mse = np.mean((y_train - train_pred) ** 2)\n",
    "test_mse = np.mean((y_test - test_pred) ** 2)\n",
    "train_r2 = 1 - train_mse / np.var(y_train)\n",
    "test_r2 = 1 - test_mse / np.var(y_test)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE METRICS\")\n",
    "print(f\"Training R¬≤: {train_r2:.4f}\")\n",
    "print(f\"Test R¬≤: {test_r2:.4f}\")\n",
    "print(f\"Final train loss: {history['train_losses'][-1]:.6f}\")\n",
    "print(f\"Final test loss: {history['test_losses'][-1]:.6f}\")\n",
    "\n",
    "if test_r2 > 0.8:\n",
    "    print(\"\\nüéâ Excellent performance!\")\n",
    "elif test_r2 > 0.6:\n",
    "    print(\"\\nüëç Good performance!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Could use more training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}